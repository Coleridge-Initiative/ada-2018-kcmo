{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing your Datasets\n",
    "---\n",
    "This notebook will take you around the different ways you can analyze your data. \n",
    "This involves looking at basic metrics in the larger dataset, taking a random sample, creating derived variables, making sense of the missing values & so on. \n",
    "\n",
    "We will be mostly using SQL (via psycopg2), hence giving you the opportunity to interact with the database directly. The same queries can also be handled by Pandas in Python (by converting your datasets into dataframes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In an ideal world, we will have all of the data we want with all of the desirable properties (no missing values, no errors, standard formats, and so on). \n",
    "However, that is hardly ever true - and we have to work with using our datasets to answer questions of interest as intelligently as possible. \n",
    "\n",
    "In this notebook, we will use our datasets to answer some questions of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "After going through this notebook, you will have a good understanding around: \n",
    "\n",
    "- How to create new tables of interest from the larger tables in database\n",
    "- How to decide on the variables of interest\n",
    "- How to quickly look through aggregate metrics before proceeding with analysis\n",
    "- Possible pitfalls\n",
    "- How to handle missing values\n",
    "- How to join newly created tables\n",
    "- How to think about caveats in your final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "    - [Learning Objectives](#Learning-Objectives)\n",
    "    - [Methods](#Methods)\n",
    "- [Setup-Coding](#Setup-Coding)\n",
    "    - [Imports](#Imports)\n",
    "    - [Connection-Settings](#Connection-Settings)\n",
    "    - [Rollback Coding](#Rollback-Coding)\n",
    "    \n",
    "- [Analysis](#Analysis)\n",
    "    - [Step-1](#Step-1)\n",
    "    - [STEP-2; DATA-A](#STEP-2;-DATA-A)\n",
    "    - [STEP-2; DATA-B](#STEP-2;-DATA-B)\n",
    "    - [STEP-2; DATA-C](#STEP-2;-DATA-C)\n",
    "    - [STEP-2; DATA-D](#STEP-2;-DATA-D)\n",
    "    - [STEP-2; DATA-E-1](#STEP-2;-DATA-E-1)\n",
    "    - [STEP-2; DATA-E-2](#STEP-2;-DATA-E-2)\n",
    "    - [STEP-2; DATA-F](#STEP-2;-DATA-F)\n",
    "    \n",
    "- [Using summary queries in SQL](#Using-summary-queries-in-SQL)\n",
    "- [Getting to know the IDHS database](#Getting-to-know-the-IDHS-database)\n",
    "- [Printing by INDEX](#Printing-by-INDEX)\n",
    "- [Sample Queries](#Sample-Queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We will be using the psycopg2 Python package to access tables in our class database  server - PostgreSQL. \n",
    "\n",
    "To read the results of our queries, we will be using the pandas Python package, which has the ability to read tabular data from SQL queries into a pandas DataFrame object.\n",
    "\n",
    "Within SQL, we will use various queries:\n",
    "\n",
    "- CREATE TABLE\n",
    "- SELECT ROWS\n",
    "- SUMMING OVER GROUPS\n",
    "- Counting Distinct Values of desired variables\n",
    "- Ordering data by chosen variables\n",
    "- Selecting a random sub-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Analyzed\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For this exercise, we will take the datasets you've been introduced to this morning and explore: \n",
    "\n",
    "- a) different questions that can be answered using these datasets.\n",
    "- b) the weeds of how we can misinterpret if we do not study the data carefully and understand it well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In Python, we `import` packages. The `import` command allows us to use libraries created by others in our own work by \"importing\" them. You can think of importing a library as opening up a toolbox and pulling out a specific tool. \n",
    "- NumPy is short for numerical python. NumPy is a lynchpin in Python's scientific computing stack. Its strengths include a powerful *N*-dimensional array object, and a large suite of functions for doing numerical computing. \n",
    "- Pandas is a library in Python for data analysis that uses the DataFrame object from R which is similiar to a spreedsheet but allows you to do your analysis programaticaly rather than the point-and-click of Excel. It is a lynchpin of the PyData stack.  \n",
    "- Psycopg2 is a python library for interfacing with a PostGreSQL database. \n",
    "- Matplotlib is the standard plotting library in python. \n",
    "`%matplotlib inline` is a so-called \"magic\" function of Jupyter that enables plots to be displayed inline with the code and text of a notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general use imports\n",
    "import datetime\n",
    "import glob\n",
    "import inspect\n",
    "import numpy\n",
    "import os\n",
    "import six\n",
    "import warnings\n",
    "\n",
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "# CSV file reading-related imports\n",
    "import csv\n",
    "\n",
    "# database interaction imports\n",
    "import psycopg2\n",
    "import psycopg2.extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When in doubt, use shift + tab to read the documentation of a method.\n",
    "#### The `help()` function provides information on what you can do with a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Instead of using pgAdmin or the command line sql too directly, we can also carry out sql queries using python. But more power of python and pandas comes from that they can greatly facilitate descpritive statistics of the data, which is rather complicated to do, if not possible, in sql per se. Moreover, python and pandas plus matplotlib package can create data visualizations that greatly helps data analysis. We will see some of these advantages in the following content.\n",
    "\n",
    "Pandas provides many ways to load data. It allows the user to read the data from a local csv or excel file, or pull the data from a relational database. Since we are working with the relational database appliedda in this course, we will demonstrate how to use pandas to read data from a relational database. For examples to read data from a csv file, refert to the pandas documentation [Getting Data In/Out](pandas.pydata.org/pandas-docs/stable/10min.html#getting-data-in-out).\n",
    "\n",
    "The function to create a sql query and put the data into a pandas dataframe (more to come) is `pd.read_sql()`. Just like doing a sql query from pgAdmin, this function will ask for some information about the database, and what query you woul like to run. Let's walk through the example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a connection to the appliedda database\n",
    "In the most simple case, only 2 parameters are required by the `pd.read_sql()` function to pull data. The first parameter is the connection to the database. To create a connection we need to use the psycopg2 package and tell it which database and which host we want to connect to, just like in pgAdmin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter 1: connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to create a connection to the database, we need to pass the name of the database and host of the database\n",
    "# db_name = \"appliedda\"\n",
    "# db_host = \"10.10.2.10\"\n",
    "# conn = psycopg2.connect(database=db_name, host=db_host) #database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nj995/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# TEMP:\n",
    "business_licenses = pd.read_csv('../../data/KCMO/BusinessLicense2013_2018NYU_01222018.csv')\n",
    "# od_main_JT01 = pd.read_csv('../../data/LODES/mo_od_main_JT01.csv')\n",
    "wac = pd.read_csv('../../data/LODES/mo_wac_S000_JT01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter 2: query\n",
    "This part is similar to writing a sql query in pgAdmin. Depending on what data we are interested in, we can use different queries to pull different data. In this example, we will pull all the content of wage_person data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# QUERY = '''\n",
    "# SELECT *\n",
    "# FROM idhs.hh_indcase_spells h\n",
    "# WHERE h.start_date >= '2015-01-01' AND h.end_date <= '2015-03-31'\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEMP:\n",
    "business_licenses = business_licenses[business_licenses['fdtmfilingPeriod']=='12/31/17'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wac = wac[wac['cbsaname']==\"Kansas City, MO-KS\"].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- the three quotation marks surrounding the query body is called multi-line string. It is quite handy for writing sql queries because the new line character will be considered part of the string, instead of breaking the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull data from the database\n",
    "Now that we know what the arguments are for the query, we can pass them to the `pd.read_sql()` function, and obtain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we pass the query and the connection to the pd.read_sql() function and assign the variable `business_licenses` \n",
    "# to the dataframe returned by the function\n",
    "# business_licenses = pd.read_sql(QUERY, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEMP:\n",
    "business_licenses = business_licenses\n",
    "wac = wac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_licenses.columns = ['business_activity', 'address', 'legal_name', 'dba_name', 'filing_period']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "### What is the current distribution of jobs by industrial sector across Kansas City, MO? What is the trend over the last 10 years?\n",
    "### Other interesting questions we can answer using same/similar datasets\n",
    "- How many blocks have industry jobs in Kansas City, MO?\n",
    "- To what extent to the different counties that make up Kansas City, MO, differ in job?\n",
    "- Distribution of these jobs by gender, race, age, income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Looking for the variables of interest in the LODES data\n",
    "- Overall Jobs\n",
    "    - See variable `C000` for total jobs by Census Block.\n",
    "- Worker Classification:\n",
    "    - Total job count is split by worker age (`CA01` to `CA03`), worker wage (`CE01` to `CE03`), worker race (`CR01` to `CR05`), worker ethnicity (`CT01` to `CT02`), worker education (`CD01` to `CD04`), and worker sex (`CS01` to `CS02`).\n",
    "- Industry Classification:\n",
    "    - The number of jobs by NAICS (North American Industry Classification System) Codes are given in variables `CNS01` to `CNS20`.\n",
    "    - *Some NAICS Codes are grouped together when they refer to a same sector (codes 31 to 33 all are all manufacturing codes, for example)*\n",
    "    - Look in the ADRF for the <span style=\"background-color: #FFFF00\">*2007 North American Industry Classification System (NAICS) Definitions data file* </span>. We have the mapping of variables `CNS01` to `CNS20` to the NAICS classification code.\n",
    "- Year Varible\n",
    "    - See `year` variable.\n",
    "- Geography\n",
    "    - The `w_geocode` is the workplace Census Block Code. This block-level variable has been mapped to state codes (`st`, `stusps`, `stname`), county codes (`cty`, `ctyname`), metropolitan area codes (`cbsa`, `cbsaname`), zipcodes (`zcta`).\n",
    "\n",
    "       \n",
    "### Step 2: Getting/Creating relevant data tables\n",
    "- [DATA A](#DATA-A): Identify how many blocks in the Kansas City, MO, metropolitan area had jobs in 2017. In which block were there the highest number of jobs?\n",
    "- [DATA B](#DATA-B): What NAICS Industry Code employed the most people in 2017?\n",
    "\n",
    "\n",
    "- [DATA C](#DATA-C): Getting a random sample of 10,000 rows from wage data\n",
    "- [DATA D](#DATA-D): Matching DATA A and DATA C\n",
    "- [DATA E](#DATA-E): MATCHING Data D HH_Member data to subset information only for head of households\n",
    "- [DATA F](#DATA-F): Comparing avergae wages across sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA A\n",
    "- back to [Steps](#Step-1:-Looking-for-the-variables-of-interest-in-the-LODES-data)\n",
    "\n",
    "Getting a list of all employers in manufacturing industries in 2010. We will use the dataset \"ides.il_qcew_employers\" for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate SQL\n",
    "sql_string = \"CREATE table if not exists \" + output_schema + \".\" + table_unique_prefix + \"naics_2010 as\"\n",
    "sql_string += \" SELECT DISTINCT empr_no, seinunit, ein, name_legal, auxiliary_naics \"\n",
    "#chosen the variables of interest\n",
    "sql_string += \" from ides.il_qcew_employers \"\n",
    "sql_string += \" where substr(auxiliary_naics,1,2) in ('31', '32', '33')\"\n",
    "#subset the data to contain only those values whose naics value starts from 31, 32 or 33\n",
    "sql_string += \" and year=2010 and quarter=1\"\n",
    "#subset the data for the time period of interest\n",
    "sql_string += \";\"\n",
    "#this seim-colon is not necessary in psycopg2 but it is widely used across SQL supporting programs. \n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE NAICS_2010 created on \" + str( datetime.datetime.now() ) )\n",
    "\n",
    "#Rollback query below if you need it\n",
    "#pgsql_connection.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now that we created our table, let us look at some of the top rows\n",
    "#generating read SQL\n",
    "sql_string1 = \"select * from \" + output_schema +  \".\" + table_unique_prefix + \"naics_2010 limit 10\"\n",
    "#since we gave a limit 10 option in our query, it is not necessary to use the head command below\n",
    "#(unless you want to see fewer than 10 rows)\n",
    "\n",
    "# read it\n",
    "r = pd.read_sql(sql_string1, con=pgsql_connection)\n",
    "r.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#It is likely that you will see that some employers do not have a legal name. \n",
    "#While not necessary for this aggregate analysis- this might be a question of interest before proceeding. \n",
    "\n",
    "\n",
    "#Let us find how many employers do not have a legal name?\n",
    "\n",
    "#generating read SQL\n",
    "sql_string2 = \"select count(distinct empr_no) from \" + output_schema + \".\" +  table_unique_prefix + \"naics_2010 \" \n",
    "sql_string2+= \" WHERE name_legal in ('nan', '.', '') \"\n",
    "# read it\n",
    "r = pd.read_sql(sql_string2, con=pgsql_connection)\n",
    "r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#It is important to see how many total number of employers are there in the data. \n",
    "#This is to take the value of missing legal names into 'statistical perspective'\n",
    "\n",
    "#generating read SQL\n",
    "sql_string3 = \"select count(distinct empr_no) from \" + output_schema + \".\" + table_unique_prefix + \"naics_2010 \"\n",
    "\n",
    "# read it\n",
    "r = pd.read_sql(sql_string3, con=pgsql_connection)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "\n",
    "- XXX of our data has missing legal names\n",
    "- A good rule of thumb is that we can drop data with missing values if missing values is less than 5% of the data. \n",
    "    - However, in this case: \n",
    "     - a) our variable of interest 'name_legal' is not important to us\n",
    "     - b) and missing values are very high\n",
    "\n",
    "**So we will not drop the data**\n",
    "\n",
    "*An alternative and probably better way to see if the employers with no legal names can be dropped is to see the below: \n",
    "    1. what percentage of employees work in these firms\n",
    "    2. how many wages are earned by the employees working with these employers\n",
    "This will again be only necessary if we care about this variable to begin with. \n",
    "Since this is an aggregate level study- we do not care about individual employers or their names. \n",
    "However, it could have been a variable of interest in some other study*\n",
    "\n",
    "- We could have done the same analysis for other variables of interest also such as: \n",
    "    - seinunit or ein\n",
    "    - naics (but it will not be missing since we subsetted our data for manufacturing codes initially)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING A CONDITION STATEMENT IN SQL\n",
    "\n",
    "##### Of interest: If an employer has a legal name or not\n",
    "#### SQL QUERIES USED: \n",
    "    * CASE WHEN\n",
    "    * GROUP BY\n",
    "    * SUM\n",
    "    * OVER\n",
    "    * DISTINCT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##WE COULD HAVE DONE THE SAME THING IN ONE QUERY ALSO\n",
    "#Calculating the percentage of employers having a legal name vs the ones not\n",
    "\n",
    "#CREATING AN ID with a CONDITION STATEMENT (CALLED CASE in SQL)\n",
    "sql_string3 = \"select case when name_legal in ('nan', '.', '') then 1 else 2 end as name_type, \"\n",
    "#Creating an ID with a value of 1 if name_legal is missing and a value of 2 if not missing\n",
    "sql_string3 += \"count(distinct empr_no), \"\n",
    "#Counting the distinct number of employers associated with both IDs\n",
    "sql_string3 += \"count(distinct empr_no)/(sum(count(distinct empr_no)) over()) PER \"\n",
    "#Counting the percentage of values in both IDs\n",
    "sql_string3 += \"from \" + output_schema + \".\" + table_unique_prefix + \"naics_2010 \" \n",
    "#SPECIFYING THE BASE TABLE\n",
    "sql_string3 += \" group by name_type \"\n",
    "#GROUPING THE SUM for both ID\n",
    "\n",
    "# read it\n",
    "r = pd.read_sql(sql_string3, con=pgsql_connection)\n",
    "r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2; DATA-B\n",
    "\n",
    "Getting a list of all employees and their wage earnings for first quarter in 2010 from \"ides.il_wage\"\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA B\n",
    "#DATA B: Getting a list of all employees and their wage earnings for first quarter in 2010 from \"ides.il_wage\"\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \"CREATE table if not exists \" + output_schema + \".\" + table_unique_prefix + \"employee_2010 as\"\n",
    "sql_string += \" SELECT DISTINCT ssn, empr_no, seinunit, ein, wage, hours, weeks\"\n",
    "sql_string += \" from ides.il_wage \"\n",
    "sql_string += \" where year=2010 and quarter=1\"\n",
    "sql_string += \";\"\n",
    "\n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE employee_2010 created on \" + str( datetime.datetime.now() ) )\n",
    "\n",
    "#pgsql_cursor.execute(\"drop table data_prep.naics_2010\")\n",
    "# pgsql_connection.commit()\n",
    "\n",
    "#pgsql_connection.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#looking at our data\n",
    "#generating read SQL\n",
    "sql_string2 = \"select * from \" + output_schema + \".\" + table_unique_prefix + \"employee_2010\"\n",
    "#sql_string2 += \" limit 10\"\n",
    "\n",
    "# read it\n",
    "r = pd.read_sql(sql_string2, con=pgsql_connection)\n",
    "r.head(5)\n",
    "#You can test the time taken to run this query by using the limit option vs not using. \n",
    "#While the limit option will lead to the query reading only 10 rows, \n",
    "#not using it will lead the query reading all of the rows- yet, reporting only 5 \n",
    "\n",
    "#It is always better to use the limit option if we know we are not interested in reading the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#looking at #employees\n",
    "#generating read SQL\n",
    "sql_string3 = \"select count(distinct ssn) from \" + output_schema + \".\" + table_unique_prefix + \"employee_2010\"\n",
    "# read it\n",
    "r = pd.read_sql(sql_string3, con=pgsql_connection)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_string3 = \"select count(*) from \" + output_schema + \".\" + table_unique_prefix + \"employee_2010\"\n",
    "r = pd.read_sql(sql_string3, con=pgsql_connection)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#looking at employees with non-missing wages\n",
    "\n",
    "sql_string4 = \"select count(distinct ssn) from \" + output_schema + \".\" + table_unique_prefix + \"employee_2010\"+ \" where wage>0\"\n",
    "r = pd.read_sql(sql_string4, con=pgsql_connection)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### STEP-2; DATA-C\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### SQL QUERY- taking a random sample\n",
    "\n",
    "We want to take a random subset of 10,000 employees to shorten the running time of this query. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Taking a random sample of employees\n",
    "\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"emp_2010_random as\"\n",
    "sql_string += \" select * from \" + output_schema + \".\" + table_unique_prefix + \"employee_2010\"\n",
    "sql_string += \" ORDER BY RANDOM()  \"\n",
    "sql_string += \" LIMIT 10000\"\n",
    "\n",
    "\n",
    "#we use distinct function so we only get unique rows\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE emp_2010_random created on \" + str( datetime.datetime.now() ) )\n",
    "#pgsql_connection.rollback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TESTING IF THIS IS INDEED 1000 rows\n",
    "\n",
    "\n",
    "sql_string = \"select count(*) from \" + output_schema + \".\" + table_unique_prefix + \"emp_2010_random\"\n",
    "\n",
    "#we use distinct function so we only get unique rows\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "r\n",
    "#pgsql_connection.rollback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pgsql_connection.rollback()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2; DATA-D\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### SQL QUERY-  Matching the Data\n",
    "- LEFT JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA D\n",
    "#DATA D: MATCHING DATA A & DATA B--- to get a list of employees who are working in manufacturing industry\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" +  table_unique_prefix + \"emp_manuf as\"\n",
    "sql_string += \" select distinct a.name_legal, a.auxiliary_naics, b.* from \" + output_schema + \".\" + table_unique_prefix + \"naics_2010 as a\"\n",
    "sql_string += \" LEFT JOIN \" + output_schema + \".\" + table_unique_prefix + \"emp_2010_random as b\"  \n",
    "sql_string += \" on a.empr_no=b.empr_no \"\n",
    "sql_string += \" and a.seinunit=b.seinunit \"\n",
    "sql_string += \" and a.ein=b.ein\"\n",
    "\n",
    "#we use distinct function so we only get unique rows\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE emp_manuf created on \" + str( datetime.datetime.now() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sql_string = \"select * from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF\"+ \" LIMIT 1\"\n",
    "# r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "# r\n",
    "\n",
    "sql_string = \"select count(*), count(distinct ssn) from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF\"\n",
    "r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2; DATA-E-1\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### SQL QUERY-  \n",
    "- CASE WHEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA E-1\n",
    "#DATA E-1: Preparing the HH_MEMBER data\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"hh_mem0 as\"\n",
    "sql_string += \" select distinct ssn_hash, rootrace, ssnind ,sex\"\n",
    "sql_string += \" from idhs.hh_member \"  \n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE hh_mem0 created on \" + str( datetime.datetime.now() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counting number of distinct SSN vs number of rows in this data\n",
    "\n",
    "sql_string = \"select count(*), count(distinct ssn_hash) from \" + output_schema + \".\" + table_unique_prefix + \"hh_mem0\"\n",
    "r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pgsql_connection.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sql_string  = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"hh_mem1 as\"\n",
    "sql_string += \" select ssn_hash,  count(*) as cnt \"\n",
    "sql_string += \"from \" + output_schema + \".\" + table_unique_prefix + \"hh_mem0 group by ssn_hash\"\n",
    "sql_string += \" order by cnt desc \"\n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading the newly created table\n",
    "sql_string=\"select * from \" + output_schema + \".\"  + ind + \"hh_mem1 limit 5\"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a sub table which has unique rows for each ssn\n",
    "sql_string  = \"Select cnt, count(*) as cnt,\"\n",
    "sql_string += \" 100*(count(*)/(sum(count(*)) over())) as per\" \n",
    "sql_string += \" from \"+ output_schema + \".\" + table_unique_prefix + \"hh_mem1\"\n",
    "sql_string += \" group by cnt\"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA E-1\n",
    "#DATA E-1: Creating the subsetted data table for hh_member which is cleaned to maintain one unique row per individual\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"hh_mem2 as\"\n",
    "sql_string += \" select distinct ssn_hash, rootrace, ssnind ,sex\"\n",
    "sql_string += \" from \" + output_schema + \".\" + table_unique_prefix + \"hh_mem0 \"  \n",
    "sql_string += \" where ssn_hash in \"\n",
    "sql_string += \" (select ssn_hash from \" + output_schema + \".\" + table_unique_prefix + \"hh_mem1\"\n",
    "sql_string += \" where cnt=1) \"\n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE hh_mem2 created on \" + str( datetime.datetime.now() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pgsql_connection.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2; DATA-E-2\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### SQL QUERY-  Matching the Data\n",
    "- INNER JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA E-2\n",
    "#DATA E: MATCHING DATA WITH HH_MEMBERS\n",
    "#WE only want information for head of households\n",
    "\n",
    "#You can use the base data hh_mem0 as well which has not been cleaned. \n",
    "#The only difference is that then you will not be able to use the summary measure 'avg' as explained further\n",
    "#keep reading!\n",
    "\n",
    "match_table= \"hh_mem2\"\n",
    "#match_table= \"hh_mem0\"\n",
    "\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"emp_manuf\" + \"_\" +match_table+ \" as\"\n",
    "sql_string += \" select a.*, b.ssn_hash, b.rootrace, b.ssnind , b.sex\"\n",
    "sql_string += \" from \" + output_schema + \".\" + table_unique_prefix + \"emp_manuf as a\"  \n",
    "sql_string += \" inner join \"  + output_schema + \".\" + table_unique_prefix + match_table  +  \" as b\"  \n",
    "sql_string += \" on a.ssn=b.ssn_hash\"\n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE emp_manuf_hh created on \" + str( datetime.datetime.now() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#number of employees\n",
    "sql_string = \"select count(*) as cnt_rows, count(distinct ssn) as cnt_ind from \" + output_schema + \".\" + table_unique_prefix + \"emp_manuf\" + \"_\" +match_table\n",
    "r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "r\n",
    "\n",
    "#If you match the above data with hh_mem0 (not cleaned), you will get ## records for ## employees. \n",
    "#If you match the above data with hh_mem2 (cleaned), you will get ## records for ##-X employees. \n",
    "\n",
    "# sql_string = \"select * from \" + output_schema + \".\" + ind + \"EMP_MANUF_HH LIMIT 2\"\n",
    "# s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "# s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2; DATA-F\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### Getting aggregate measures by SEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA F\n",
    "#DATA F: Aggregating data - by sex\n",
    "#WE only want information for head of households\n",
    "\n",
    "\n",
    "match_table= \"hh_mem2\"\n",
    "#match_table= \"hh_mem0\"\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"emp_manuf_hh_Sex as\"\n",
    "sql_string += \" select sex, count(distinct ssn) as cnt, sum(wage) as sum, \"\n",
    "sql_string += \"100*(count(distinct ssn)/sum(count(distinct ssn)) over()) as per_cnt, \"\n",
    "sql_string += \"100*(sum(wage)/ (sum(sum(wage)) over())) as per_wage\"\n",
    "\n",
    "sql_string += \" from \" + output_schema + \".\" + table_unique_prefix + \"emp_manuf_\" + match_table + \" group by sex;\"\n",
    "\n",
    "\n",
    "#group by ssn, rootrace, sex\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE emp_manuf_hh created on\" + str( datetime.datetime.now() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "\n",
    "#sql_string = \"DROP TABLE \" + output_schema + \".\" + ind + \"EMP_MANUF_HH_Sex \"\n",
    "\n",
    "sql_string = \"select * from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF_HH_Sex \"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Calculating average wages for each gender group\n",
    "\n",
    "sql_string = \"select sex, sum/cnt as avg_wage  from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF_HH_Sex  \"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using summary queries in SQL\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### When to use these instead\n",
    "Instead of actually calculating the average, we can rely on some simple SQL queries to compute mean measures. \n",
    "\n",
    "- Since we cleaned our data to have one row per individual- we can also rely on a simple SQL query 'average' to calculate average wages by any variable of interest. \n",
    "- However, if we had not cleaned our data-- then it will have to be necessary to actually 'calculate'- since SQL will take the number of rows- and not the number of distinct individuals (ssn) to calculate averages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ALTERNATIVE and QUICKER WAY TO DO THIS\n",
    "\n",
    "sql_string = \"select sex, count(distinct ssn) as ind_cnt, \"\n",
    "sql_string += \"avg(wage)  from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF_HH group by sex \"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OTHER SUMMARY MEASURES\n",
    "\n",
    "sql_string = \"select rootrace, count(distinct ssn) as ind_cnt, avg(wage) avg from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF_HH group by rootrace order by avg \"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s\n",
    "\n",
    "#  1=White, not of Hispanic origin, \n",
    "#  2=Black, not of Hispanic origin, \n",
    "#  3=American Indian or Alaskan Native, \n",
    "#  6=Hispanic (includes Mexican, Puerto Rican, Cuban, and Other South American), \n",
    "#  7=Asian or Pacific Islander (includes Indo-Chinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to know the IDHS database\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Looking at the variables of interest\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \" SELECT * \"\n",
    "sql_string += \" FROM idhs.hh_member\"\n",
    "sql_string += \" LIMIT 10\"\n",
    "sql_string += \";\"\n",
    "\n",
    "# read it\n",
    "r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "r.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDHS Database\n",
    "## HH_Member table Descrition\n",
    "\n",
    "If you go to the ADRF explorer, and read documentation, you will see what the different variables mean. \n",
    "Here, we paste the description of the variables we will be using for this notebook. \n",
    "\n",
    "1. sex- 1 for male, 2 for female\n",
    "2. rootrace        \n",
    "    - 1=White, not of Hispanic origin, \n",
    "    - 2=Black, not of Hispanic origin, \n",
    "    - 3=American Indian or Alaskan Native, \n",
    "    - 6=Hispanic (includes Mexican, Puerto Rican, Cuban, and Other South American), \n",
    "    - 7=Asian or Pacific Islander (includes Indo-Chinese)\n",
    "3. ssn_hash- hashed SSN\n",
    "4. ssnind- Indicates the validity of the recipient social security number. \n",
    "    - J=Validity unknown, \n",
    "    - K=SSA not in SSA file, \n",
    "    - L=Sex code does not match SSA file,\n",
    "    - M=DOB does not match SSA file, \n",
    "    - N=DOB and sex code do not match SSA file, \n",
    "    - O=Name does not match SSA file (sex code and DOB not checked), \n",
    "    - P=SS5 form is pending with SSA, R=SS5 form is pending with enumeration control, \n",
    "    - V=SS Administration has verified SSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sql_string1 = \"SELECT sex, count( *) as cnt, count(*)/sum(count(*)) over() as PER from idhs.hh_member group by sex\"\n",
    "# r1 = pd.read_sql(sql_string1, con=pgsql_connection)\n",
    "# print(r1)\n",
    "\n",
    "sql_string2 = \"SELECT ssnind, count( *) as cnt, 100*(count(*)/sum(count(*)) over()) as PER from idhs.hh_member group by ssnind\"\n",
    "r2 = pd.read_sql(sql_string2, con=pgsql_connection)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing by INDEX\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "    \n",
    "We can also print the results of our SQL queries by Column INDEX. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing by INDEX\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \" SELECT *\"\n",
    "sql_string += \" FROM idhs.hh_member\"\n",
    "sql_string += \" LIMIT 10\"\n",
    "sql_string += \";\"\n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "results=pgsql_cursor.fetchall()\n",
    "\n",
    "#PRINTING BY INDEX\n",
    "\n",
    "print \"\\nShow me the databases:\\n\"\n",
    "for row in results: \n",
    "    print row[1]\n",
    "   #print \"\", row[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Queries\n",
    "\n",
    "- Count of Male & Females within head of household data\n",
    "- Count of Individuals with no wages in a quarter\n",
    "- The maximum wage in a particular industry in a quarter\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Count of Male & Females within head of household data\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \" SELECT sex, count(distinct ssn_hash) as cnt \"\n",
    "sql_string += \" FROM idhs.hh_member\"\n",
    "sql_string += \" group by sex\"\n",
    "sql_string += \";\"\n",
    "\n",
    "# execute it.\n",
    "#pgsql_cursor.execute(sql_string)\n",
    "#pgsql_connection.commit()\n",
    "\n",
    "r2 = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Count of Individuals with no wages in a quarter\n",
    "\n",
    "# **LONG QUERY- DO NOT RUN WHILE IN CLASS**\n",
    "year='2010'\n",
    "quarter='2'\n",
    "\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \" SELECT count(distinct ssn) as cnt \"\n",
    "sql_string += \" FROM ides.il_wage\"\n",
    "sql_string += \" where year=\" + year +\"and quarter= \" + quarter\n",
    "sql_string += \" and wage=0\"\n",
    "\n",
    "# execute it.\n",
    "#pgsql_cursor.execute(sql_string)\n",
    "#pgsql_connection.commit()\n",
    "\n",
    "r2 = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The maximum wage and min wages in a particular time period\n",
    "\n",
    "\n",
    "# **LONG QUERY- DO NOT RUN WHILE IN CLASS**\n",
    "year='2010'\n",
    "quarter='2'\n",
    "\n",
    "\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \" SELECT min(wage) as min, max(wage) as max \"\n",
    "sql_string += \" FROM ides.il_wage\"\n",
    "sql_string += \" where year=\" + year +\"and quarter= \" + quarter\n",
    "#sql_string += \" and wage>100\"\n",
    "\n",
    "# execute it.\n",
    "#pgsql_cursor.execute(sql_string)\n",
    "#pgsql_connection.commit()\n",
    "\n",
    "r2 = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Close Connection and cursor\n",
    "pgsql_cursor.close()\n",
    "pgsql_connection.close()\n",
    "\n",
    "print( \"psycopg2 cursor and connection closed at \" + str( datetime.datetime.now() ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
