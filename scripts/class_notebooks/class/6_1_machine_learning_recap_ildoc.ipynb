{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a recap of the machine learning lecture and notebook.\n",
    "\n",
    "We will take the IL DOC data and build predictive models for recidivism\n",
    " \n",
    "1. Import Python Libraries\n",
    "2. Connect to the Database\n",
    "3. Create rows for nalysis\n",
    "4. Create labels for each row\n",
    "5. Create features for each row (based on the data of prediction for each row)\n",
    "6. Create Training and Test/Validation Sets\n",
    "7. Process Features within the training and test sets\n",
    "    1. Create dummy variables\n",
    "    2. Impute Missing values\n",
    "    3. Scale/Normalize Variables\n",
    "8. Build Models: For each model type\n",
    "    1. Select features to use\n",
    "    2. Select Label to build model for\n",
    "    3. Fit model on training set\n",
    "    4. Predict/Score on Test set\n",
    "    5. Evaluate (try different metrics)\n",
    "    6. Store results (print or csv)\n",
    "9. Compare models to see how they work\n",
    "10. Go deeper into well performing models to see which features are useful/predictive\n",
    "11. Check for what types of people it puts in high risk groups/low risk groups\n",
    "12 Check for biases\n",
    "13. Decide which model to move forward with for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup - Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from __future__ import division \n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve,roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sqlalchemy import create_engine\n",
    "#import pydotplus\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\":1.25, \"lines.markersize\":8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_name = \"appliedda\"\n",
    "hostname = \"10.10.2.10\"\n",
    "conn = psycopg2.connect(database=db_name, host = hostname) #database connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create rows that we want and labels for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "--create labels for 2 year readmission outcome\n",
    "\n",
    "create table ada_class3.ildoc_exit_admit_joined as\n",
    "\n",
    "select docnbr, curadm_date, exit_date, next_admit_date,\n",
    "\n",
    "case \n",
    "   when (next_admit_date - exit_date <= 730) then 1    \n",
    "   else 0\n",
    "end as two_year_readmit \n",
    "from\n",
    "(\n",
    "select exit.docnbr, exit.curadm_date, exit.exit_date, \n",
    "    min(admit.curadm_date) as next_admit_date\n",
    "from ildoc.ildoc_exit as exit left join ildoc.ildoc_admit as admit \n",
    "    on exit.docnbr = admit.docnbr and exit.exit_date <= admit.curadm_date\n",
    "group by exit.docnbr, exit.curadm_date, exit.exit_date\n",
    ") d;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Features\n",
    "\n",
    "For each exit, we will create:\n",
    "- demographic features: sex, gender\n",
    "- features from current stay: lengtgh of stay, age at exit, etc.\n",
    "- aggregate features from all stays up to now: # of stays, age at first stay, total number of days in prison, etc.\n",
    "\n",
    "## Remember: These features can *only* come from data on or before the exit date since that is your time of prediction\n",
    "\n",
    "If you are predicting at admit time or during someone's stay in prison, then you should have a date pf prediction to use.\n",
    "\n",
    "We will create different types of features in different tables (with different types of source data) and then join them at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "--create feature set 1\n",
    "\n",
    "create table ada_class3.temp_ildoc_features1 as\n",
    "\n",
    "select a.docnbr, a.curadm_date, a.exit_date,\n",
    "\n",
    "max(((exit_date - birth_date)/365)) as age_at_exit,\n",
    "max(sex) as sex,\n",
    "max(race) as race,\n",
    "max( a.exit_date -  a.curadm_date) as days_in_prison_this_time,\n",
    "max(birth_date) as birth_date\n",
    "\n",
    "from ada_class3.ildoc_exit_admit_joined a\n",
    "join ildoc.ildoc_exit b using (docnbr, curadm_date, exit_date)\n",
    "group by 1,2,3;\n",
    "\n",
    "\n",
    "--create feature set 2\n",
    "\n",
    "create table ada_class3.temp_ildoc_features2 as\n",
    "\n",
    "select a.docnbr, a.exit_date, a.curadm_date,\n",
    "\n",
    "count(distinct b.exit_date) as prior_exits,\n",
    "min (b.curadm_date) as first_admit_date,\n",
    "sum(b.exit_date - b.curadm_date) as total_days_in_prison,\n",
    "sum(b.exit_date - b.curadm_date)/count(distinct b.exit_date) as avg_days_in_prison\n",
    "\n",
    "from ada_class3.ildoc_exit_admit_joined a\n",
    "left join ildoc.ildoc_exit b on a.docnbr = b.docnbr and a.exit_date >= b.exit_date\n",
    "group by 1,2,3;\n",
    "\n",
    "--create feature set 3\n",
    "\n",
    "create table ada_class3.temp_ildoc_features3 as \n",
    "\n",
    "select a.docnbr,\n",
    "max(first_admit_date - birth_date)/365 as age_at_first_admit \n",
    "\n",
    "from ildoc.ildoc_exit a\n",
    "left join (select docnbr, min(curadm_date) as first_admit_date \n",
    "from ildoc.ildoc_exit group by docnbr) b using (docnbr)\n",
    "group by 1 ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--create joined feature and labels table\n",
    "\n",
    "create table ada_class3.ildoc_matrix as\n",
    "select a.docnbr,a.exit_date,a.curadm_date, a.next_admit_date, a.two_year_readmit,age_at_exit, sex,race,\n",
    "days_in_prison_this_time, prior_exits, first_admit_date, total_days_in_prison,avg_days_in_prison,\n",
    "age_at_first_admit\n",
    "\n",
    "from \n",
    "\n",
    "ada_class3.ildoc_exit_admit_joined a left join ada_class3.temp_ildoc_features1 f1  \n",
    "using (docnbr, curadm_date, exit_date)\n",
    "left join  ada_class3.temp_ildoc_features2 f2  \n",
    "using (docnbr, curadm_date, exit_date) left join\n",
    "ada_class3.temp_ildoc_features3 f3 using (docnbr);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull data in to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.read_sql(\"select * from ada_class3.ildoc_matrix where exit_date is not null;\", conn, parse_dates = ['exit_date','curadm_date', 'next_admit_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Dummy variables (convert categorical to binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_dummify = ['sex', 'race']\n",
    "df_all = pd.get_dummies(df_all, dummy_na = True, columns = columns_to_dummify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train1 = df_all[df_all['exit_date'] < '2009-06-01']\n",
    "df_test1 = df_all[df_all['exit_date'].between('2009-06-01','2010-06-01')]\n",
    "df_train2 = df_all[df_all['exit_date'] < '2012-06-01']\n",
    "df_test2 = df_all[df_all['exit_date'].between('2012-06-01' , '2013-06-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing values and impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print df_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1['age_at_exit'].fillna(df_train1['age_at_exit'].mean(), inplace=True)\n",
    "df_train1['age_at_first_admit'].fillna(df_train1['age_at_first_admit'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "df_test1['age_at_exit'].fillna(df_test1['age_at_exit'].mean(), inplace=True)\n",
    "df_test1['age_at_first_admit'].fillna(df_test1['age_at_first_admit'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "df_train2['age_at_exit'].fillna(df_train2['age_at_exit'].mean(), inplace=True)\n",
    "df_train2['age_at_first_admit'].fillna(df_train2['age_at_first_admit'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "df_test2['age_at_exit'].fillna(df_test2['age_at_exit'].mean(), inplace=True)\n",
    "df_test2['age_at_first_admit'].fillna(df_test2['age_at_first_admit'].mean(), inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define  feature groups and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_features = ['days_in_prison_this_time','age_at_exit','prior_exits','total_days_in_prison','avg_days_in_prison',\n",
    "                 'race_ASN','race_BLK','race_HSP','race_IND','race_WHI', 'race_nan','race_UNK',\n",
    "                'sex_F', 'sex_M', 'sex_nan' ]\n",
    "\n",
    "sex_features = ['sex_F', 'sex_M', 'sex_nan']\n",
    "race_features = ['race_ASN','race_BLK','race_HSP','race_IND','race_WHI', 'race_nan','race_UNK']\n",
    "\n",
    "sel_label = 'two_year_readmit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_to_use = all_features\n",
    "\n",
    "X_train = df_train1[features_to_use]\n",
    "y_train = df_train1[sel_label]\n",
    "X_test = df_test1[features_to_use]\n",
    "y_test = df_test1[sel_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale/Normalize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\n",
    "X_test[X_test.columns] = scaler.fit_transform(X_test[X_test.columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's fit a model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = RandomForestClassifier(n_estimators=1000, n_jobs = -1 )\n",
    "model.fit( X_train, y_train )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on the Test Set and Look at the Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  from our \"predictors\" using the model.\n",
    "y_scores = model.predict_proba(X_test)[:,1]\n",
    "df_test1['y_score'] = y_scores\n",
    "sns.distplot(y_scores, kde=False, rug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate: Calculate Precision and Recall at different levels of thresholds and intervention capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls \n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = y_test\n",
    "plot_precision_recall_n(expected,y_scores, 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THRESHOLD THRESHOLD THRESHOLD\n",
    "To explore the effect of choosing different thresholds to turn the prediction scores to 0 or 1, we will select one arbitrary threshold and computer the confusion matrix, accuracy, precision, and recall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "\n",
    "calc_threshold = lambda x,y: 0 if x < y else 1 \n",
    "predicted = np.array( [calc_threshold(score,Threshold) for score in y_scores] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Confusion Matrix, Accuracy, Precision, and Recall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(expected,predicted)\n",
    "\n",
    "print \"THRESHOLD = \" + str(threshold) + \"\\n\"\n",
    "print \"Confusion Matrix\\n[[TN   FP]\\n [FN  TP]]\\n\\n\",conf_matrix\n",
    "\n",
    "# generate an accuracy score by comparing expected to predicted.\n",
    "\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "print( \"\\nAccuracy = \" + str( round(accuracy*100,2) ) ) + \"%\"\n",
    "\n",
    "\n",
    "precision = round(precision_score(expected, predicted)*100,0)\n",
    "recall = round(recall_score(expected, predicted)*100,0)\n",
    "print( \"Precision = \" + str( precision ) + \"%\" )\n",
    "print( \"Recall= \" + str(recall)) + \"%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score)\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_precision_recall(expected, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores,k):\n",
    "    \n",
    "    threshold = np.sort(y_scores)[::-1][int(k*len(y_scores))]\n",
    "    y_pred = np.asarray([1 if i >= threshold else 0 for i in y_scores ])\n",
    "    return precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p_at_1 = precision_at_k(expected,y_scores, 0.01)\n",
    "print('Precision at 1%: {:.2f}'.format(p_at_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So far we've run one model and looked at the results. Now,\n",
    "# Let's run a lot of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clfs = {'RF': RandomForestClassifier(n_estimators=1000, n_jobs=-1),\n",
    "       'ET': ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        'LR': LogisticRegression(penalty='l1', C=1e5),\n",
    "        'SGD':SGDClassifier(loss='log'),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, random_state=17, n_estimators=10),\n",
    "        'NB': GaussianNB()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sel_clfs = ['RF', 'ET', 'LR', 'SGD', 'GB', 'NB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_p_at_k = 0\n",
    "df_results = pd.DataFrame()\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "for selected_classifier in sel_clfs:\n",
    "    clf = clfs[selected_classifier]\n",
    "    clf.fit( X_train, y_train )\n",
    "    print clf\n",
    "    y_score = clf.predict_proba(X_test)[:,1]\n",
    "    predicted = np.array(y_score)\n",
    "    expected = np.array(y_test)\n",
    "    plot_precision_recall_n(expected,predicted, selected_classifier)\n",
    "    p_at_1 = precision_at_k(expected,y_score, 0.01)\n",
    "    p_at_5 = precision_at_k(expected,y_score,0.05)\n",
    "    p_at_10 = precision_at_k(expected,y_score,0.10)\n",
    "    p_at_20 = precision_at_k(expected,y_score,0.20)\n",
    "    fpr, tpr, thresholds = roc_curve(expected,y_score)\n",
    "    auc_val = auc(fpr,tpr)\n",
    "    df_results = df_results.append([{\n",
    "        'Classifier Type':selected_classifier,\n",
    "        'precision_at_1_percent':p_at_1,\n",
    "        'precision_at_5_percent':p_at_5,\n",
    "        'precision_at_10_percent':p_at_10,\n",
    "        'precision_at_20_percent':p_at_20,\n",
    "        'Area Under Curve':auc_val,\n",
    "        'Classifier Details': clf\n",
    "    }])\n",
    "    \n",
    "    #feature importances\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        feature_import = dict(\n",
    "            zip(features_to_use,clf.coef_.ravel()))\n",
    "    elif hasattr(clf, 'feature_importances_'):\n",
    "        feature_import = dict(\n",
    "            zip(features_to_use, clf.feature_importances_))\n",
    "    print(\"FEATURE IMPORTANCES\")\n",
    "    print(feature_import)\n",
    "    \n",
    "    plt.clf()\n",
    "    sns.set_style('whitegrid')\n",
    "    f, ax = plt.subplots(figsize=(36,12))\n",
    "    sns.barplot(x=feature_import.keys(), y = feature_import.values())\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    #plt.rcParams[\"xtick.labelsize\"]=24\n",
    "   \n",
    "    plt.show()\n",
    "    \n",
    "#saving results to csv\n",
    "df_results.to_csv('modelrun.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Model Against Baselines\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "It is important to check our model against a reasonable **baseline** to know how well our model is doing. Without any context, 78% accuracy can sound really great... but it's not so great when you remember that you could do almost that well by declaring everyone will not need benefits in the next year, which would be stupid (not to mention useless) model. \n",
    "\n",
    "A good place to start is checking against a *random* baseline, assigning every example a label (positive or negative) completely at random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_p_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_score = [random.uniform(0,1) for i in enumerate(y_test)] \n",
    "random_predicted = np.array( [calc_threshold(score,0.5) for score in random_score] )\n",
    "random_p_at_5 = precision_at_k(expected,random_predicted, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good practice is checking against an \"expert\" or rule of thumb baseline. For example, say that talking to people at the IDHS, you find that they think it's much more likely that someone who has been on assistance multiple times already will need assistance in the future. Then you should check that your classifier does better than just labeling everyone who has had multiple past admits as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reenter_predicted = np.array([ 1 if n_spells > 3 else 0 for n_spells in df_testing.n_spells.values ])\n",
    "reenter_p_at_1 = precision_at_k(expected,reenter_predicted,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_non_reenter = np.array([0 for n_spells in df_testing.n_spells.values])\n",
    "all_non_reenter_p_at_1 = precision_at_k(expected, all_non_reenter,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=2.25, rc={\"lines.linewidth\":2.25, \"lines.markersize\":8})\n",
    "fig, ax = plt.subplots(1, figsize=(22,12))\n",
    "sns.barplot(['Random','All no need', 'More than 3 Spell','Model'],\n",
    "            [random_p_at_5, all_non_reenter_p_at_1, reenter_p_at_1, max_p_at_k],\n",
    "            palette=['#6F777D','#6F777D','#6F777D','#800000'])\n",
    "sns.despine()\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('precision at 1%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "*[Go back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "- Hastie et al.'s [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) is a classic and is available online for free.\n",
    "- James et al.'s [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), also available online, includes less mathematics and is more approachable.\n",
    "- Wu et al.'s [Top 10 Algorithms in Data Mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
