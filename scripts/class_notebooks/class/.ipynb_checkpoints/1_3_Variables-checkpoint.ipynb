{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing your Datasets\n",
    "\n",
    "This notebook will take you around the different ways you can analyze your data. \n",
    "This involves looking at basic metrics in the larger dataset, taking a random sample, creating derived variables, making sense of the missing values & so on. \n",
    "\n",
    "We will be mostly using SQL (via psycopg2), hence giving you the opportunity to interact with the database directly. The same queries can also be handled by Pandas in Python (by converting your datasets into dataframes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "    - [Learning Objectives](#Learning-Objectives)\n",
    "    - [Methods](#Methods)\n",
    "- [Setup-Coding](#Setup-Coding)\n",
    "    - [Imports](#Imports)\n",
    "    - [Connection-Settings](#Connection-Settings)\n",
    "    - [Rollback Coding](#Rollback-Coding)\n",
    "    \n",
    "- [Analysis](#Analysis)\n",
    "    - [Step-1](#Step-1)\n",
    "    - [STEP-2; DATA-A](#STEP-2;-DATA-A)\n",
    "    - [STEP-2; DATA-B](#STEP-2;-DATA-B)\n",
    "    - [STEP-2; DATA-C](#STEP-2;-DATA-C)\n",
    "    - [STEP-2; DATA-D](#STEP-2;-DATA-D)\n",
    "    - [STEP-2; DATA-E-1](#STEP-2;-DATA-E-1)\n",
    "    - [STEP-2; DATA-E-2](#STEP-2;-DATA-E-2)\n",
    "    - [STEP-2; DATA-F](#STEP-2;-DATA-F)\n",
    "    \n",
    "- [Using summary queries in SQL](#Using-summary-queries-in-SQL)\n",
    "- [Getting to know the IDHS database](#Getting-to-know-the-IDHS-database)\n",
    "- [Printing by INDEX](#Printing-by-INDEX)\n",
    "- [Sample Queries](#Sample-Queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In an ideal world, we will have all of the data we want with all of the desirable properties (no missing values, no errors, standard formats, and so on). \n",
    "However, that is hardly ever true- and we have to work with using our datasets to answer questions of interest as intelligently as possible. \n",
    "\n",
    "In this notebook, we will use our datasets to answer some questions of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "After going through this notebook, you will have a good understanding around: \n",
    "\n",
    "- How to create new tables of interest from the larger tables in database\n",
    "- How to decide on the variables of interest\n",
    "- How to quickly look through aggregate metrics before proceeding with analysis\n",
    "- Possible pitfalls\n",
    "- How to handle missing values\n",
    "- How to join newly created tables\n",
    "- How to think about caveats in your final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We will be using the psycopg2 Python package to access tables in our class database  server - PostgreSQL. \n",
    "\n",
    "To read the results of our queries, we will be using the pandas Python package, which has the ability to read tabular data from SQL queries into a pandas DataFrame object.\n",
    "\n",
    "Within SQL, we will use various queries:\n",
    "\n",
    "- CREATE TABLE\n",
    "- SELECT ROWS\n",
    "- SUMMING OVER GROUPS\n",
    "- Counting Distinct Values of desired variables\n",
    "- Ordering data by chosen variables\n",
    "- Selecting a random sub-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets Analyzed\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For this exercise, we will take the datasets you've been introduced to over the last few days and explore: \n",
    "\n",
    "- a) different questions that can be answered using these datasets.\n",
    "- b) the weeds of how we can misinterpret if we do not study the data carefully and understand it well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup-Coding\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We first need to get connected to the database. This is as we did in Day 2 in the Intro to SQL session. \n",
    "\n",
    "Here, we use the psycopg2 package to interact with the class server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general use imports\n",
    "import datetime\n",
    "import glob\n",
    "import inspect\n",
    "import numpy\n",
    "import os\n",
    "import six\n",
    "import warnings\n",
    "\n",
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "# CSV file reading-related imports\n",
    "import csv\n",
    "\n",
    "# database interaction imports\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "\n",
    "\n",
    "print( \"Imports loaded at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Database\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# schema name - change to your project's schema\n",
    "output_schema = \"<your_project_schema>\"\n",
    "\n",
    "# Use your initials as unique identifier for tables within project schema.\n",
    "table_unique_prefix = \"<your_initials>\"\n",
    "\n",
    "# Database connection properties\n",
    "db_host = \"10.10.2.10\"\n",
    "db_port = -1\n",
    "db_username = None\n",
    "db_password = None\n",
    "db_name = \"appliedda\"\n",
    "\n",
    "print( \"Database connection properties initialized at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB Connection  using `psycopg2`\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "And then a direct psycopg2 connection and cursor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create psycopg2 connection to Postgresql\n",
    "\n",
    "# example connect() call that uses all the possible parameters\n",
    "#pgsql_connection = psycopg2.connect( host = db_host, port = db_port, database = db_name, user = db_username, password = db_password )\n",
    "\n",
    "# for SQLAlchemy, just needed database name. \n",
    "pgsql_connection = psycopg2.connect( host = db_host, database = db_name )\n",
    "\n",
    "print( \"Postgresql connection to database \\\"\" + db_name + \"\\\" created at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a cursor that maps column names to values\n",
    "pgsql_cursor = pgsql_connection.cursor( cursor_factory = psycopg2.extras.DictCursor )\n",
    "\n",
    "print( \"Postgresql cursor for database \\\"\" + db_name + \"\\\" created at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rollback, in case you need it.\n",
    "pgsql_connection.rollback()\n",
    "\n",
    "print( \"Postgresql connection for database \\\"\" + db_name + \"\\\" rolled back at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "### Question: What is the distribution of wages by sex among employees in manufacturing industry in 2010 Q1? \n",
    "\n",
    "#### Other interesting questions we can answer using same/similar datasets analyzed\n",
    "- How many distinct employers in the manufacturing industry in 2010 Q1\n",
    "- How many distinct employees in the manufacturing infustry in 2010 Q1\n",
    "- Distribution of heads of household by gender \n",
    "- Distribution of heads of household by race\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## STEPS\n",
    "\n",
    "- ### STEP1: Looking in ADRF for the variables of interest\n",
    "    - Industry Classification\n",
    "        - NAICS Code- North American Industry Classification System (variable- NAICS, auxiliary_naics)\n",
    "            *we will use auxiliary naics since it does not have missing values (only 1%); and 0% for 2010*\n",
    "            Look in ADRF for *2007 North American Industry Classification System (NAICS) Definitions data file*. We have 6 digit NAICS classification code. To subset industry, we will use the first two digits of the longer code. Any code starting with 31, 32 or 33 is specifying manufacturing industry. \n",
    "        - SIC- Standard Industry Classification\n",
    "            *NAICS has replaced the SIC system in 1997.*\n",
    "    - Employer Identification\n",
    "        - empr_no, ein, seinunit\n",
    "    - Year of Interest\n",
    "        - 2010\n",
    "       \n",
    "- ### [STEP2](#STEP-2): Getting/Creating relevant data tables\n",
    "    - [DATA A](#STEP-2;-DATA-A): Getting a list of all employers in manufacturing industries in 2010. We will use the dataset \"ides.il_qcew_employers\" for this\n",
    "    - [DATA B](#STEP-2;-DATA-B): Getting a list of all employees and their wage earnings for first quarter in 2010 from \"ides.il_wage\"\n",
    "    - [DATA C](#STEP-2;-DATA-C): Getting a random sample of 10,000 rows from wage data\n",
    "    - [DATA D](#STEP-2;-DATA-D): Matching DATA A and DATA C\n",
    "    - [DATA E](#STEP-2;-DATA-E): MATCHING Data D HH_Member data to subset information only for head of households\n",
    "    - [DATA F](#STEP-2;-DATA-F): Comparing avergae wages across sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP-2\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2; DATA-A\n",
    "\n",
    "Getting a list of all employers in manufacturing industries in 2010. We will use the dataset \"ides.il_qcew_employers\" for this\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate SQL\n",
    "sql_string = \"CREATE table if not exists \" + output_schema + \".\" + table_unique_prefix + \"naics_2010 as\"\n",
    "sql_string += \" SELECT DISTINCT empr_no, seinunit, ein, name_legal, auxiliary_naics \"\n",
    "#chosen the variables of interest\n",
    "sql_string += \" from ides.il_qcew_employers \"\n",
    "sql_string += \" where substr(auxiliary_naics,1,2) in ('31', '32', '33')\"\n",
    "#subset the data to contain only those values whose naics value starts from 31, 32 or 33\n",
    "sql_string += \" and year=2010 and quarter=1\"\n",
    "#subset the data for the time period of interest\n",
    "sql_string += \";\"\n",
    "#this seim-colon is not necessary in psycopg2 but it is widely used across SQL supporting programs. \n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE NAICS_2010 created on \" + str( datetime.datetime.now() ) )\n",
    "\n",
    "#Rollback query below if you need it\n",
    "#pgsql_connection.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now that we created our table, let us look at some of the top rows\n",
    "#generating read SQL\n",
    "sql_string1 = \"select * from \" + output_schema +  \".\" + table_unique_prefix + \"naics_2010 limit 10\"\n",
    "#since we gave a limit 10 option in our query, it is not necessary to use the head command below\n",
    "#(unless you want to see fewer than 10 rows)\n",
    "\n",
    "# read it\n",
    "r = pd.read_sql(sql_string1, con=pgsql_connection)\n",
    "r.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#It is likely that you will see that some employers do not have a legal name. \n",
    "#While not necessary for this aggregate analysis- this might be a question of interest before proceeding. \n",
    "\n",
    "\n",
    "#Let us find how many employers do not have a legal name?\n",
    "\n",
    "#generating read SQL\n",
    "sql_string2 = \"select count(distinct empr_no) from \" + output_schema + \".\" +  table_unique_prefix + \"naics_2010 \" \n",
    "sql_string2+= \" WHERE name_legal in ('nan', '.', '') \"\n",
    "# read it\n",
    "r = pd.read_sql(sql_string2, con=pgsql_connection)\n",
    "r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#It is important to see how many total number of employers are there in the data. \n",
    "#This is to take the value of missing legal names into 'statistical perspective'\n",
    "\n",
    "#generating read SQL\n",
    "sql_string3 = \"select count(distinct empr_no) from \" + output_schema + \".\" + table_unique_prefix + \"naics_2010 \"\n",
    "\n",
    "# read it\n",
    "r = pd.read_sql(sql_string3, con=pgsql_connection)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "\n",
    "- XXX of our data has missing legal names\n",
    "- A good rule of thumb is that we can drop data with missing values if missing values is less than 5% of the data. \n",
    "    - However, in this case: \n",
    "     - a) our variable of interest 'name_legal' is not important to us\n",
    "     - b) and missing values are very high\n",
    "\n",
    "**So we will not drop the data**\n",
    "\n",
    "*An alternative and probably better way to see if the employers with no legal names can be dropped is to see the below: \n",
    "    1. what percentage of employees work in these firms\n",
    "    2. how many wages are earned by the employees working with these employers\n",
    "This will again be only necessary if we care about this variable to begin with. \n",
    "Since this is an aggregate level study- we do not care about individual employers or their names. \n",
    "However, it could have been a variable of interest in some other study*\n",
    "\n",
    "- We could have done the same analysis for other variables of interest also such as: \n",
    "    - seinunit or ein\n",
    "    - naics (but it will not be missing since we subsetted our data for manufacturing codes initially)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING A CONDITION STATEMENT IN SQL\n",
    "\n",
    "##### Of interest: If an employer has a legal name or not\n",
    "#### SQL QUERIES USED: \n",
    "    * CASE WHEN\n",
    "    * GROUP BY\n",
    "    * SUM\n",
    "    * OVER\n",
    "    * DISTINCT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##WE COULD HAVE DONE THE SAME THING IN ONE QUERY ALSO\n",
    "#Calculating the percentage of employers having a legal name vs the ones not\n",
    "\n",
    "#CREATING AN ID with a CONDITION STATEMENT (CALLED CASE in SQL)\n",
    "sql_string3 = \"select case when name_legal in ('nan', '.', '') then 1 else 2 end as name_type, \"\n",
    "#Creating an ID with a value of 1 if name_legal is missing and a value of 2 if not missing\n",
    "sql_string3 += \"count(distinct empr_no), \"\n",
    "#Counting the distinct number of employers associated with both IDs\n",
    "sql_string3 += \"count(distinct empr_no)/(sum(count(distinct empr_no)) over()) PER \"\n",
    "#Counting the percentage of values in both IDs\n",
    "sql_string3 += \"from \" + output_schema + \".\" + table_unique_prefix + \"naics_2010 \" \n",
    "#SPECIFYING THE BASE TABLE\n",
    "sql_string3 += \" group by name_type \"\n",
    "#GROUPING THE SUM for both ID\n",
    "\n",
    "# read it\n",
    "r = pd.read_sql(sql_string3, con=pgsql_connection)\n",
    "r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2; DATA-B\n",
    "\n",
    "Getting a list of all employees and their wage earnings for first quarter in 2010 from \"ides.il_wage\"\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA B\n",
    "#DATA B: Getting a list of all employees and their wage earnings for first quarter in 2010 from \"ides.il_wage\"\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \"CREATE table if not exists \" + output_schema + \".\" + table_unique_prefix + \"employee_2010 as\"\n",
    "sql_string += \" SELECT DISTINCT ssn, empr_no, seinunit, ein, wage, hours, weeks\"\n",
    "sql_string += \" from ides.il_wage \"\n",
    "sql_string += \" where year=2010 and quarter=1\"\n",
    "sql_string += \";\"\n",
    "\n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE employee_2010 created on \" + str( datetime.datetime.now() ) )\n",
    "\n",
    "#pgsql_cursor.execute(\"drop table data_prep.naics_2010\")\n",
    "# pgsql_connection.commit()\n",
    "\n",
    "#pgsql_connection.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#looking at our data\n",
    "#generating read SQL\n",
    "sql_string2 = \"select * from \" + output_schema + \".\" + table_unique_prefix + \"employee_2010\"\n",
    "#sql_string2 += \" limit 10\"\n",
    "\n",
    "# read it\n",
    "r = pd.read_sql(sql_string2, con=pgsql_connection)\n",
    "r.head(5)\n",
    "#You can test the time taken to run this query by using the limit option vs not using. \n",
    "#While the limit option will lead to the query reading only 10 rows, \n",
    "#not using it will lead the query reading all of the rows- yet, reporting only 5 \n",
    "\n",
    "#It is always better to use the limit option if we know we are not interested in reading the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#looking at #employees\n",
    "#generating read SQL\n",
    "sql_string3 = \"select count(distinct ssn) from \" + output_schema + \".\" + table_unique_prefix + \"employee_2010\"\n",
    "# read it\n",
    "r = pd.read_sql(sql_string3, con=pgsql_connection)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_string3 = \"select count(*) from \" + output_schema + \".\" + table_unique_prefix + \"employee_2010\"\n",
    "r = pd.read_sql(sql_string3, con=pgsql_connection)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#looking at employees with non-missing wages\n",
    "\n",
    "sql_string4 = \"select count(distinct ssn) from \" + output_schema + \".\" + table_unique_prefix + \"employee_2010\"+ \" where wage>0\"\n",
    "r = pd.read_sql(sql_string4, con=pgsql_connection)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### STEP-2; DATA-C\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### SQL QUERY- taking a random sample\n",
    "\n",
    "We want to take a random subset of 10,000 employees to shorten the running time of this query. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Taking a random sample of employees\n",
    "\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"emp_2010_random as\"\n",
    "sql_string += \" select * from \" + output_schema + \".\" + table_unique_prefix + \"employee_2010\"\n",
    "sql_string += \" ORDER BY RANDOM()  \"\n",
    "sql_string += \" LIMIT 10000\"\n",
    "\n",
    "\n",
    "#we use distinct function so we only get unique rows\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE emp_2010_random created on \" + str( datetime.datetime.now() ) )\n",
    "#pgsql_connection.rollback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TESTING IF THIS IS INDEED 1000 rows\n",
    "\n",
    "\n",
    "sql_string = \"select count(*) from \" + output_schema + \".\" + table_unique_prefix + \"emp_2010_random\"\n",
    "\n",
    "#we use distinct function so we only get unique rows\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "r\n",
    "#pgsql_connection.rollback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pgsql_connection.rollback()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2; DATA-D\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### SQL QUERY-  Matching the Data\n",
    "- LEFT JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA D\n",
    "#DATA D: MATCHING DATA A & DATA B--- to get a list of employees who are working in manufacturing industry\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" +  table_unique_prefix + \"emp_manuf as\"\n",
    "sql_string += \" select distinct a.name_legal, a.auxiliary_naics, b.* from \" + output_schema + \".\" + table_unique_prefix + \"naics_2010 as a\"\n",
    "sql_string += \" LEFT JOIN \" + output_schema + \".\" + table_unique_prefix + \"emp_2010_random as b\"  \n",
    "sql_string += \" on a.empr_no=b.empr_no \"\n",
    "sql_string += \" and a.seinunit=b.seinunit \"\n",
    "sql_string += \" and a.ein=b.ein\"\n",
    "\n",
    "#we use distinct function so we only get unique rows\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE emp_manuf created on \" + str( datetime.datetime.now() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sql_string = \"select * from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF\"+ \" LIMIT 1\"\n",
    "# r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "# r\n",
    "\n",
    "sql_string = \"select count(*), count(distinct ssn) from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF\"\n",
    "r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2; DATA-E-1\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### SQL QUERY-  \n",
    "- CASE WHEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA E-1\n",
    "#DATA E-1: Preparing the HH_MEMBER data\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"hh_mem0 as\"\n",
    "sql_string += \" select distinct ssn_hash, rootrace, ssnind ,sex\"\n",
    "sql_string += \" from idhs.hh_member \"  \n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE hh_mem0 created on \" + str( datetime.datetime.now() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counting number of distinct SSN vs number of rows in this data\n",
    "\n",
    "sql_string = \"select count(*), count(distinct ssn_hash) from \" + output_schema + \".\" + table_unique_prefix + \"hh_mem0\"\n",
    "r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pgsql_connection.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sql_string  = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"hh_mem1 as\"\n",
    "sql_string += \" select ssn_hash,  count(*) as cnt \"\n",
    "sql_string += \"from \" + output_schema + \".\" + table_unique_prefix + \"hh_mem0 group by ssn_hash\"\n",
    "sql_string += \" order by cnt desc \"\n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading the newly created table\n",
    "sql_string=\"select * from \" + output_schema + \".\"  + ind + \"hh_mem1 limit 5\"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a sub table which has unique rows for each ssn\n",
    "sql_string  = \"Select cnt, count(*) as cnt,\"\n",
    "sql_string += \" 100*(count(*)/(sum(count(*)) over())) as per\" \n",
    "sql_string += \" from \"+ output_schema + \".\" + table_unique_prefix + \"hh_mem1\"\n",
    "sql_string += \" group by cnt\"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA E-1\n",
    "#DATA E-1: Creating the subsetted data table for hh_member which is cleaned to maintain one unique row per individual\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"hh_mem2 as\"\n",
    "sql_string += \" select distinct ssn_hash, rootrace, ssnind ,sex\"\n",
    "sql_string += \" from \" + output_schema + \".\" + table_unique_prefix + \"hh_mem0 \"  \n",
    "sql_string += \" where ssn_hash in \"\n",
    "sql_string += \" (select ssn_hash from \" + output_schema + \".\" + table_unique_prefix + \"hh_mem1\"\n",
    "sql_string += \" where cnt=1) \"\n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE hh_mem2 created on \" + str( datetime.datetime.now() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pgsql_connection.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2; DATA-E-2\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### SQL QUERY-  Matching the Data\n",
    "- INNER JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA E-2\n",
    "#DATA E: MATCHING DATA WITH HH_MEMBERS\n",
    "#WE only want information for head of households\n",
    "\n",
    "#You can use the base data hh_mem0 as well which has not been cleaned. \n",
    "#The only difference is that then you will not be able to use the summary measure 'avg' as explained further\n",
    "#keep reading!\n",
    "\n",
    "match_table= \"hh_mem2\"\n",
    "#match_table= \"hh_mem0\"\n",
    "\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"emp_manuf\" + \"_\" +match_table+ \" as\"\n",
    "sql_string += \" select a.*, b.ssn_hash, b.rootrace, b.ssnind , b.sex\"\n",
    "sql_string += \" from \" + output_schema + \".\" + table_unique_prefix + \"emp_manuf as a\"  \n",
    "sql_string += \" inner join \"  + output_schema + \".\" + table_unique_prefix + match_table  +  \" as b\"  \n",
    "sql_string += \" on a.ssn=b.ssn_hash\"\n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE emp_manuf_hh created on \" + str( datetime.datetime.now() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#number of employees\n",
    "sql_string = \"select count(*) as cnt_rows, count(distinct ssn) as cnt_ind from \" + output_schema + \".\" + table_unique_prefix + \"emp_manuf\" + \"_\" +match_table\n",
    "r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "r\n",
    "\n",
    "#If you match the above data with hh_mem0 (not cleaned), you will get ## records for ## employees. \n",
    "#If you match the above data with hh_mem2 (cleaned), you will get ## records for ##-X employees. \n",
    "\n",
    "# sql_string = \"select * from \" + output_schema + \".\" + ind + \"EMP_MANUF_HH LIMIT 2\"\n",
    "# s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "# s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2; DATA-F\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### Getting aggregate measures by SEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 2- DATA F\n",
    "#DATA F: Aggregating data - by sex\n",
    "#WE only want information for head of households\n",
    "\n",
    "\n",
    "match_table= \"hh_mem2\"\n",
    "#match_table= \"hh_mem0\"\n",
    "\n",
    "sql_string = \"CREATE TABLE if not exists \" + output_schema + \".\" + table_unique_prefix + \"emp_manuf_hh_Sex as\"\n",
    "sql_string += \" select sex, count(distinct ssn) as cnt, sum(wage) as sum, \"\n",
    "sql_string += \"100*(count(distinct ssn)/sum(count(distinct ssn)) over()) as per_cnt, \"\n",
    "sql_string += \"100*(sum(wage)/ (sum(sum(wage)) over())) as per_wage\"\n",
    "\n",
    "sql_string += \" from \" + output_schema + \".\" + table_unique_prefix + \"emp_manuf_\" + match_table + \" group by sex;\"\n",
    "\n",
    "\n",
    "#group by ssn, rootrace, sex\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"TABLE emp_manuf_hh created on\" + str( datetime.datetime.now() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "\n",
    "#sql_string = \"DROP TABLE \" + output_schema + \".\" + ind + \"EMP_MANUF_HH_Sex \"\n",
    "\n",
    "sql_string = \"select * from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF_HH_Sex \"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Calculating average wages for each gender group\n",
    "\n",
    "sql_string = \"select sex, sum/cnt as avg_wage  from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF_HH_Sex  \"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using summary queries in SQL\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "- back to [STEPS](#STEPS)\n",
    "\n",
    "#### When to use these instead\n",
    "Instead of actually calculating the average, we can rely on some simple SQL queries to compute mean measures. \n",
    "\n",
    "- Since we cleaned our data to have one row per individual- we can also rely on a simple SQL query 'average' to calculate average wages by any variable of interest. \n",
    "- However, if we had not cleaned our data-- then it will have to be necessary to actually 'calculate'- since SQL will take the number of rows- and not the number of distinct individuals (ssn) to calculate averages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ALTERNATIVE and QUICKER WAY TO DO THIS\n",
    "\n",
    "sql_string = \"select sex, count(distinct ssn) as ind_cnt, \"\n",
    "sql_string += \"avg(wage)  from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF_HH group by sex \"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OTHER SUMMARY MEASURES\n",
    "\n",
    "sql_string = \"select rootrace, count(distinct ssn) as ind_cnt, avg(wage) avg from \" + output_schema + \".\" + table_unique_prefix + \"EMP_MANUF_HH group by rootrace order by avg \"\n",
    "s = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "s\n",
    "\n",
    "#  1=White, not of Hispanic origin, \n",
    "#  2=Black, not of Hispanic origin, \n",
    "#  3=American Indian or Alaskan Native, \n",
    "#  6=Hispanic (includes Mexican, Puerto Rican, Cuban, and Other South American), \n",
    "#  7=Asian or Pacific Islander (includes Indo-Chinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to know the IDHS database\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Looking at the variables of interest\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \" SELECT * \"\n",
    "sql_string += \" FROM idhs.hh_member\"\n",
    "sql_string += \" LIMIT 10\"\n",
    "sql_string += \";\"\n",
    "\n",
    "# read it\n",
    "r = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "r.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDHS Database\n",
    "## HH_Member table Descrition\n",
    "\n",
    "If you go to the ADRF explorer, and read documentation, you will see what the different variables mean. \n",
    "Here, we paste the description of the variables we will be using for this notebook. \n",
    "\n",
    "1. sex- 1 for male, 2 for female\n",
    "2. rootrace        \n",
    "    - 1=White, not of Hispanic origin, \n",
    "    - 2=Black, not of Hispanic origin, \n",
    "    - 3=American Indian or Alaskan Native, \n",
    "    - 6=Hispanic (includes Mexican, Puerto Rican, Cuban, and Other South American), \n",
    "    - 7=Asian or Pacific Islander (includes Indo-Chinese)\n",
    "3. ssn_hash- hashed SSN\n",
    "4. ssnind- Indicates the validity of the recipient social security number. \n",
    "    - J=Validity unknown, \n",
    "    - K=SSA not in SSA file, \n",
    "    - L=Sex code does not match SSA file,\n",
    "    - M=DOB does not match SSA file, \n",
    "    - N=DOB and sex code do not match SSA file, \n",
    "    - O=Name does not match SSA file (sex code and DOB not checked), \n",
    "    - P=SS5 form is pending with SSA, R=SS5 form is pending with enumeration control, \n",
    "    - V=SS Administration has verified SSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sql_string1 = \"SELECT sex, count( *) as cnt, count(*)/sum(count(*)) over() as PER from idhs.hh_member group by sex\"\n",
    "# r1 = pd.read_sql(sql_string1, con=pgsql_connection)\n",
    "# print(r1)\n",
    "\n",
    "sql_string2 = \"SELECT ssnind, count( *) as cnt, 100*(count(*)/sum(count(*)) over()) as PER from idhs.hh_member group by ssnind\"\n",
    "r2 = pd.read_sql(sql_string2, con=pgsql_connection)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing by INDEX\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "- back to [Analysis](#Analysis)\n",
    "    \n",
    "We can also print the results of our SQL queries by Column INDEX. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing by INDEX\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \" SELECT *\"\n",
    "sql_string += \" FROM idhs.hh_member\"\n",
    "sql_string += \" LIMIT 10\"\n",
    "sql_string += \";\"\n",
    "\n",
    "# execute it.\n",
    "pgsql_cursor.execute(sql_string)\n",
    "pgsql_connection.commit()\n",
    "results=pgsql_cursor.fetchall()\n",
    "\n",
    "#PRINTING BY INDEX\n",
    "\n",
    "print \"\\nShow me the databases:\\n\"\n",
    "for row in results: \n",
    "    print row[1]\n",
    "   #print \"\", row[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Queries\n",
    "\n",
    "- Count of Male & Females within head of household data\n",
    "- Count of Individuals with no wages in a quarter\n",
    "- The maximum wage in a particular industry in a quarter\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Count of Male & Females within head of household data\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \" SELECT sex, count(distinct ssn_hash) as cnt \"\n",
    "sql_string += \" FROM idhs.hh_member\"\n",
    "sql_string += \" group by sex\"\n",
    "sql_string += \";\"\n",
    "\n",
    "# execute it.\n",
    "#pgsql_cursor.execute(sql_string)\n",
    "#pgsql_connection.commit()\n",
    "\n",
    "r2 = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Count of Individuals with no wages in a quarter\n",
    "\n",
    "# **LONG QUERY- DO NOT RUN WHILE IN CLASS**\n",
    "year='2010'\n",
    "quarter='2'\n",
    "\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \" SELECT count(distinct ssn) as cnt \"\n",
    "sql_string += \" FROM ides.il_wage\"\n",
    "sql_string += \" where year=\" + year +\"and quarter= \" + quarter\n",
    "sql_string += \" and wage=0\"\n",
    "\n",
    "# execute it.\n",
    "#pgsql_cursor.execute(sql_string)\n",
    "#pgsql_connection.commit()\n",
    "\n",
    "r2 = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The maximum wage and min wages in a particular time period\n",
    "\n",
    "\n",
    "# **LONG QUERY- DO NOT RUN WHILE IN CLASS**\n",
    "year='2010'\n",
    "quarter='2'\n",
    "\n",
    "\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \" SELECT min(wage) as min, max(wage) as max \"\n",
    "sql_string += \" FROM ides.il_wage\"\n",
    "sql_string += \" where year=\" + year +\"and quarter= \" + quarter\n",
    "#sql_string += \" and wage>100\"\n",
    "\n",
    "# execute it.\n",
    "#pgsql_cursor.execute(sql_string)\n",
    "#pgsql_connection.commit()\n",
    "\n",
    "r2 = pd.read_sql(sql_string, con=pgsql_connection)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Close Connection and cursor\n",
    "pgsql_cursor.close()\n",
    "pgsql_connection.close()\n",
    "\n",
    "print( \"psycopg2 cursor and connection closed at \" + str( datetime.datetime.now() ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
