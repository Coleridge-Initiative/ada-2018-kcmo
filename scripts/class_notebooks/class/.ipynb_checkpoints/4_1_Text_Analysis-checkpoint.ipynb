{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "\n",
    "    - [Learning Outcomes](#Learning-Outcomes)\n",
    "    - [Glossary of Terms](#Glossary-of-Terms)\n",
    "\n",
    "- [Setup](#Setup)\n",
    "- [Data Orientation](#Data-Orientation)\n",
    "\n",
    "    - [Load the Data](#Load-the-Data)\n",
    "    - [Explore the Data](#Explore-the-Data)\n",
    "    - [How many facilities and types of facilities are in this dataset?](#How-many-facilities-and-types-of-facilities-are-in-this-dataset?)\n",
    "\n",
    "- [Topic Modeling](#Topic-Modeling)\n",
    "\n",
    "    - [Preparing Text Data for NLP](#Preparing-Text-Data-for-NLP) \n",
    "    \n",
    "        - [Creating a matrix of features from text - Bag of N-gram Example](#Creating-a-matrix-of-features-from-text---Bag-of-N-gram-Example)\n",
    "        \n",
    "            - **_[Exercise 1 - convert corpus to matrix](#Exercise-1---convert-corpus-to-matrix)_**\n",
    "        \n",
    "        - [Calculating Word Counts](#Calculating-Word-Counts)\n",
    "            \n",
    "            - **_[Exercise 2 - getting word counts](#Exercise-2---getting-word-counts)_**\n",
    "    \n",
    "        - [Creating Text Corpus - choosing text to analyze](#Creating-Text-Corpus---choosing-text-to-analyze)\n",
    "        - [Text Cleaning and Normalization](#Text-Cleaning-and-Normalization)\n",
    "        \n",
    "            - [First Description, Before Cleaning](#First-Description,-Before-Cleaning)\n",
    "            - [First Description, After Cleaning](#First-Description,-After-Cleaning)\n",
    "            \n",
    "        - [Tokenizing text - breaking it into pieces](#Tokenizing-text---breaking-it-into-pieces)\n",
    "        - [Removing meaningless text - Stopwords](#Removing-meaningless-text---Stopwords)\n",
    "    \n",
    "            - **_[Exercise 3 - practicing slicing](#Exercise-3---practicing-slicing)_**\n",
    "\n",
    "    - [Topic Modeling on Cleaned Data](#Topic-Modeling-on-Cleaned-Data)\n",
    "    \n",
    "        - [Stemming and Lemmatization - Distilling text data](#Stemming-and-Lemmatization---Distilling-text-data)\n",
    "        - [N-grams - Adding context by creating N-grams](#N-grams---Adding-context-by-creating-N-grams)\n",
    "        - [TF-IDF - Weighting terms based on frequency](#TF-IDF---Weighting-terms-based-on-frequency)\n",
    "        - **_[Exercise 4 - Refining a topic model](#Exercise-4---Refining-a-topic-model)_**\n",
    "        - **_[Exercise 5 - Interpreting a model's \"topics\"](#Exercise-5---Interpreting-a-model's-\"topics\")_**\n",
    "    \n",
    "- [Supervised Learning: Document Classification](#Supervised-Learning:-Document-Classification)\n",
    "\n",
    "    - [Supervised Learning - Prepare the Data](#Supervised-Learning---Prepare-the-Data)\n",
    "    - [Prepare Data for Document Classification](#Prepare-Data-for-Document-Classification)\n",
    "    - [Model Training - Train Document Classification Model](#Model-Training---Train-Document-Classification-Model)\n",
    "    - [Model Evaluation - Precision and Recall](#Model-Evaluation---Precision-and-Recall)\n",
    "    - [Model Evaluation - Feature Importances](#Model-Evaluation---Feature-Importances)\n",
    "    \n",
    "        - **_[Exercise 6 - interpreting feature importances](#Exercise-6---interpreting-feature-importances)_**\n",
    "    \n",
    "    - [Model Evaluation - Cross-validation](#Model-Evaluation---Cross-validation)\n",
    "    \n",
    "        - **_[Exercise 7 - Try a 5-fold cross-validation](#Exercise-7---Try-a-5-fold-cross-validation)_**\n",
    "    \n",
    "    - [Model Output - Examples of Document Classification](#Model-Output---Examples-of-Document-Classification)\n",
    "    \n",
    "- [Further Resources](#Further-Resources)\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "**Text analysis** is used to extract useful information from or summarize a large amount of unstructured text stored in documents. This opens up the opportunity of using text data alongside more conventional data sources (e.g. surveys and administrative data). The goal of text analysis is to take a large corpus of complex and unstructured text data and extract important and meaningful messages in a comprehensible way. \n",
    "\n",
    "Text analysis can help with the following tasks:\n",
    "\n",
    "* **Information Retrieval**: Find relevant information in a large database, such as a systematic literature review, that would be very time-consuming for humans to do manually. \n",
    "\n",
    "* **Clustering and Text Categorization**: Summarize a large corpus of text by finding the most important phrases, using methods like topic modeling. \n",
    "\n",
    "* **Text Summarization**: Create category-sensitive text summaries of a large corpus of text. \n",
    "\n",
    "* **Machine Translation**: Translate documents from one language to another. \n",
    "\n",
    "In this tutorial, we are going to analyze social services descriptions using topic modeling to examine the content of our data and document classification to tag the type of job in the advertisement. \n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this tutorial, you will...\n",
    "* Learn how to transform a corpus of text into a structured matrix format so that we can apply natural language processing (NLP) methods\n",
    "* Learn the basics and applications of topic modeling\n",
    "* Learn how to do document tagging and evaluate the results\n",
    "\n",
    " \n",
    "## Glossary of Terms\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Glossary of Terms:\n",
    "\n",
    "* **Corpus**: A corpus is the set of all text documents used in your analysis; for example, your corpus of text may include hundreds of research articles.\n",
    "\n",
    "* **Tokenize**: Tokenization is the process by which text is separated into meaningful terms or phrases. In English this is easy to do for individual words, as they are separated by whitespace; however, it can get more complicated to  automate determining which groups of words constitute meaningful phrases. \n",
    "\n",
    "* **Stemming**: Stemming is normalizing text by reducing all forms or conjugations of a word to the word's most basic form. In English, this can mean making a rule of removing the suffixes \"ed\" or \"ing\" from the end of all words, but it gets more complex. For example, \"to go\" is irregular, so you need to tell the algorithm that \"went\" and \"goes\" stem from a common lemma, and should be considered alternate forms of the word \"go.\"\n",
    "\n",
    "* **TF-IDF**: TF-IDF (term frequency-inverse document frequency) is an example of feature engineering where the most important words are extracted by taking account their frequency in documents and the entire corpus of documents as a whole.\n",
    "\n",
    "* **Topic Modeling**: Topic modeling is an unsupervised learning method where groups of words that often appear together are clustered into topics. Typically, the words in one topic should be related and make sense (e.g. boat, ship, captain). Individual documents can fall under one topic or multiple topics. \n",
    "\n",
    "* **LDA**: LDA (Latent Dirichlet Allocation) is a type of probabilistic model commonly used for topic modeling. \n",
    "\n",
    "* **Stop Words**: Stop words are words that have little semantic meaning but occur very frequently, like prepositions, articles and common nouns. For example, every document (in English) will probably contain the words \"and\" and \"the\" many times. You will often remove them as part of preprocessing using a list of stop words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "import nltk\n",
    "import ujson\n",
    "import re\n",
    "import time\n",
    "import progressbar\n",
    "\n",
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "from six.moves import zip, range \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('stopwords') #download the latest stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Orientation\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Our dataset for this tutorial will be a description of social services in PLACE, and how the subset we're using was created, can be found in the `data` folder in this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "To start, we'll load the data into a pandas DataFrame from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_socialservices_data = pd.read_csv('./data/socialservices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Our text data table has 7 fields:\n",
    "\n",
    "- `FACID` - unique ID for each facility.\n",
    "- `facname` - name of the current facility.\n",
    "- `factype` - type of the current facility.\n",
    "- `facurl` - URL of facility's main web page.\n",
    "- `facloc` - Location of facilitiy.\n",
    "- `abouturl` - URL of \"about\" page used to collect text.\n",
    "- `textfromurl` - Text collected from about URL.\n",
    "\n",
    "Let's take a look at examples of the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_socialservices_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many facilities and types of facilities are in this dataset?\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Next, let's get an idea of the contents of this data set.  First, an overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# overview of contents of data file\n",
    "df_socialservices_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list unique facility types\n",
    "df_socialservices_data.factype.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list unique facility names\n",
    "df_socialservices_data.facname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count of unique facility names\n",
    "df_socialservices_data.facname.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are X facilities, categorized into Y unique facility types: education, income, health, and safety net. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We are going to apply topic modeling, an unsupervised learning method, to our corpus to find the high-level topics in our corpus as a \"first go\" for exploring our data. Through this process, we'll discuss how to clean and preprocess our data to get the best results.\n",
    "\n",
    "Topic modeling is a broad subfield of machine learning and natural language processing. We are going to focus on a common modeling approach called Latent Dirichlet Allocation (LDA). \n",
    "\n",
    "To use topic modeling, we first have to assume that topics exist in our corpus, and that some small number of these topics can \"explain\" the corpus. Topics in this context refer to words from the corpus, in a list that is ranked by probability. A single document can be explained by multiple topics. For instance, an article on net neutrality would fall under the topic \"technology\" as well as the topic \"politics.\" The set of topics used by a document is known as the document's allocation, hence, the name Latent Dirchlet Allocation, each document has an allocation of latent topics allocated by Dirchlet distribution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Text Data for NLP\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The first important step in working with text data is cleaning and processing the data, which includes (but is not limited to):\n",
    "\n",
    "- forming a corpus of text\n",
    "- tokenization\n",
    "- removing stop-words\n",
    "- finding words co-located together (N-grams)\n",
    "- stemming and lemmatization\n",
    "\n",
    "Each of these steps will be discussed below. \n",
    "\n",
    "The ultimate goal is to transform our text data into a form an algorithm can work with, because a document or a corpus of text cannot be fed directly into an algorithm. Algorithms expect numerical feature vectors with certain fixed sizes, and can't handle documents, which are basically sequences of symbols with variable length. We will be transforming our text corpus into a *bag of n-grams* to be further analyzed. In this form our text data is represented as a matrix where each row refers to a specific job description (document) and each column is the occurence of a word (feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a matrix of features from text - Bag of N-gram Example\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Ultimately, we want to take our collection of documents, corpus, and convert it into a matrix. Fortunately, `sklearn` has a pre-built object, `CountVectorizer`, that can tokenize, eliminate stopwords, identify n-grams, and stem our corpus, and output a matrix in one step. Before we apply the vectorizer to our corpus of data, let's apply it to a toy example so that we see what the output looks like and how a bag of words is represented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_words( corpus,\n",
    "                         NGRAM_RANGE = ( 0, 1 ),\n",
    "                         stop_words = None,\n",
    "                         stem = False,\n",
    "                         MIN_DF = 0.05,\n",
    "                         MAX_DF = 0.95,\n",
    "                         USE_IDF = False ):\n",
    "\n",
    "    \"\"\"\n",
    "    Turn a corpus of text into a bag-of-words.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: ls\n",
    "        test of documents in corpus    \n",
    "    NGRAM_RANGE: tuple\n",
    "        range of N-gram. Default (0,1)\n",
    "    stop_words: ls\n",
    "        list of commonly occuring words that have little semantic\n",
    "        value\n",
    "    stem: bool\n",
    "        use a stemmer to stem words\n",
    "    MIN_DF: float\n",
    "       exclude words that have a frequency less than the threshold\n",
    "    MAX_DF: float\n",
    "        exclude words that have a frequency greater than the threshold\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bag_of_words: scipy sparse matrix\n",
    "        scipy sparse matrix of text\n",
    "    features:\n",
    "        ls of words\n",
    "    \"\"\"\n",
    "    #parameters for vectorizer \n",
    "    ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "    STRIP_ACCENTS = 'unicode'\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "    if stem:\n",
    "        tokenize = lambda x: [stemmer.stem(i) for i in x.split()]\n",
    "    else:\n",
    "        tokenize = None\n",
    "    vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                                tokenizer=tokenize, \n",
    "                                ngram_range=NGRAM_RANGE,\n",
    "                                stop_words = stop_words,\n",
    "                                strip_accents=STRIP_ACCENTS,\n",
    "                                min_df = MIN_DF,\n",
    "                                max_df = MAX_DF)\n",
    "    \n",
    "    bag_of_words = vectorizer.fit_transform( corpus ) #transform our corpus is a bag of words \n",
    "    features = vectorizer.get_feature_names()\n",
    "\n",
    "    if USE_IDF:\n",
    "        NORM = None #turn on normalization flag\n",
    "        SMOOTH_IDF = True #prvents division by zero errors\n",
    "        SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "        transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)\n",
    "        #get the bag-of-words from the vectorizer and\n",
    "        #then use TFIDF to limit the tokens found throughout the text \n",
    "        tfidf = transformer.fit_transform(bag_of_words)\n",
    "        \n",
    "        return tfidf, features\n",
    "    else:\n",
    "        return bag_of_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create example corpus.\n",
    "toy_corpus = [ 'this is document one', 'this is document two', 'text analysis on documents is fun' ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to bag of words\n",
    "toy_bag_of_words, toy_features = create_bag_of_words( toy_corpus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# review - our corpus:\n",
    "toy_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features derived from the corpus\n",
    "toy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bag of words that results:\n",
    "np_bag_of_words = toy_bag_of_words.toarray()\n",
    "np_bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has been transformed from a document into a 3 x 9 matrix, where each row in the matrix corresponds to a document, and each column corresponds to a feature (in the order they appear in `toy_features`). A 1 indicates the existence of the feature or word in the document, and a 0 indicates the word is not present.\n",
    "\n",
    "It is very common that this representation will be a \"sparse\" matrix, or a matrix that has a lot of 0s. With sparse matrices, it is often more efficient to keep track of which values *aren't* 0 and where those non-zero entries are located, rather than to save the entire matrix. To save space, the `scipy` library has special ways of storing sparse matrices in an efficient way. \n",
    "\n",
    "Our toy corpus is now ready to be analyzed. We used this toy example to illustrate how a document is turned into a matrix to be used in text analysis. When you're applying this to real text data, the matrix will be much larger and harder to interpret, but it's important that you know the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 - convert corpus to matrix\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "To check your knowledge, make your own toy corpus and turn it into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#solution\n",
    "exercise_corpus = [ 'Batman is friends with Superman', \n",
    "                    'Superman is enemies with Lex Luthor',\n",
    "                    'Batman is enemies with Lex Luthor' ] \n",
    "exercise_bag_of_words, exercise_features = create_bag_of_words( exercise_corpus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert bag of words to array\n",
    "np_bag_of_words = exercise_bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show features:\n",
    "exercise_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output derived bag of words:\n",
    "np_bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Word Counts\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As an initial look into the data, we can examine the most frequently occuring words in our corpus. We can sum the columns of the bag_of_words and then convert to a numpy array. From here we can zip the features and word_count into a dictionary, and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_counts( bag_of_words, feature_names ):\n",
    "\n",
    "    \"\"\"\n",
    "    Get the ordered word counts from a bag_of_words\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bag_of_words: obj\n",
    "        scipy sparse matrix from CounterVectorizer\n",
    "    feature_names: ls\n",
    "        list of words\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    word_counts: dict\n",
    "        Dictionary of word counts\n",
    "    \"\"\"\n",
    "\n",
    "    # convert bag of words to array\n",
    "    np_bag_of_words = bag_of_words.toarray()\n",
    "    \n",
    "    # calculate word count.\n",
    "    word_count = np.sum(np_bag_of_words,axis=0)\n",
    "    \n",
    "    # convert to flattened array.\n",
    "    np_word_count = np.asarray(word_count).ravel()\n",
    "    \n",
    "    # create dict of words mapped to count of occurrences of each word.\n",
    "    dict_word_counts = dict( zip(feature_names, np_word_count) )\n",
    "    \n",
    "    # Create ordered dictionary\n",
    "    orddict_word_counts = OrderedDict( sorted(dict_word_counts.items(), key=lambda x: x[1], reverse=True), )\n",
    "    \n",
    "    return orddict_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get ordered word counts for our example corpus.\n",
    "get_word_counts(toy_bag_of_words, toy_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the words \"document\" and \"documents\" both appear separately in the list. Should they be treated as the same words, since one is just the plural of the other, or should they be considered distinct words? These are the types of decisions you will have to make in your preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 - getting word counts\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Get the word counts of your exercise corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_word_counts(exercise_bag_of_words, exercise_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Text Corpus - choosing text to analyze\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First we need to form our corpus, or the set of all descriptions from all websites. We can pull out the array of descriptions from the data frame using the data frame's `.values` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = df_socialservices_data['textfromurl'].values #pull all the descriptions and put them in a numpy array \n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model topics with LDA\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As a start to modeling topics, we define below a function \"create_topics\" that uses Latent Dirichlet Allocation (LDA) to find topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_topics(tfidf, features, N_TOPICS=3, N_TOP_WORDS=5,):\n",
    "    \"\"\"\n",
    "    Given a matrix of features of text data generate topics\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    tfidf: scipy sparse matrix\n",
    "        sparse matrix of text features\n",
    "    N_TOPICS: int\n",
    "        number of topics (default 10)\n",
    "    N_TOP_WORDS: int\n",
    "        number of top words to display in each topic (default 10)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ls_keywords: ls\n",
    "        list of keywords for each topics\n",
    "    doctopic: array\n",
    "        numpy array with percentages of topic that fit each category\n",
    "    N_TOPICS: int\n",
    "        number of assumed topics\n",
    "    N_TOP_WORDS: int\n",
    "        Number of top words in a given topic. \n",
    "    \"\"\"\n",
    "    \n",
    "    with progressbar.ProgressBar(max_value=progressbar.UnknownLength) as bar:\n",
    "        i=0\n",
    "        lda = LatentDirichletAllocation( n_topics= N_TOPICS,\n",
    "                                       learning_method='online') #create an object that will create 5 topics\n",
    "        bar.update(i)\n",
    "        i+=1\n",
    "        doctopic = lda.fit_transform( tfidf )\n",
    "        bar.update(i)\n",
    "        i+=1\n",
    "        \n",
    "        ls_keywords = []\n",
    "        for i,topic in enumerate(lda.components_):\n",
    "            word_idx = np.argsort(topic)[::-1][:N_TOP_WORDS]\n",
    "            keywords = ', '.join( features[i] for i in word_idx)\n",
    "            ls_keywords.append(keywords)\n",
    "            print(i, keywords)\n",
    "            bar.update(i)\n",
    "            i+=1\n",
    "            \n",
    "    return ls_keywords, doctopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words and set of features from our social services corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_bag_of_words, corpus_features = create_bag_of_words(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine our features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first aspect to notice about the feature list is that the first few entries are numbers that have no real semantic meaning. The feature lists also includes numerous other useless words, such as prepositions and articles, that will just add noise to our analysis. \n",
    "\n",
    "We can also notice the words *action* and *activities*, or the words *addition* and *additional*, are close enough to each other that it might not make sense to treat them as entirely separate words. Part of your cleaning and preprocessing duties will be manually inspecting your lists of features, seeing where these issues arise, and making decisions to either remove them from your analysis or address them separately. \n",
    "\n",
    "Let's get the count of the number of times that each of the words appears in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_word_counts( corpus_bag_of_words, corpus_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our top words are articles, prepositions and conjunctions that are not informative whatsoever, so we're probably not going to come up with anything interesting (\"garbage in, garbage out\"). \n",
    "\n",
    "Nevertheless, let's forge blindly ahead and try to create topics, and see the quality of the results that we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls_corpus_keywords, corpus_doctopic = create_topics(corpus_bag_of_words, corpus_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics don't give us any real insight to what the data contains - one of the topics is \"and, the, to, of, in\"! There are some hints to the subjects of the websites (\"YWCA\", \"youth\") and their locations (\"Evanston\"), but the signal is being swamped by the noise. \n",
    "\n",
    "The word \"click\" also comes up. This word might be useful in some contexts, but since we scraped this data from websites, it's likely that \"click\" is more related to the website itself (e.g. \"Click here to find out more\") as opposed to the content of the website. \n",
    "\n",
    "We'll have to clean and process our data to get any meaningful information out of this text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "To clean and normalize text, we'll remove all special characters, numbers, and punctuation, so we're left with only the words themselves. Then we will make all the text lowercase; this uniformity will ensure that the algorithm doesn't treat \"the\" and \"The\" as different words, for example. \n",
    "\n",
    "To remove the special characters, numbers and punctuation we will use regular expressions. \n",
    "\n",
    "\n",
    "**Regular Expressions**, or \"regexes\" for short, let you find all the words or phrases in a document or text file that match a certain pattern. These rules are useful for pulling out useful information from a large amount of text. For example, if you want to find all email addresses in a document, you might look for everything that looks like *some combination of letters, _, .* followed by *@*, followed by more letters, and ending in *.com* or *.edu*. If you want to find all the credit card numbers in a document, you might look for everywhere you see the pattern \"four numbers, space, four numbers, space, four numbers, space, four numbers.\" Regexes are also helpful if you are scraping information from websites, because you can use them to separate the content from the HTML code used for formatting the website.\n",
    "\n",
    "A full tutorial on regular expressions would be outside the scope of this tutorial, but many good tutorials that can be found on-line. [regex101.com](regex101.com) is also a great interactive tool for developing and checking regular expressions.\n",
    "\n",
    ">\"Some people, when confronted with a problem, think \n",
    ">'I know, I'll use regular expressions.'   Now they have two problems.\"\n",
    "> -- Jaime Zawinski\n",
    "\n",
    "*A word of warning:* Regexes can work much more quickly than plain text sorting; however, if your regular expressions are becoming overly complicated, it's a good idea to find a simpler way to do what you want to do. Any developer should keep in mind there is a trade-off between optimization and understandability. The general philosophy of programming in Python is that your code is meant to be as understandable by *people* as much as possible, because human time is more valuable than computer time. You should therefore lean toward understandability rather than overly optimizing your code to make it run as quickly as possible. Your future-self, code-reviewers, people who inherit your code, and anyone else who has to make sense of your code in the future will appreciate it. \n",
    "\n",
    "For our purposes, we are going to use a regular expression to match all characters that are not letters -- punctuation, quotes, special characters and numbers -- and replace them with spaces. Then we'll make all of the remaining characters lowercase.  \n",
    "\n",
    "We will be using the `re` library in python for regular expression matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of the punctuations and set all characters to lowercase\n",
    "RE_PREPROCESS = re.compile( r'\\W+|\\d+' ) #the regular expressions that matches all non-characters\n",
    "\n",
    "#get rid of punctuation and make everything lowercase\n",
    "#the code below works by looping through the array of text (\"corpus\")\n",
    "#for a given piece of text ( \"description\" ) we invoke the `re.sub` command \n",
    "#the `re.sub` command takes 3 arguments: (1) the regular expression to match, \n",
    "#(2) what we want to substitute in place of that matching string (' ', a space)\n",
    "#and (3) the text we want to apply this to. \n",
    "#we then invoke the `lower()` method on the output of the `re.sub` command\n",
    "#to make all the remaining characters lowercase.\n",
    "#the result is a list, where each entry in the list is a cleaned version of the\n",
    "#corresponding entry in the original corpus.\n",
    "#we then make the list into a numpy array to use it in analysis\n",
    "\n",
    "processed_corpus = np.array( [ re.sub( RE_PREPROCESS, ' ', description ).lower() for description in corpus ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at an example of the results of this cleanup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Description, Before Cleaning\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, we'll look at the first description in our corpus, before it was cleaned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text includes a lot of useful information, but also includes some things we don't want or need. There are some weird special characters (...). There are also some numbers, which are informative and interesting to a human reading the text (phone numbers, addresses, ...), but when we break down the documents into individual words, the numbers will become meaningless. We'll also want to remove all punctuation, so that we can say any two things separated by a space are individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Description, After Cleaning\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now, let's look at this text after cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All lowercase, all numbers and special characters have been removed. Out text is now normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing text - breaking it into pieces\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we've cleaned our text, we can *tokenize* it by deciding which words or phrases are the most meaningful. Normally the `CountVectorizer` handles this for us, but in this case, we'll split our text into individual words manually to show how it is done.\n",
    "\n",
    "To go from a whole document to a list of individual words, we can use the `.split()` command. By default, this command splits based on spaces in between words, so we don't need to specify that explicitly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = processed_corpus[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing meaningless text - Stopwords\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Stopwords are words that are found commonly throughout a text and carry little semantic meaning. Examples of common stopwords are prepositions (\"to\", \"on\", \"in\"), articles (\"the\", \"an\", \"a\"), conjunctions (\"and\", \"or\", \"but\") and common nouns. For example, the words *the* and *of* are totally ubiquitous, so they won't serve as meaningful features, whether to distinguish documents from each other or to tell what a given document is about. You may also run into words that you want to remove based on where you obtained your corpus of text or what it's about. There are many lists of common stopwords available for you to use, both for general documents and for specific contexts, so you don't have to start from scratch.   \n",
    "\n",
    "We can eliminate stopwords by checking all the words in our corpus against a list of commonly occuring stopwords that comes with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample of stopwords\n",
    "#this is an example of slicing where we implicitly start at the beginning and move to the end\n",
    "#we select every 10th entry in the array\n",
    "eng_stopwords[::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this list includes \"weren\" and \"hasn\" as well as single letters (\"t\"). Why do you think these are contained in the list of stopwords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 - practicing slicing\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Try slicing to retrieve every 5th word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords[::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Topic Modeling on Cleaned Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we've cleaned up our data a little bit, let's see what our bag of words looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create bag of words from processed_corpus\n",
    "processed_bag_of_words, processed_features = create_bag_of_words( processed_corpus, stop_words = eng_stopwords )\n",
    "dict_processed_word_counts = get_word_counts( processed_bag_of_words, processed_features )\n",
    "dict_processed_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Now this is starting to look like a reasonable representation of our corpus of text. \n",
    "\n",
    "We mentioned that, in addition to stopwords that are common across all types of text analysis problems, there wil also be specific stopwords based on the context of your domain. Notice how the top words include words like \"...\"? It makes sense that these words are so common - so they won't be very helpful in analysis. \n",
    "\n",
    "One quick way to remove some of these domain-specific stopwords is by dropping some of your most frequent words. We'll start out by dropping the top 20. You'll want to change this number, playing with making it bigger and smaller, to see how it affects your resulting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get top 20 stopwords (slice from start through 20 items in list)\n",
    "top_20_words = list(dict_processed_word_counts.keys())[:20]\n",
    "\n",
    "# create new stopword list by combining default stopwords with our top 20.\n",
    "domain_specific_stopwords = eng_stopwords + top_20_words\n",
    "\n",
    "# make a new bag of words excluding custom stopwords.\n",
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what do we have now?\n",
    "dict_processed_word_counts = get_word_counts(processed_bag_of_words, processed_features)\n",
    "dict_processed_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit better - although we still see some words that are probably very common (\"...\"), words like \"...\" will probably help us come up with more specific categories within the broader realm of social services. Let's see what topics we produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are starting to get somewhere! We can manipulate the number of topics we want to find and the number of words to use for each topic to see if we can understand more from our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look for 5 topics, include 10 words in each.\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                       N_TOPICS = 5,\n",
    "                                                       N_TOP_WORDS= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some structure is starting to reveal itself - .... Adding more topics has revealed to larger subtopics. Let's see if increasing the number of topics gives us more information.\n",
    "\n",
    "However, we can see that .... are still present -  This is an iterative process - after seeing the results of some analysis, you will need to go back to the preprocessing step and add more words to your list of stopwords or change how you cleaned the data.\n",
    "\n",
    "Now let's try 10 topics with 15 words each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10 topics, 15 words each\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                       N_TOPICS = 10,\n",
    "                                                       N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a good amount of topics for now. Some of the top words are quite similar, like \"....\" Let's move to stemming and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization - Distilling text data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We can further process our text through *stemming and lemmatization*, or replacing words with their root or simplest form. For example \"systems,\" \"systematic,\" and \"system\" are all different words, but we can replace all these words with \"system\" without sacrificing much meaning. \n",
    "\n",
    "- A **lemma** is the original dictionary form of a word (e.g. the lemma for \"lies,\" \"lied,\" and \"lying\" is \"lie\").\n",
    "- The process of turning a word into its simplest form is **stemming**. There are several well known stemming algorithms -- Porter, Snowball, Lancaster -- that all have their respective strengths and weaknesses.\n",
    "\n",
    "For this tutorial, we'll use the Snowball Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examples of how a Stemmer works:\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem('lies'))\n",
    "print(stemmer.stem(\"lying\"))\n",
    "print(stemmer.stem('systematic'))\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try creating a bag of stemmed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include stemming when creating our bag of words.\n",
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords,\n",
    "                                                                 stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create topics with stemmed words.\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                       N_TOPICS = 10,\n",
    "                                                       N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we think of these topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams - Adding context by creating N-grams\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Obviously, reducing a document to a bag of words means losing much of its meaning - we put words in certain orders, and group words together in phrases and sentences, precisely to give them more meaning. If you follow the processing steps we've gone through so far, splitting your document into individual words and then removing stopwords, you'll completely lose all phrases like \"kick the bucket,\" \"commander in chief,\" or \"sleeps with the fishes.\" \n",
    "\n",
    "One way to address this is to break down each document similarly, but rather than treating each word as an individual unit, treat each group of 2 words, or 3 words, or *n* words, as a unit. We call this a \"bag of *n*-grams,\" where *n* is the number of words in each chunk. Then you can analyze which groups of words commonly occur together (in a fixed order). \n",
    "\n",
    "Let's transform our corpus into a bag of n-grams with *n*=2: a bag of 2-grams, AKA a bag of bi-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create bag of words with stemmed words and 2-grams (NGRAM_RANGE = (0, 2)).\n",
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords,\n",
    "                                                                 stem=True,\n",
    "                                                                 NGRAM_RANGE=(0,2))\n",
    "\n",
    "# Create topics.\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                       N_TOPICS = 10,\n",
    "                                                       N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this lets us uncover patterns that we couldn't when we just used a bag of words: \"north shore\" and \"domest violenc\" come up as words. Note that this still includes the individual words, as well as the bi-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF - Weighting terms based on frequency\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "A final step in cleaning and processing our text data is **Term Frequency-Inverse Document Frequency (TF-IDF)**. TF-IDF is based on the idea that the words (or terms) that are most related to a certain topic will occur frequently in documents on that topic, and infrequently in unrelated documents.  TF-IDF re-weights words so that we emphasize words that are unique to a document and suppress words that are common throughout the corpus by inversely weighting terms based on their frequency within the document and across the corpus.\n",
    "\n",
    "Let's look at how using TF-IDF affects our bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create bag of words including TF-IDF weighting.\n",
    "processed_bag_of_words, processed_features = create_bag_of_words( processed_corpus,\n",
    "                                                                  stop_words = domain_specific_stopwords,\n",
    "                                                                  stem = True,\n",
    "                                                                  NGRAM_RANGE = ( 0, 2 ),\n",
    "                                                                  USE_IDF = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's see what we have:\n",
    "dict_word_counts = get_word_counts( processed_bag_of_words, processed_features )\n",
    "dict_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words counts have been reweighted to emphasize the more meaningful words of the corpus, while de-emphasizing the words that are found commonly throughout the corpus.\n",
    "\n",
    "How does this affect our topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                       N_TOPICS = 10,\n",
    "                                                       N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 4 - Refining a topic model\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "You can only develop an intuition for the right number of topics and topic words suitable for a given problem by iterating until you find a good match. \n",
    "\n",
    "Change the number of topics and topic words until you get an intution of how many words and topics are enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exercise_keywords, exercise_doctopic = create_topics( processed_bag_of_words, \n",
    "                                                      processed_features, \n",
    "                                                      N_TOPICS = 5,\n",
    "                                                      N_TOP_WORDS= 25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exercise_keywords, exercise_doctopic = create_topics( processed_bag_of_words, \n",
    "                                                      processed_features, \n",
    "                                                      N_TOPICS = 10,\n",
    "                                                      N_TOP_WORDS= 25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grab the topic_id of the majority topic for each document and store it in a list\n",
    "ls_topic_id = [np.argsort(processed_doctopic[comment_id])[::-1][0] for comment_id in range(len(corpus))]\n",
    "df_socialservices_data['topic_id'] = ls_topic_id #add to the dataframe so we can compare with the job titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each row is tagged with a topic ID. Let's see how well the topics explain the social services by looking at the first topic, and seeing how similar the social services within that topic are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_num = 0\n",
    "print(processed_keywords[topic_num])\n",
    "df_socialservices_data[ df_socialservices_data.topic_id == topic_num ].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 5 - Interpreting a model's \"topics\"\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Examine the other topic IDs, and see if the \"topics\" we identified make sense as groupings of social service agencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_num = 3\n",
    "print(processed_keywords[topic_num])\n",
    "df_socialservices_data[ df_socialservices_data.topic_id == topic_num ].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Document Classification\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Previously, we used topic modeling to infer relationships between social service facilities within the data. That is an example of unsupervised learning: we were looking to uncover structure in the form of topics, or groups of agencies, but we did not necessarily know the ground truth of how many groups we should find or which agencies belonged in which group.  \n",
    "\n",
    "Now we turn our attention to supervised learning. In supervised learning, we have a *known* outcome or label (*Y*) that we want to produce given some data (*X*), and in general, we want to be able to produce this *Y* when we *don't* know it, or when we *only* have *X*. \n",
    "\n",
    "In order to produce labels we need to first have examples our algorithm can learn from, a \"training set.\" In the context of text analysis, developing a training set can be very expensive, as it can require a large amount of human labor or linguistic expertise. **Document classification** is an example of supervised learning in which want to characterize our documents based on their contents (*X*). A common example of document classification is spam e-mail detection. Another example of supervised learning in text analysis is *sentiment analysis*, where *X* is our documents and *Y* is the state of the author. This \"state\" is dependent on the question you're trying to answer, and can range from the author being happy or unhappy with a product to the author being politically conservative or liberal. Another example is *part-of-speech tagging* where *X* are individual words and *Y* is the part-of-speech. \n",
    "\n",
    "In this section, we'll train a classifier to classify social service agencies. Let's see if we can label a new website as belonging to facility type \"income\" or \"health.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - Prepare the Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at counts\n",
    "df_socialservices_data.factype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a mask column we can use to flag rows with facility type in our types of interest.\n",
    "mask = df_socialservices_data.factype.isin(['income','health'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use mask to subset our data.\n",
    "df_income_health = df_socialservices_data[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and testing sets (20% held back for training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train and test sets.\n",
    "df_train, df_test = train_test_split(df_income_health, test_size=0.20, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at our training set.\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure we only have the facility types we expect.\n",
    "df_train['factype'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at the counts for each value.\n",
    "Counter(df_train['factype'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at our testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at our testing set.\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure we only have the facility types we expect.\n",
    "df_test['factype'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at the counts for each value.\n",
    "Counter(df_test['factype'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Document Classification\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In order to feed out data into a classifier, we need to pull out the labels (*Y*) and a clean corpus of documents (*X*) for our training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare training data - get labels we'll train on.\n",
    "train_labels = df_train.factype.values\n",
    "\n",
    "# prepare training data - clean text.\n",
    "train_corpus = np.array( [re.sub(RE_PREPROCESS, ' ', text).lower() for text in df_train.textfromurl.values])\n",
    "\n",
    "# prepare testing data - get labels we'll train on.\n",
    "test_labels = df_test.factype.values\n",
    "\n",
    "# prepare testing data - clean text.\n",
    "test_corpus = np.array( [re.sub(RE_PREPROCESS, ' ', text).lower() for text in df_test.textfromurl.values])\n",
    "\n",
    "# make list of all labels across train and test (should just be 'income' and 'health')\n",
    "labels = np.append(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we had done in the unsupervised learning context, we have to transform our data. This time we have to transform our testing and training set into two different bags of words. The classifier will learn from the training set, and we will evaluate the classifier's performance on the testing set.\n",
    "\n",
    "First, we create a CountVectorizer that we'll use to convert our text documents to matrices of features based on words contained within our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters for vectorizer \n",
    "ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "STRIP_ACCENTS = 'unicode'\n",
    "TOKENIZER = None\n",
    "NGRAM_RANGE = (0,2) #Range for pharases of words\n",
    "MIN_DF = 0.01 # Exclude words that have a frequency less than the threshold\n",
    "MAX_DF = 0.8  # Exclude words that have a frequency greater then the threshold \n",
    "\n",
    "vectorizer = CountVectorizer( analyzer = ANALYZER,\n",
    "                              tokenizer = None, # alternatively tokenize_and_stem but it will be slower \n",
    "                              ngram_range = NGRAM_RANGE,\n",
    "                              stop_words = stopwords.words( 'english' ),\n",
    "                              strip_accents = STRIP_ACCENTS,\n",
    "                              min_df = MIN_DF,\n",
    "                              max_df = MAX_DF )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a TF-IDF transformer, and create our bags of words, then weight them using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NORM = None          # turn on normalization flag\n",
    "SMOOTH_IDF = True    # prevents division by zero errors\n",
    "SUBLINEAR_IDF = True # replace TF with 1 + log(TF)\n",
    "USE_IDF = True       # flag to control whether to use TFIDF\n",
    "\n",
    "transformer = TfidfTransformer( norm = NORM,\n",
    "                                smooth_idf = SMOOTH_IDF,\n",
    "                                sublinear_tf = True )\n",
    "\n",
    "# timing code - start!\n",
    "start_time = time.time()\n",
    "\n",
    "# get the bag-of-words for train and test from the vectorizer and\n",
    "# then use TFIDF to limit the tokens found throughout the text \n",
    "train_bag_of_words = vectorizer.fit_transform( train_corpus ) #using all the data on for generating features!! Bad!\n",
    "test_bag_of_words = vectorizer.transform( test_corpus )\n",
    "\n",
    "# if we use IDF, compute it here.\n",
    "if USE_IDF:\n",
    "    train_tfidf = transformer.fit_transform(train_bag_of_words)\n",
    "    test_tfidf = transformer.transform(test_bag_of_words)\n",
    "\n",
    "# Get list of the feature names, for passing to our model.\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "# timing code - done!\n",
    "print('Time Elapsed: {0:.2f}s'.format( time.time() - start_time ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot pass the labels \"income\" or \"health\" directly to the classifier. Instead, we to encode them as 0s and 1s using the `labelencoder` part of `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#relabel our labels as a 0 or 1\n",
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(labels)\n",
    "labels_binary = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(zip(labels,labels_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create arrays of indices so we can access the training and testing sets accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = df_train.shape[ 0 ]\n",
    "train_set_idx = np.arange( 0, train_size )\n",
    "test_set_idx = np.arange( train_size, len( labels ) )\n",
    "train_labels_binary = labels_binary[ train_set_idx ]\n",
    "test_labels_binary = labels_binary[ test_set_idx ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training - Train Document Classification Model\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier we are using in the example is LogisticRegression. As we saw in the Machine Learning tutorial, first we decide on a classifier, then we fit the classifier to the data to create a model. We can then test our model on the test set by passing the features (*X*) from our test set to get predicted labels. The model will output the probability of each document being classified as income or health. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create our LogisticRegression classifier.\n",
    "clf = LogisticRegression(penalty='l1')\n",
    "\n",
    "# train the classifer to create our model.\n",
    "mdl = clf.fit( train_tfidf, labels_binary[ train_set_idx ] )\n",
    "\n",
    "# create scores for each of the documents predicting whether each refers to \n",
    "#     an income or health agency\n",
    "y_score = mdl.predict_proba( test_tfidf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation - Precision and Recall\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we have calculated a score for each of our facility types of interest, we look at how well our model performed by outputting precision and recall curves at different cutoffs.\n",
    "\n",
    "First, we define the function that will do this work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_n( y_true, y_prob, model_name ):\n",
    "\n",
    "    \"\"\"\n",
    "    y_true: ls\n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    \n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we output the graphs for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " plot_precision_recall_n( labels_binary[ test_set_idx ], y_score[:,1], 'LR' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine our precision-recall curve we can see that our precision is 1 up to 25 percent of the population. We can use a \"precision at *k*\" curve to see what percent of the corpus can be tagged by the classifier, and which should undergo a manual clerical review. Based on this curve, we might say that we can use our classifier to tag the 25% of the documents that have the highest scores as 1, and manually review the rest. \n",
    "\n",
    "Alternatively, we can try to maximize the entire precision-recall space. In this case we need a different metric - \"Area Under Curve\" (AUC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score[:,1])\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_precision_recall(labels_binary[test_set_idx],y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The AUC shows how accurate our scores are under different cutoff thresholds. The model will output a score between 0 and 1. We specify a  range of cutoff values and label all of the examples as 0 or 1 based on whether they are above or below each cutoff value. The closer our scores are to the true values, the more resilient they are to different cutoffs. For instance, if our scores were perfect, our AUC would be 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation - Feature Importances\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Next, we look at the importance of different features (words) in our model.\n",
    "\n",
    "The function that will calculate these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_feature_importances( coef, features, labels, num_features = 10 ):\n",
    "\n",
    "    \"\"\"\n",
    "    output feature importances\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coef: numpy\n",
    "        feature importances\n",
    "    features: ls \n",
    "        feature names\n",
    "    labels: ls\n",
    "        labels for the classifier\n",
    "    num_features: int\n",
    "        number of features to output (default 10)\n",
    "    \n",
    "    Example\n",
    "    --------\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    coef = mdl.coef_.ravel()\n",
    "\n",
    "    dict_feature_importances = dict( zip(features, coef) )\n",
    "    orddict_feature_importances = OrderedDict( \n",
    "                                    sorted(dict_feature_importances.items(), key=lambda x: x[1]) )\n",
    "\n",
    "    ls_sorted_features  = list(orddict_feature_importances.keys())\n",
    "\n",
    "    label0_features = ls_sorted_features[:num_features] \n",
    "    label1_features = ls_sorted_features[-num_features:] \n",
    "\n",
    "    print(labels[0],label0_features)\n",
    "    print(labels[1], label1_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_feature_importances(mdl.coef_.ravel(), features, ['health','income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importances give us the words which are the most relevant for distinguishing the type of social service agency (between income and health). Some of these make sense (\"city church\" seems more likely to be health than income), but some don't make as much sense, or seem to be artifacts from the website that we should remove (\"housing humancarelogo\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Exercise 6 - interpreting feature importances\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Display the top 25 feature importances to get an intution of which words are the most and least important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know how to pass into the function we want the top 25 feature importances. We can do this by consulting the docstring of the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this docstring we can see that `num_features` is a keyword argument that is set to 10 by default. We can pass `num_features=25` into the keyword argument instead to get the top 25 feature importances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_feature_importances(mdl.coef_.ravel(), \n",
    "                            features,\n",
    "                            ['health','income'],\n",
    "                            num_features=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation - Cross-validation\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Recall from the machine learning tutorial that we are seeking the find the most general pattern in the data in order to have to most general model that will be successful at classifying new unseen data. Our previous strategy above was the *Out-of-sample and holdout set*. With this strategy we try to find a general pattern by randomly dividing our data into a test and training set based on some percentage split (e.g., 50-50 or 80-20). We train on the test set and evaluate on the test set, where we pretend that we don't have the labels for the test set. A significant drawback with this approach is that we may be lucky or unlucky with our random split, and so our estimate of how we'd perform on truly new data is overly optimistic or overly pessimistic. A possible solution is to create many random splits into training and testing sets and evaluate each split to estimate the performance of a given model. \n",
    "\n",
    "A more sophisticated holdout training and testing procedure is *cross-validation*. In cross-validation we split our data into *k* folds or partitions, where *k* is usually 5 or 10. We then iterate k times. In each iteration, one of the folds is used as a test set, and the rest of the folds are combined to form the training set. We can then evaluate the performance at each iteration to estimate the performance of a given method. An advantage of using cross-validation is all examples of data are used in the training set at least once. \n",
    "\n",
    "Define function to create test and train bags of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_test_train_bag_of_words(train_corpus, test_corpus):\n",
    "\n",
    "    \"\"\"\n",
    "    Create test and training set bag of words\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_corpus: ls\n",
    "        ls of raw text for text corpus.\n",
    "    test_corpus: ls\n",
    "        ls of raw text for train corpus. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    (train_bag_of_words,test_bag_of_words): scipy sparse matrix\n",
    "        bag-of-words representation of train and test corpus\n",
    "    features: ls\n",
    "        ls of words used as features. \n",
    "    \"\"\"\n",
    "\n",
    "    #parameters for vectorizer \n",
    "    ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "    STRIP_ACCENTS = 'unicode'\n",
    "    TOKENIZER = None\n",
    "    NGRAM_RANGE = (0,2) #Range for pharases of words\n",
    "    MIN_DF = 0.01 # Exclude words that have a frequency less than the threshold\n",
    "    MAX_DF = 0.8  # Exclude words that have a frequency greater then the threshold \n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                                tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                                ngram_range=NGRAM_RANGE,\n",
    "                                stop_words = stopwords.words('english'),\n",
    "                                strip_accents=STRIP_ACCENTS,\n",
    "                                min_df = MIN_DF,\n",
    "                                max_df = MAX_DF)\n",
    "    \n",
    "    NORM = None #turn on normalization flag\n",
    "    SMOOTH_IDF = True #prevents division by zero errors\n",
    "    SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "    USE_IDF = True #flag to control whether to use TFIDF\n",
    "\n",
    "    transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)\n",
    "\n",
    "    #get the bag-of-words from the vectorizer and\n",
    "    #then use TFIDF to limit the tokens found throughout the text \n",
    "    train_bag_of_words = vectorizer.fit_transform( train_corpus ) \n",
    "    test_bag_of_words = vectorizer.transform( test_corpus )\n",
    "    if USE_IDF:\n",
    "        train_tfidf = transformer.fit_transform(train_bag_of_words)\n",
    "        test_tfidf = transformer.transform(test_bag_of_words)\n",
    "    features = vectorizer.get_feature_names()\n",
    "\n",
    "    \n",
    "    return train_tfidf, test_tfidf, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, use scikit-learn's StratifiedKFold object to generate our folds (we'll do 3 here as an example), and then train and validate across all combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# create folds\n",
    "cv = StratifiedKFold( train_labels_binary, n_folds = 3 )\n",
    "\n",
    "# get our labels\n",
    "train_labels_binary = le.transform(train_labels)\n",
    "\n",
    "# for each fold, get rows specified as train and test, then\n",
    "#     train and test our model with it.\n",
    "for i, ( train, test ) in enumerate(cv):\n",
    "    \n",
    "    # break out train and test data.\n",
    "    cv_train = train_corpus[train]\n",
    "    cv_test = train_corpus[test]\n",
    "    \n",
    "    # create bags of words and get feature names\n",
    "    bag_of_words_train, bag_of_words_test, feature_names = create_test_train_bag_of_words(cv_train, \n",
    "                                                                                          cv_test)\n",
    "    # fit our model and then use it to predict values in test set.\n",
    "    probas_ = clf.fit(bag_of_words_train, \n",
    "                      train_labels_binary[train]).predict_proba(bag_of_words_test)\n",
    "    cv_test_labels = train_labels_binary[test]\n",
    "    \n",
    "    # draw precision and recall curve\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(cv_test_labels, probas_[:,1] )\n",
    "\n",
    "    # calculate and plot AUC\n",
    "    auc_val = auc( recall_curve,precision_curve )\n",
    "    plt.plot( recall_curve, precision_curve, label = 'AUC-PR {0} {1:.2f}'.format( i, auc_val ) )\n",
    "\n",
    "#-- END loop over folds --#\n",
    "                                                                          \n",
    "# and, plot the collected graphs.\n",
    "plt.ylim( 0, 1.05 )    \n",
    "plt.xlabel( 'Recall' )\n",
    "plt.ylabel( 'Precision' )\n",
    "plt.legend( loc = \"lower left\", fontsize = 'x-small' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we did 3-fold cross-validation and plotted precision-recall curves for each iteration. You can see that there is a marked difference between the iterations. We can then average the AUC-PR of each iteration to estimate the performance of our method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 7 - Try a 5-fold cross-validation\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Try 5-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "cv = StratifiedKFold(train_labels_binary, n_folds=5)\n",
    "train_labels_binary = le.transform(train_labels)\n",
    "for i, (train,test) in enumerate(cv):\n",
    "    cv_train = train_corpus[train]\n",
    "    cv_test = train_corpus[test]\n",
    "    bag_of_words_train, bag_of_words_test, feature_names = create_test_train_bag_of_words(cv_train, \n",
    "                                                                                          cv_test)\n",
    "    \n",
    "    probas_ = clf.fit(bag_of_words_train, \n",
    "                      train_labels_binary[train]).predict_proba(bag_of_words_test)\n",
    "    cv_test_labels = train_labels_binary[test]\n",
    "    \n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(cv_test_labels,\n",
    "                                                                          probas_[:,1])\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    plt.plot(recall_curve, precision_curve, label='AUC-PR {0} {1:.2f}'.format(i,auc_val))\n",
    "    \n",
    "plt.ylim(0,1.05)    \n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc=\"lower left\", fontsize='x-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Output - Examples of Document Classification\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, look at the text the model is most certain of being categorized as each facility type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_comments = 2\n",
    "label0_comment_idx = y_score[:,1].argsort()[:num_comments] \n",
    "label1_comment_idx = y_score[:,1].argsort()[-num_comments:]\n",
    "test_set_labels = labels[test_set_idx]\n",
    "#convert back to the indices of the original dataset\n",
    "top_comments_testing_set_idx = np.concatenate([label0_comment_idx, \n",
    "                                               label1_comment_idx])\n",
    "\n",
    "\n",
    "#these are the 5 comments the model is most sure of \n",
    "for i in top_comments_testing_set_idx:\n",
    "    print(\n",
    "        u\"\"\"{}:{}\\n---\\n{}\\n===\"\"\".format(test_set_labels[i],\n",
    "                                          y_score[i,1],\n",
    "                                          test_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top 2 examples that the model is the most sure of for each label. We can see our important feature words in the descriptions, which gives a hint of how the model made these classifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further Resources\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great resource for NLP in Python is \n",
    "[Natural Language Processing with Python](https://www.amazon.com/Natural-Language-Processing-Python-Analyzing/dp/0596516495)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
