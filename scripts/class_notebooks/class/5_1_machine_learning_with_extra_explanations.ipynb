{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#Introduction)\n",
    "- [Glossary of Terms](#Glossary-of-Terms)\n",
    "- [Setup](#Setup)\n",
    "- [The Machine Learning Process](#The-Machine-Learning-Process)\n",
    "- [Problem Formulation](#Problem-Formulation)\n",
    "- [Label Generation](#Creating-Labels)\n",
    "- [Feature Generation](#Feature-Generation)\n",
    "- [Model Fitting](#Model-Fitting)\n",
    "- [Model Evaluation](#Model-Evaluation)\n",
    "- [Machine Learning Pipeline](#Machine-Learning-Pipeline)\n",
    "- [Survey of Algorithms](#Survey-of-Algorithms)\n",
    "- [Assess Model Against Baselines](#Assess-Model-Against-Baselines)\n",
    "- [Exercise](#Exercise)\n",
    "- [Resources](#Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this tutorial, we'll discuss how to formulate a research question in the machine learning framework; how to transform raw data into something that can be fed into a model; how to build, evaluate, compare, and select models; and how to reasonably and accurately interpret model results. You'll also get hands-on experience using the `scikit-learn` package in Python to model the data you're familiar with from previous tutorials. \n",
    "\n",
    "\n",
    "This tutorial is based on chapter 6 of [Big Data and Social Science](https://github.com/BigDataSocialScience/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossary of Terms\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "**Glossary of Terms:**\n",
    "\n",
    "- **Learning**: In machine learning, you'll hear about \"learning a model.\" This is what you probably know as \n",
    "*fitting* or *estimating* a function, or *training* or *building* a model. These terms are all synonyms and are \n",
    "used interchangeably in the machine learning literature.\n",
    "- **Examples**: These are what you probably know as *data points* or *observations*. \n",
    "- **Features**: These are what you probably know as *independent variables*, *attributes*, *predictors*, \n",
    "or *explanatory variables.*\n",
    "- **Underfitting**: This happens when a model is too simple and does not capture the structure of the data well \n",
    "enough.\n",
    "- **Overfitting**: This happens when a model is too complex or too sensitive to the noise in the data; this can\n",
    "result in poor generalization performance, or applicability of the model to new data. \n",
    "- **Regularization**: This is a general method to avoid overfitting by applying additional constraints to the model. \n",
    "For example, you can limit the number of features present in the final model, or the weight coefficients applied\n",
    "to the (standardized) features are small.\n",
    "- **Supervised learning** involves problems with one target or outcome variable (continuous or discrete) that we want\n",
    "to predict, or classify data into. Classification, prediction, and regression fall into this category. We call the\n",
    "set of explanatory variables $X$ **features**, and the outcome variable of interest $Y$ the **label**.\n",
    "- **Unsupervised learning** involves problems that do not have a specific outcome variable of interest, but rather\n",
    "we are looking to understand \"natural\" patterns or groupings in the data - looking to uncover some structure that \n",
    "we do not know about a priori. Clustering is the most common example of unsupervised learning, another example is \n",
    "principal components analysis (PCA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "---\n",
    "*[Back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "Before we begin, run the code cell below to initialize the libraries we'll be using in this assignment. We're already familiar with `numpy`, `pandas`, and `psycopg2` from previous tutorials. Here we'll also be using [`scikit-learn`](http://scikit-learn.org) to fit modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from __future__ import division \n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve,roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sqlalchemy import create_engine\n",
    "#import pydotplus\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\":1.25, \"lines.markersize\":8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_name = \"appliedda\"\n",
    "hostname = \"10.10.2.10\"\n",
    "conn = psycopg2.connect(database=db_name, host = hostname) #database connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database connection allows us to make queries to a database from Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tables = pd.read_sql(\"\"\"SELECT * FROM ides.il_wage limit 10;\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tables.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Machine Learning Process\n",
    "*[Go back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "- [**Understand the problem and goal.**](#problem-formulation) *This sounds obvious but is often nontrivial.* Problems typically start as vague \n",
    "descriptions of a goal - improving health outcomes, increasing graduation rates, understanding the effect of a \n",
    "variable *X* on an outcome *Y*, etc. It is really important to work with people who understand the domain being\n",
    "studied to dig deeper and define the problem more concretely. What is the analytical formulation of the metric \n",
    "that you are trying to optimize?\n",
    "- [**Formulate it as a machine learning problem.**](#problem-formulation) Is it a classification problem or a regression problem? Is the \n",
    "goal to build a model that generates a ranked list prioritized by risk, or is it to detect anomalies as new data \n",
    "come in? Knowing what kinds of tasks machine learning can solve will allow you to map the problem you are working on\n",
    "to one or more machine learning settings and give you access to a suite of methods.\n",
    "- **Data exploration and preparation.** Next, you need to carefully explore the data you have. What additional data\n",
    "do you need or have access to? What variable will you use to match records for integrating different data sources?\n",
    "What variables exist in the data set? Are they continuous or categorical? What about missing values? Can you use the \n",
    "variables in their original form, or do you need to alter them in some way?\n",
    "- [**Feature engineering.**](#feature-generation) In machine learning language, what you might know as independent variables or predictors \n",
    "or factors or covariates are called \"features.\" Creating good features is probably the most important step in the \n",
    "machine learning process. This involves doing transformations, creating interaction terms, or aggregating over data\n",
    "points or over time and space.\n",
    "- **Method selection.** Having formulated the problem and created your features, you now have a suite of methods to\n",
    "choose from. It would be great if there were a single method that always worked best for a specific type of problem. Typically, in machine learning, you take a variety of methods and try them, empirically validating which one is the best approach to your problem.\n",
    "- [**Evaluation.**](#evaluation) As you build a large number of possible models, you need a way choose the best among them. We'll cover methodology to validate models on historical data and discuss a variety of evaluation metrics. The next step is to validate using a field trial or experiment.\n",
    "- [**Deployment.**](#deployment) Once you have selected the best model and validated it using historical data as well as a field\n",
    "trial, you are ready to put the model into practice. You still have to keep in mind that new data will be coming in,\n",
    "and the model might change over time.\n",
    "\n",
    "\n",
    "\n",
    "You're probably used to fitting models in physical or social science classes. In those cases, you probably had a hypothesis or theory about the underlying process that gave rise to your data, chose an appropriate model based on prior knowledge and fit it using least squares, and used the resulting parameter or coefficient estimates (or confidence intervals) for inference. This type of modeling is very useful for *interpretation*.\n",
    "\n",
    "In machine learning, our primary concern is *generalization*. This means that:\n",
    "- **We care less about the structure of the model and more about the performance** This means that we'll try out a whole bunch of models at a time and choose the one that works best, rather than determining which model to use ahead of time. We can then choose to select a *suboptimal* model if we care about a specific model type. \n",
    "- **We don't (necessarily) want the model that best fits the data we've *already seen*,** but rather the model that will perform the best on *new data*. This means that we won't gauge our model's performance using the same data that we used to fit the model (e.g., sum of squared errors or $R^2$), and that \"best fit\" or accuracy will most often *not* determine the best model.  \n",
    "- **We can include a lot of variables in to the model.** This may sound like the complete opposite of what you've heard in the past, and it can be hard to swallow. But we will use different methods to deal with many of those concerns in the model fitting process by using a more automatic variable selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "*[Go back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "First, turning something into a real objective function. What do you care about? Do you have data on that thing? What action can you take based on your findings? Do you risk introducing any bias based on the way you model something? \n",
    "\n",
    "## Four Main Types of ML Tasks for Policy Problems\n",
    "- **Description**: [How can we identify and respond to the most urgent online government petitions?](https://dssg.uchicago.edu/project/improving-government-response-to-citizen-requests-online/)\n",
    "- **Prediction**: [Which students will struggle academically by third grade?](https://dssg.uchicago.edu/project/predicting-students-that-will-struggle-academically-by-third-grade/)\n",
    "- **Detection**: [Which police officers are likely to have an adverse interaction with the public?](https://dssg.uchicago.edu/project/expanding-our-early-intervention-system-for-adverse-police-interactions/)\n",
    "- **Behavior Change**: [How can we prevent juveniles from interacting with the criminal justice system?](https://dssg.uchicago.edu/project/preventing-juvenile-interactions-with-the-criminal-justice-system/)\n",
    "  \n",
    "## Our Machine Learning Problem\n",
    ">Of all the head of households that have been off of government assistance for one month, who is likely to need assistance in the >next year. This is an example of a *binary prediction classification problem*.\n",
    "\n",
    "\n",
    "Note the outcome window of 1 year(s) is completely arbitrary. You could use a window of 5, 3, 1 years or 1 day. The outcome window will depend on how often you receive new data -- there is no sense in making the same predictions on the same data -- how accurate your predictions are for a given time period or on what time-scale you can use the output of the data. \n",
    "\n",
    "# Data Exploration and Preparation. \n",
    "\n",
    "We have already explored the data in the first module and database modules. In order to predict whether someone will need benefits, we will be using data from the `idhs.hh_member` , `idhs.hh_member_info`, and `idhs.hh_indcase_spells` table to create **labels** and **features**. \n",
    "\n",
    "\n",
    "## Building a Model\n",
    "\n",
    "We need to munge our dataset into **features** (predictors, or independent variables, or $X$ variables) and **labels** (dependent variables, or $Y$ variables).  For ease of reference, in subsequent examples, names of variables that pertain to predictors will start with \"`X_`\", and names of variables that pertain to outcome variables will start with \"`y_`\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Labels\n",
    "\n",
    "Labels are the dependent variables, or *Y* variables, that we are trying to predict. In the machine learning framework, your labels are usually *binary*: true or false, encoded as 1 or 0. In this case, our label is whether a person will likely need assistance in the future. Let's pick a day to make our prediction, `2007-01-01`. We will look back two years into the past for everyone who received assistance from `2005-2006` but did not receive assistance in the last month of  `2006`. \n",
    "\n",
    "We can write SQL code in `psql`, `dbeaver`, `pgAdmin`, or programmaticaly generate the SQL and pass to the DB using `psycopg2` to create the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_labels(date_of_prediction,\n",
    "                  conn,\n",
    "                  past_days=730,\n",
    "                  off_benefit_days=30,\n",
    "                  prediction_horizon=365,\n",
    "                  schema='ada_class3',\n",
    "                  overwrite=False):\n",
    "    \"\"\"\n",
    "    Generate a list of labels and return the \n",
    "    table as a dataframe.\n",
    "    Parameters\n",
    "    ----------\n",
    "    date_of_prediction: str\n",
    "        string for the day predictions are made on '2006-01-01'\n",
    "    past_days: int\n",
    "        number of days we are looking into the past to see how\n",
    "        long people have been on benefits\n",
    "    off_benefit_days: int\n",
    "        amount of days in the past someone does not have assistance\n",
    "    overwrite: bool\n",
    "        if True runs the query if table does\n",
    "        not exist\n",
    "    schema: str\n",
    "        name of the schema tables will be written to\n",
    "    conn: obj\n",
    "        psycopg2 conection object to database\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_labels: DataFrame\n",
    "        Dataframe of labels\n",
    "    \"\"\"\n",
    "    \n",
    "    table_date = date_of_prediction.replace('-','')\n",
    "    \n",
    "    # check if the table you're trying to create already exists\n",
    "    cursor = conn.cursor()\n",
    "    query = \"\"\"\n",
    "            select * from information_schema.tables \n",
    "            where table_name=\\'binary_label_{table_date}\\'\n",
    "            and table_schema=\\'{schema}\\';\n",
    "            \"\"\".format(table_date=table_date, schema=schema);\n",
    "    cursor.execute(query) \n",
    "    \n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "        print('generating labels')\n",
    "        sql_script=\"\"\"---------------------------------------------------------------------\n",
    "--LABELS--------------------------------------------------------------\n",
    "----------------------------------------------------------------------\n",
    "/*\n",
    "Create a label. Predict who is likely to go back on \n",
    "benefits after being off for at least one year within\n",
    "the next year.\n",
    "*/\n",
    "\n",
    "--fields\n",
    "--{date_of_prediction}: day the prediction is being made\n",
    "--{past_days}: number of days into the past of people receiving\n",
    "--             benefits\n",
    "--{off_benefit_days}: number of days off benefits from day or prediction\n",
    "--{prediction_horizon}: number of days into the future we are making a prediction\n",
    "--{table_date}: date for the table\n",
    "\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS {schema}.hh_indcase_spells_before_{table_date}; \n",
    "CREATE TABLE {schema}.hh_indcase_spells_before_{table_date} AS\n",
    "SELECT * \n",
    "FROM idhs.hh_indcase_spells\n",
    "WHERE start_date >= ('{date_of_prediction}'::date - {past_days}) \n",
    "and end_date < ('{date_of_prediction}'::date - {off_benefit_days});\n",
    "\n",
    "COMMIT;\n",
    "\n",
    "--find all the records from 2006\n",
    "DROP TABLE IF EXISTS hh_indcase_spells_{table_date};\n",
    "CREATE TEMP TABLE hh_indcase_spells_{table_date} AS\n",
    "SELECT *\n",
    "FROM idhs.hh_indcase_spells\n",
    "WHERE start_date >= ('{date_of_prediction}'::date - {off_benefit_days}) \n",
    "and end_date < '{date_of_prediction}'::date; \n",
    "\n",
    "COMMIT;\n",
    "\n",
    "--grab all the people that are in the first table that are \n",
    "--not in the second\n",
    "DROP TABLE IF EXISTS hh_before_{table_date}_recptno;\n",
    "CREATE TEMP TABLE hh_before_{table_date}_recptno AS\n",
    "SELECT DISTINCT(recptno)\n",
    "FROM {schema}.hh_indcase_spells_before_{table_date}\n",
    "WHERE recptno NOT IN (\n",
    "\tSELECT DISTINCT(recptno)\n",
    "\tFROM hh_indcase_spells_{table_date});  \n",
    "\n",
    "COMMIT;\n",
    "\n",
    "--grab the list of cases during 2006-2006\n",
    "DROP TABLE IF EXISTS hh_after_{table_date};\n",
    "CREATE TEMP TABLE hh_after_{table_date} AS\n",
    "SELECT *\t\n",
    "FROM idhs.hh_indcase_spells\n",
    "WHERE start_date >= '{date_of_prediction}'::date \n",
    "AND end_date <= ('{date_of_prediction}'::date + {prediction_horizon});\n",
    "\n",
    "COMMIT;\n",
    "\n",
    "-- \n",
    "DROP TABLE IF EXISTS label_{table_date};\n",
    "CREATE TEMP TABLE label_{table_date} AS\n",
    "SELECT a.recptno, b.benefit_type, b.start_date, b.end_date\n",
    "FROM hh_before_{table_date}_recptno a\n",
    "LEFT JOIN hh_after_{table_date} b ON a.recptno = b.recptno;\n",
    "\n",
    "COMMIT;\n",
    "\n",
    "-- turn into binary labels\n",
    "DROP TABLE IF EXISTS pre_binary_label_{table_date};\n",
    "CREATE TEMP TABLE pre_binary_label_{table_date} AS\n",
    "SELECT recptno,\n",
    "case when benefit_type is null then 0 else 1 end benefits\n",
    "FROM label_{table_date};\n",
    "\n",
    "COMMIT;\n",
    "\n",
    "DROP TABLE IF EXISTS {schema}.binary_label_{table_date};\n",
    "CREATE TABLE {schema}.binary_label_{table_date} AS\n",
    "SELECT DISTINCT recptno, benefits\n",
    "FROM pre_binary_label_{table_date};\n",
    "\n",
    "COMMIT;\n",
    "-----------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "        \"\"\".format(date_of_prediction=date_of_prediction,\n",
    "                   past_days=past_days,\n",
    "                   off_benefit_days=off_benefit_days,\n",
    "                   prediction_horizon=prediction_horizon,\n",
    "                   table_date=table_date,\n",
    "                   schema=schema)\n",
    "    \n",
    "        cursor.execute(sql_script)\n",
    "    else:\n",
    "        print('Table already generated')\n",
    "    \n",
    "    cursor.close()\n",
    "    df_label = pd.read_sql('select * from {schema}.binary_label_{table_date};'.format(table_date=table_date,\n",
    "                                                                                     schema=schema), conn)\n",
    "    \n",
    "    return df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_label_2007 = create_labels('2007-01-01',\n",
    "                              conn,\n",
    "                              overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_label_2008 = create_labels('2008-01-01',\n",
    "                              conn,\n",
    "                              overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_label_2007.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a label: 0 indicates *did not need assistance for a year*, 1 indicates that person did receive assistance in one year (in our case 2005). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation\n",
    "*[Go back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "\n",
    "Our features are our independent variables or predictors. Good features make machine learning systems effective. \n",
    "The better the features the easier it is the capture the structure of the data. You generate features using domain knowledge. In general, it is better to have more complex features and a simpler model rather than vice versa. Keeping the model simple makes it faster to train and easier to understand rather then extensively searching for the \"right\" model and \"right\" set of parameters. \n",
    "\n",
    "Machine Learning Algorithms learn a solution to a problem from sample data. The set of features is the best representation of the sample data to learn a solution to a problem. \n",
    "\n",
    "- **Feature engineering** is the process of transforming raw data into features that better represent the underlying problem/data/structure  to the predictive models, resulting in improved model accuracy on unseen data.\" ( from [Discover Feature Engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) ).  In text, for example, this might involve deriving traits of the text like word counts, verb counts, or topics to feed into a model rather than simply giving it the raw text.\n",
    "\n",
    "Example of feature engineering are: \n",
    "\n",
    "- **Transformations**, such a log, square, and square root.\n",
    "- **Dummy (binary) variables**, also known as *indicator variables*, often done by taking categorical variables\n",
    "(such as city) which do not have a numerical value, and adding them to models as a binary value.\n",
    "- **Discretization**. Several methods require features to be discrete instead of continuous. This is often done \n",
    "by binning, which you can do by equal width. \n",
    "- **Aggregation.** Aggregate features often constitute the majority of features for a given problem. These use \n",
    "different aggregation functions (*count, min, max, average, standard deviation, etc.*) which summarize several\n",
    "values into one feature, aggregating over varying windows of time and space. For example, given urban data, \n",
    "we would want to calculate the *number* (and *min, max, mean, variance*, etc.) of crimes within an *m*-mile radius\n",
    "of an address in the past *t* months for varying values of *m* and *t*, and then use all of them as features.\n",
    "\n",
    "Our preliminary features are the following\n",
    "\n",
    "- `n_spells` (Aggregation): Total number of spells someonse has had up until the date of prediction.\n",
    "\n",
    "- `age` (Transformation): The age feature is created by substracting the bdate_year with the current year of prediction. \n",
    "\n",
    "- `edlevel` (Binary): 0 if the person has less than a high school education and 1 if they are more than a high school education. \n",
    "\n",
    "- `workexp` (Binary): 0 if no work experience 1 if there is some sort of work experience\n",
    "\n",
    "- `married` (Binary): 1 if the person is married 0 if they are not. \n",
    "\n",
    "- `gender`: (Binary) 1(male) 2(female)\n",
    "\n",
    "- `n_days_last_spell`: (Aggregation) The number of days since a person's last spell.\n",
    "\n",
    "- `(foodstamp, tanf, granf)`: (Binary) 0 if the last benefit was not foodstamp, tanf or grantf, 1 if it was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(date_of_prediction, conn, past_days=7200,\n",
    "                    schema='ada_class3', overwrite=False):\n",
    "    \"\"\"\n",
    "    Generate a list of features and return the \n",
    "    table as a dataframe.\n",
    "    \n",
    "    Note: There has to be a table of labels that\n",
    "    correspond with the same time period. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    date_of_prediction: str\n",
    "        date to make prediction from e.g., '2006-01-01'\n",
    "    conn: obj\n",
    "        psycopg2 conection object to database\n",
    "    past_days: int\n",
    "        number of days to look into the past for the\n",
    "        wage feature\n",
    "    schema: str\n",
    "        schema to write tables into\n",
    "    overwrite: bool\n",
    "        If True will run SQL script if tables\n",
    "        do not exist. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    table_name: str\n",
    "        name of table with features\n",
    "    \"\"\"\n",
    "    table_date = date_of_prediction.replace('-','')\n",
    "        \n",
    "    cursor = conn.cursor()\n",
    "    query = \"\"\"select * from information_schema.tables \n",
    "            where table_name=\\'feature_table_{table_date}\\'\n",
    "            and table_schema=\\'{schema}\\';\"\"\".format(table_date=table_date, schema=schema)\n",
    "    print(query)\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "\n",
    "    \n",
    "        sql_script=\"\"\"\n",
    "        -----------------------------------------------------------------------\n",
    "--FEATURE CREATION-----------------------------------------------------\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "--number of individual spells\n",
    "--find how many records before the end_date\n",
    "-- group by ssn_hash\n",
    "drop table if exists feature_n_spells_{table_date};\n",
    "create temp table feature_n_spells_{table_date} as \n",
    "select recptno, count(*) n_spells\n",
    "from idhs.hh_indcase_spells\n",
    "where end_date <= '{date_of_prediction}'::date\n",
    "and recptno in (\n",
    "\tselect distinct(recptno)\n",
    "\tfrom {schema}.binary_label_{table_date})\n",
    "group by recptno; \n",
    "\n",
    "commit;\n",
    "\n",
    "-- age \n",
    "drop table if exists feature_age_{table_date};\n",
    "create temp table feature_age_{table_date} as\n",
    "select recptno, \n",
    "\t(date_part('year','{date_of_prediction}'::date)-bdate_year) age\n",
    "from idhs.hh_member\n",
    "where recptno in (\n",
    "select distinct(recptno) from {schema}.binary_label_{table_date}); \n",
    "\n",
    "commit; \n",
    "\n",
    "-- marstat, edlvel, workexp\n",
    "\n",
    "drop table if exists last_case_before_{table_date};\n",
    "create temp table last_case_before_{table_date} as\n",
    "select distinct on (\"recptno\") \trecptno,\n",
    "\t\t\t\tch_dpa_caseid,\n",
    "\t\t\t        start_date,\n",
    "\t\t\t        end_date,\n",
    "\t\t\t\t(end_date::date - start_date::date) n_days_spell,\n",
    "\t\t\t\tbenefit_type\n",
    "from {schema}.hh_indcase_spells_before_{table_date}\n",
    "where recptno in (\n",
    "select distinct(recptno) from {schema}.binary_label_{table_date})\n",
    "order by recptno, end_date desc;\n",
    "\n",
    "commit; \n",
    "\n",
    "drop table if exists pre_categorical_features;\n",
    "create temp table pre_categorical_features as \n",
    "select \tc.recptno,\n",
    "\tc.n_days_spell,\n",
    "case when c.edlevel in ('A', 'B', 'C', 'D', 'E', 'F','1','2','3','4') then 0 \n",
    "     when c.edlevel is NULL then 0 \n",
    "     else 1 end edlevel,\n",
    "case when c.martlst in (0,1,3,4,5,6) then 0 else 1 end marstat,\n",
    "case when c.workexp in ('0','1') then 0 else 1 end workexp,\n",
    "case when c.benefit_type = 'foodstamp' then 1 else 0 end foodstamp,\n",
    "case when c.benefit_type = 'tanf46' then 1 else 0 end tanf,\n",
    "case when c.benefit_type = 'grant' then 1 else 0 end grantf,\n",
    "('{date_of_prediction}'::date - end_date::date) n_days_last_spell\n",
    "from (select a.recptno,\n",
    "       \tb.edlevel,\n",
    "\tb.workexp,\n",
    "\tb.martlst,\n",
    "\ta.end_date,\n",
    "\ta.n_days_spell,\n",
    "\ta.benefit_type\n",
    "from last_case_before_{table_date} a\n",
    "join idhs.member_info b\n",
    " on a.recptno = b.recptno and a.ch_dpa_caseid=b.ch_dpa_caseid) as c; \n",
    "-- how much money are they earning\n",
    "\n",
    "commit; \n",
    "\n",
    "-- gender\n",
    "drop table if exists feature_gender_{table_date};\n",
    "create temp table feature_gender_{table_date} as\n",
    "select recptno, sex gender\n",
    "from idhs.hh_member\n",
    "where recptno in (\n",
    "select distinct(recptno) from {schema}.binary_label_{table_date}); \n",
    "\n",
    "commit; \n",
    "\n",
    "--salary\n",
    "drop table if exists {schema}.recptno_ssn_{table_date}; \n",
    "create table {schema}.recptno_ssn_{table_date} as\n",
    "select a.recptno, b.ssn_hash\n",
    "from {schema}.binary_label_{table_date} a\n",
    "join idhs.hh_member b on a.recptno = b.recptno;\n",
    "\n",
    "commit; \n",
    "\n",
    "drop table if exists {schema}.wage_ssn_{table_date};\n",
    "create table {schema}.wage_ssn_{table_date} as\n",
    "select *\n",
    "from {schema}.il_wage_hh_recipient\n",
    "where ssn in ( \tselect distinct ssn_hash\n",
    "\tfrom {schema}.recptno_ssn_{table_date});\n",
    "\n",
    "commit; \n",
    "\n",
    "drop table if exists {schema}.feature_wage_{table_date};\n",
    "create table {schema}.feature_wage_{table_date} as  \n",
    "select \trecptno, \n",
    "\tsum(wage) total_wages,\n",
    "\t count(distinct(year,quarter)) n_quarters\n",
    "from {schema}.wage_ssn_{table_date}\n",
    "where year > date_part('year', timestamp '{date_of_prediction}'::date-{past_days}) \n",
    "and year < date_part('year', timestamp '{date_of_prediction}'::date)\n",
    "group by recptno;\n",
    "\n",
    "commit; \n",
    "\n",
    "--create feature table\n",
    "drop table if exists {schema}.feature_table_{table_date};\n",
    "create table {schema}.feature_table_{table_date} as \n",
    "select \ta.recptno,\n",
    "       \tb.n_spells,\n",
    "       \tc.age,\n",
    " \te.edlevel,\n",
    "\te.workexp,\n",
    "\te.marstat,\n",
    "\te.n_days_last_spell,\n",
    "\te.n_days_spell,\n",
    "\te.foodstamp,\n",
    "\te.tanf,\n",
    "\te.grantf,\n",
    "\td.gender,\n",
    "    case when f.total_wages is NULL then 0 else f.total_wages end total_wages,\n",
    "    case when f.n_quarters is NULL then 0 else f.n_quarters end n_quarters\n",
    "from {schema}.binary_label_{table_date} a\n",
    "left join feature_n_spells_{table_date} b on a.recptno=b.recptno\n",
    "left join feature_age_{table_date} c on a.recptno = c.recptno\n",
    "left join feature_gender_{table_date} d on a.recptno = d.recptno\n",
    "left join pre_categorical_features e on a.recptno = e.recptno\n",
    "left join {schema}.feature_wage_{table_date} f on a.recptno = f.recptno;\n",
    "\n",
    "commit; \n",
    "\n",
    "drop table if exists {schema}.set_{table_date};\n",
    "create table {schema}.set_{table_date} as \n",
    "select \ta.*,\n",
    "\tb.n_spells,\n",
    "\tb.age,\n",
    "\tb.edlevel,\n",
    "\tb.workexp,\n",
    "\tb.marstat,\n",
    "\tb.gender,\n",
    "\tb.n_days_last_spell,\n",
    "\tb.foodstamp,\n",
    "\tb.tanf,\n",
    "\tb.grantf,\n",
    "\tb.n_days_spell,\n",
    "    b.total_wages,\n",
    "    b.n_quarters\n",
    "from {schema}.binary_label_{table_date} a\n",
    "join {schema}.feature_table_{table_date} b on a.recptno=b.recptno;\n",
    "\n",
    "commit; \n",
    "\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "        \"\"\".format(date_of_prediction=date_of_prediction,\n",
    "                    table_date=table_date,\n",
    "                   past_days=past_days,\n",
    "                       schema=schema)\n",
    "    \n",
    "    \n",
    "        cursor.execute(sql_script)\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "    print('created {schema}.feature_table_{table_date}'.format(\n",
    "        table_date=table_date,\n",
    "        schema=schema))\n",
    "    \n",
    "    table_name = '{schema}.features_table_{table_date}'.format(schema=schema,\n",
    "                                                               table_date=table_date)\n",
    "    return table_name      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature_table = create_features('2007-01-01',conn, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_feature_table = create_features('2008-01-01',conn, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "*[Go back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "It's not enough to just build the model; we're going to need a way to know whether or not it's working. Convincing others of the quality of results is often the *most challenging* part of an analysis.  Making repeatable, well-documented work with clear success metrics makes all the difference.\n",
    "\n",
    "To convince ourselves - and others - that our modeling results will generalize, we need to hold\n",
    "some data back (not using it to train the model), then apply our model to that hold-out set and \"blindly\" predict, comparing the model's predictions to what we actually observed. This is called **cross-validation**, and it's the best way we have to estimate how a model will perform on *entirely* novel data. We call the data used to build the model the **training set**, and the rest the **test set**.\n",
    "\n",
    "In general, we'd like our training set to be as large as possible, to give our model more information. However, you also want to be as confident as possible that your model will be applicable to new data, or else the model is useless. In practice, you'll have to balance these two objectives in a reasonable way.  \n",
    "\n",
    "There are also many ways to split up your data into training and testing sets. Since you're trying to evaluate how your model will perform *in practice*, it's best to emulate the true use case of your model as closely as possible when you decide how to evaluate it. A good [tutorial on cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html) can be found on the `scikit-learn` site.\n",
    "\n",
    "One simple and commonly used method is ***k-fold* cross-validation**, which entails splitting up our dataset into *k* groups, holding out one group while training a model on the rest of the data, evaluating model performance on the held-out \"fold,\" and repeating this process *k* times (we'll get back to this in the text-analysis tutorial). Another method is **temporal validation**, which involves building a model using all the data up until a given point in time, and then testing the model on observations that happened after that point. Our problem of predicting whether someone will need government assistance is a problem in time where we are trying to predict an event in the future. Generally, if you use the future to predict the past there will be temporal effects that will help the accuracy of your predictions. We cannot use the future to predict the past in real life, so it is important to use `temporal validation` and create our training and test sets accordingly. \n",
    "\n",
    "Our training set uses labels from 2005. Starting from 2005 we found all people that received benefits from `01-2003` to `11-2004`.  The features are then developed using data from our day or prediction `2005-01-01`. *Note: it is important to segregate your data based on time when creating features. Otherwise there can be \"leakage,\" where you accidentally use information that you would not have known at the time.*  This happens often when calculating aggregation features; for instance, it is quite easy to calculate an average using values that go beyond our training set time-span and not realize it.  \n",
    "\n",
    "Our testing set will use labels for the following year 2006, and our features will be generated from 1989-2006. \n",
    "\n",
    "Notice that our testing set uses more data than our training set, because we have \"new\" data from 2005. This emulates the way our models could be used in practice. Every year we can run our model and make new predictions updating our dataset with the most recent five years of data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a matrix for training the models\n",
    "\n",
    "def create_train_or_test_matrix(date_of_prediction, conn, schema='ada_class3', overwrite=False):\n",
    "    \"\"\"\n",
    "    joins feature table with the labels table to generate a matrix\n",
    "      \n",
    "    _\n",
    "    Parameters\n",
    "    ----------\n",
    "    date_of_prediction: str\n",
    "        day to make prediction from '2005-01-01'\n",
    "    schema: str\n",
    "        schema to write the tables into\n",
    "    conn: obj\n",
    "        psycopg2 conection object to database\n",
    "    overwrite: bool\n",
    "        If True will run SQL script if tables\n",
    "        do not exist. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    table\n",
    "        table with features\n",
    "    \"\"\"\n",
    "        \n",
    "    table_date = date_of_prediction.replace('-','')\n",
    "        \n",
    "    cursor = conn.cursor()\n",
    "    query = \"\"\"\n",
    "            select * from information_schema.tables \n",
    "            where table_name=\\'set_{table_date}\\'\n",
    "            and table_schema=\\'{schema}\\';\n",
    "            \"\"\".format(table_date=table_date,\n",
    "                      schema=schema)\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "\n",
    "        sql_script=\"\"\"\n",
    "   drop table if exists class2.set_{table_date};\n",
    "create table class2.set_{table_date} as \n",
    "select \ta.*,\n",
    "\tb.n_spells,\n",
    "\tb.age,\n",
    "\tb.edlevel,\n",
    "\tb.workexp,\n",
    "\tb.marstat,\n",
    "\tb.gender,\n",
    "\tb.n_days_last_spell,\n",
    "\tb.foodstamp,\n",
    "\tb.tanf,\n",
    "\tb.grantf,\n",
    "\tb.n_days_spell,\n",
    "    b.total_wages,\n",
    "    b.n_quarters\n",
    "from class2.binary_label_{table_date} a\n",
    "join {schema}.feature_table_{table_date} b on a.recptno=b.recptno;\n",
    "\n",
    "    commit; \n",
    "\n",
    "    \"\"\".format(table_date=table_date,\n",
    "               schema=schema)\n",
    "    \n",
    "    \n",
    "        cursor.execute(sql_script)\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "    print('created {schema}.set_{table_date}'.format(\n",
    "        schema=schema, table_date=table_date))\n",
    "    \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_train_or_test_matrix('2007-01-01', conn, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_train_or_test_matrix('2008-01-01', conn, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training = pd.read_sql('select * from ada_class3.set_20070101;', conn)\n",
    "df_testing = pd.read_sql('select * from ada_class3.set_20080101;', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isnan_training_rows = df_training.isnull().any(axis=1) # Find the rows where there are NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training[isnan_training_rows].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No `NaNs` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrows_training = df_training.shape[0]\n",
    "nrows_training_isnan = df_training[isnan_training_rows].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('%of frows with NaNs {} '.format(float(nrows_training_isnan)/nrows_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training = df_training[~isnan_training_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation \n",
    "\n",
    "It is important to to do a quick check of our matrix to see if we have any outlier values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the values of the ages at see if they are reasonable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.unique( df_training['age'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! It is unlikely there are any pepole being born in the future receiving benefits in the past! This is likely due to an incorrect entry in the `birth_yr` in the `idhs.hh_member` table. On the other end of the age spectrum, the ages are more likely to be correct, but this is still something that you'd want to do a \"sanity check\" on with someone who knows the data well.\n",
    "\n",
    "Let's mark rows that have age less than 0 or age greater than 100 as NaN and then impute the age with the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = ( (df_training['age'] < 1) | (df_training['age'] > 100) )\n",
    "vals_to_replace = df_training[mask]['age'].values\n",
    "df_training['age'].replace(vals_to_replace,np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_training_age = df_training['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_training_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training['age'].fillna(mean_training_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training['age'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Balancing\n",
    "\n",
    "Let's check how much data we still have and how many examples of going back on benefits are in our training dataset. We don't necessarily need to have a perfect 50-50 balance of off-benefits/on-benefits, but it's good to know what the \"baseline\" is in our dataset, to be able to intelligently evaluate our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(df_training.shape[0]))\n",
    "df_training['benefits'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about N examples, and about X% of those are *positive* examples (needed assistance), which is what we're trying to identify. About Y% of the examples are *negative* examples (did not need assistance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isnan_testing_rows = df_testing.isnull().any(axis=1) # Find the rows where there are NaNs\n",
    "nrows_testing = df_testing.shape[0]\n",
    "nrows_testing_isnan = df_testing[isnan_testing_rows].shape[0]\n",
    "print('%of rows with NaNs {} '.format(float(nrows_testing_isnan)/nrows_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing[isnan_testing_rows].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = ( (df_testing['age'] < 1) | (df_testing['age'] > 100) )\n",
    "vals_to_replace = df_testing[mask]['age'].values\n",
    "df_testing['age'].replace(vals_to_replace,np.NaN, inplace=True)\n",
    "df_testing['age'].fillna(mean_training_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(df_testing.shape[0]))\n",
    "df_testing['benefits'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling of Values\n",
    "\n",
    "Certain models will have issue with the distance between features such as age and total earning. Age is typically a number between 0 and 100 while earnings can be number between 0 and 1000000. In order to circumvent this problem we can scale our features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_training_wage = df_training['total_wages'].min()\n",
    "max_training_wage = df_training['total_wages'].max()\n",
    "\n",
    "df_training['scaled_wages'] = (df_training['total_wages'] - min_training_wage)/(max_training_wage-min_training_wage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training[['scaled_wages','total_wages']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing['scaled_wages'] = (df_testing['total_wages'] - min_training_wage)/(max_training_wage-min_training_wage) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crosstabs\n",
    "\n",
    "We can use crosstabs to find trends and patterns in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(index=df_training['benefits'], columns=df_training['gender']).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(index=df_training['benefits'], columns=df_training['edlevel']).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(index=df_training['benefits'], columns=df_training['marstat']).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"benefits\", y=\"scaled_wages\", data=df_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"benefits\", y=\"n_days_last_spell\", data=df_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"benefits\", y=\"n_spells\", data=df_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"benefits\", y=\"age\", data=df_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_features = ['n_spells','age', 'edlevel','workexp','marstat','gender',\n",
    "                'n_days_last_spell', 'scaled_wages', 'n_quarters']\n",
    "sel_label = 'benefits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use conventions typically used in python scikitlearn\n",
    "\n",
    "X_train = df_training[sel_features].values\n",
    "y_train = df_training[sel_label].values\n",
    "X_test = df_testing[sel_features].values\n",
    "y_test = df_testing[sel_label].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "*[Go back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "In this phase, you take the predictors from your test set and apply your model to them, then assess the quality of the model by comparing the *predicted values* to the *actual values* for each record in your testing data set. \n",
    "\n",
    "- **Performance Estimation**: How well will our model do once it is deployed and applied to new data?\n",
    "\n",
    "Now let's use the model we just fit to make predictions on our test dataset, and see what our accuracy score is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's [`scikit-learn`](http://scikit-learn.org/stable/) is a commonly used, well documented Python library for machine learning. This library can help you split your data into training and test sets, fit models and use them to predict results on new data, and evaluate your results.\n",
    "\n",
    "We will start with the simplest [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model and see how well that does.\n",
    "\n",
    "You can use any number of metrics to judge your models (see [model evaluation](#model-evaluation)), but we'll use [`accuracy_score()`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) (ratio of correct predictions to total number of predictions) as our measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's fit a model\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression(penalty='l1', C=1e5)\n",
    "model.fit( X_train, y_train )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the model results, we see different parameters we can adjust as we refine the model based on running it against test data (values such as `intercept_scaling`, `max_iters`, `penalty`, and `solver`).  Example output:\n",
    "\n",
    "    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0)\n",
    "\n",
    "To adjust these parameters, one would alter the call that creates the `LogisticRegression()` model instance, passing it one or more of these parameters with a value other than the default.  So, to re-fit the model with `max_iter` of 1000, `intercept_scaling` of 2, and `solver` of \"lbfgs\" (pulled from thin air as an example), you'd create your model as follows:\n",
    "\n",
    "    model = LogisticRegression( max_iter = 1000, intercept_scaling = 2, solver = \"lbfgs\" )\n",
    "\n",
    "The basic way to choose values for, or \"tune,\" these parameters is the same as the way you choose a model: fit the model to your training data with a variety of parameters, and see which perform the best on the test set. An obvious drawback is that you can also *overfit* to your test set; in this case, you can alter your method of cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"The coefficients for each of the features are \" \n",
    "zip(sel_features, model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_coef = np.std(X_test,0)*model.coef_\n",
    "zip(sel_features, std_coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation \n",
    "\n",
    "Machine learning models usually do not produce a prediction (0 or 1) directly. Rather, models produce a score between 0 and 1 (that can sometimes be interpreted as a probability), which lets you more finely rank all of the examples from *most likely* to *least likely* to have label 1 (positive). This score is then turned into a 0 or 1 based on a user-specified threshold. For example, you might label all examples that have a score greater than 0.5 (1/2) as positive (1), but there's no reason that has to be the cutoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  from our \"predictors\" using the model.\n",
    "y_scores = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of scores and see if it makes sense to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(y_scores, kde=False, rug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing['y_score'] = y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing[['recptno', 'y_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools like `sklearn` often have a default threshold of 0.5, but a good threshold is selected based on the data, model and the specific problem you are solving. As a trial run, let's set a threshold of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calc_threshold = lambda x,y: 0 if x < y else 1 \n",
    "predicted = np.array( [calc_threshold(score,0.45) for score in y_scores] )\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Once we have tuned our scores to 0 or 1 for classification, we create a *confusion matrix*, which  has four cells: true negatives, true positives, false negatives, and false positives. Each data point belongs in one of these cells, because it has both a ground truth and a predicted label. If an example was predicted to be negative and is negative, it's a true negative. If an example was predicted to be positive and is positive, it's a true positive. If an example was predicted to be negative and is positive, it's a false negative. If an example was predicted to be positive and is negative, it's a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(expected,predicted)\n",
    "print conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of true negatives is `conf_matrix[0,0]`, false negatives `conf_matrix[1,0]`, true positives `conf_matrix[1,1]`, and false_positives `conf_matrix[0,1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the ratio of the correct predictions (both positive and negative) to all predictions. \n",
    "$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate an accuracy score by comparing expected to predicted.\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "print( \"Accuracy = \" + str( accuracy ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two additional metrics that are often used are **precision** and **recall**. \n",
    "\n",
    "Precision measures the accuracy of the classifier when it predicts an example to be positive. It is the ratio of correctly predicted positive examples to examples predicted to be positive. \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "Recall measures the accuracy of the classifier to find positive examples in the data. \n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "By selecting different thresholds we can vary and tune the precision and recall of a given classifier. A conservative classifier (threshold 0.99) will classify a case as 1 only when it is *very sure*, leading to high precision. On the other end of the spectrum, a low threshold (e.g. 0.01) will lead to higher recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted)\n",
    "print( \"Precision = \" + str( precision ) )\n",
    "print( \"Recall= \" + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we care about our whole precision-recall space, we can optimize for a metric known as the **area under the curve (AUC-PR)**, which is the area under the precision-recall curve. The maximum AUC-PR is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score)\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_precision_recall(expected, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall at k%\n",
    "\n",
    "If we only care about a specific part of the precision-recall curve we can focus on more fine-grained metrics. For instance, say there is a special program for people likely to need assistance within the next year , but only *3000 or 1% of the people in our test set*  can be admitted. In that case, we would want to prioritize the 1% who were *most likely* to need assistance within the next year, and it wouldn't matter too much how accurate we were on the 78% or so who weren't very likely to need assistane.\n",
    "\n",
    "Let's say that, out of the approximately 300,000 peoiple, we can intervene on 1% of them, or the \"top\" 3000 people in a year (where \"top\" means highest likelihood of needing assistance in the next year). We can then focus on optimizing our **precision at 1%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls \n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores,k):\n",
    "    \n",
    "    threshold = np.sort(y_scores)[::-1][int(k*len(y_scores))]\n",
    "    y_pred = np.asarray([1 if i >= threshold else 0 for i in y_scores ])\n",
    "    return precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_precision_recall_n(expected,y_scores, 'LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_at_1 = precision_at_k(expected,y_scores, 0.01)\n",
    "print('Precision at 1%: {:.2f}'.format(p_at_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "*[Go back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "When working on machine learning projects, it is a good idea to structure your code as a modular **pipeline**, which contains all of the steps of your analysis, from the original data source to the results that you report, along with documentation. This has many advantages:\n",
    "- **Reproducibility**. It's important that your work be reproducible. This means that someone else should be able\n",
    "to see what you did, follow the exact same process, and come up with the exact same results. It also means that\n",
    "someone else can follow the steps you took and see what decisions you made, whether that person is a collaborator, \n",
    "a reviewer for a journal, or the agency you are working with. \n",
    "- **Ease of model evaluation and comparison**.\n",
    "- **Ability to make changes.** If you receive new data and want to go through the process again, or if there are \n",
    "updates to the data you used, you can easily substitute new data and reproduce the process without starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey of Algorithms\n",
    "\n",
    "*[Go back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "We have only scratched the surface of what we can do with our model. We've only tried one classifier (Logistic Regression), and there are plenty more classification algorithms in `sklearn`. Let's try them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = {'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "       'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),\n",
    "        'LR': LogisticRegression(penalty='l1', C=1e5),\n",
    "        'SGD':SGDClassifier(loss='log'),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, random_state=17, n_estimators=10),\n",
    "        'NB': GaussianNB()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_clfs = ['RF', 'ET', 'LR', 'SGD', 'GB', 'NB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_p_at_k = 0\n",
    "df_results = pd.DataFrame()\n",
    "for clfNM in sel_clfs:\n",
    "    clf = clfs[clfNM]\n",
    "    clf.fit( X_train, y_train )\n",
    "    print clf\n",
    "    y_score = clf.predict_proba(X_test)[:,1]\n",
    "    predicted = np.array(y_score)\n",
    "    expected = np.array(y_test)\n",
    "    plot_precision_recall_n(expected,predicted, clfNM)\n",
    "    p_at_1 = precision_at_k(expected,y_score, 0.01)\n",
    "    p_at_5 = precision_at_k(expected,y_score,0.05)\n",
    "    p_at_10 = precision_at_k(expected,y_score,0.10)\n",
    "    fpr, tpr, thresholds = roc_curve(expected,y_score)\n",
    "    auc_val = auc(fpr,tpr)\n",
    "    df_results = df_results.append([{\n",
    "        'clfNM':clfNM,\n",
    "        'p_at_1':p_at_1,\n",
    "        'p_at_5':p_at_5,\n",
    "        'p_at_10':p_at_10,\n",
    "        'auc':auc_val,\n",
    "        'clf': clf\n",
    "    }])\n",
    "    \n",
    "    #feature importances\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        feature_import = dict(\n",
    "            zip(sel_features,clf.coef_.ravel()))\n",
    "    elif hasattr(clf, 'feature_importances_'):\n",
    "        feature_import = dict(\n",
    "            zip(sel_features, clf.feature_importances_))\n",
    "    print(\"FEATURE IMPORTANCES\")\n",
    "    print(feature_import)\n",
    "    \n",
    "    plt.clf()\n",
    "    sns.set_style('whitegrid')\n",
    "    f, ax = plt.subplots(figsize=(36,12))\n",
    "    sns.barplot(x=feature_import.keys(), y = feature_import.values(), palette=\"Blues\")\n",
    "    plt.show()\n",
    "    \n",
    "    if max_p_at_k < p_at_1:\n",
    "        max_p_at_k = p_at_1\n",
    "    print('Precision at 1%: {:.2f}'.format(p_at_1))\n",
    "df_results.to_csv('modelrun.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Model Against Baselines\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "It is important to check our model against a reasonable **baseline** to know how well our model is doing. Without any context, 78% accuracy can sound really great... but it's not so great when you remember that you could do almost that well by declaring everyone will not need benefits in the next year, which would be stupid (not to mention useless) model. \n",
    "\n",
    "A good place to start is checking against a *random* baseline, assigning every example a label (positive or negative) completely at random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_p_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_score = [random.uniform(0,1) for i in enumerate(y_test)] \n",
    "random_predicted = np.array( [calc_threshold(score,0.5) for score in random_score] )\n",
    "random_p_at_5 = precision_at_k(expected,random_predicted, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good practice is checking against an \"expert\" or rule of thumb baseline. For example, say that talking to people at the IDHS, you find that they think it's much more likely that someone who has been on assistance multiple times already will need assistance in the future. Then you should check that your classifier does better than just labeling everyone who has had multiple past admits as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reenter_predicted = np.array([ 1 if n_spells > 3 else 0 for n_spells in df_testing.n_spells.values ])\n",
    "reenter_p_at_1 = precision_at_k(expected,reenter_predicted,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_non_reenter = np.array([0 for n_spells in df_testing.n_spells.values])\n",
    "all_non_reenter_p_at_1 = precision_at_k(expected, all_non_reenter,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=2.25, rc={\"lines.linewidth\":2.25, \"lines.markersize\":8})\n",
    "fig, ax = plt.subplots(1, figsize=(22,12))\n",
    "sns.barplot(['Random','All no need', 'More than 3 Spell','Model'],\n",
    "            [random_p_at_5, all_non_reenter_p_at_1, reenter_p_at_1, max_p_at_k],\n",
    "            palette=['#6F777D','#6F777D','#6F777D','#800000'])\n",
    "sns.despine()\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('precision at 1%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Our model has just scratched the surface. Try the following: \n",
    "    \n",
    "- Create more features\n",
    "- Try more models\n",
    "- Try different parameters for your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "*[Go back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "- Hastie et al.'s [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) is a classic and is available online for free.\n",
    "- James et al.'s [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), also available online, includes less mathematics and is more approachable.\n",
    "- Wu et al.'s [Top 10 Algorithms in Data Mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
