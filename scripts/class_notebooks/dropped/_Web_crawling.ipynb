{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll use requesta and BeautifulSoup again in this tutorial:\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## We'll also use the re module for regular expressions.\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's look at this list of state universities in the US:\n",
    "top_url = 'https://en.wikipedia.org/wiki/List_of_state_universities_in_the_United_States'\n",
    "\n",
    "# Use requests.get to fetch the HTML at the specific url:\n",
    "response = requests.get(top_url)\n",
    "\n",
    "print(type(response))\n",
    "# This returns an object of type Response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And it contains all the HTML of the URL:\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the nested data object using the BeautifulSoup() function:\n",
    "soup = BeautifulSoup(response.content)\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The prettify method for making our output more readable.\n",
    "## The example below looks at the 50,000 - 51,000 characters in the scraped HTML: \n",
    "print(soup.prettify())[50000:51000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can use the find method to find the first tag (and its contents) of a certain type.\n",
    "soup.find(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring and Inspecting a Webpage\n",
    "\n",
    "Similar to the `find` method, we can use the `find_all` method to find all the tags of a certain type. But what tags are we looking for? We can look at the code for any individual part of an HTML page by clicking on it from within a browser and selecting `inspect`.\n",
    "![Inspecting an HTML Element](03-images/inspect.png)\n",
    "\n",
    "### Inspected Elements\n",
    "This will show you the underlying code that generates this element.\n",
    "\n",
    "![Results of Inspection](03-images/inspected.png)\n",
    "\n",
    "You can see that the links to the colleges are listed, meaning within `<li>` tags, as well as links, meaning within `<a>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This gets us somewhere, but there are links in here that are not colleges and some of the colleges do not have links.\n",
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Searching for <li> tags gets us closer, but there are still some non-universities in here.\n",
    "list_items = soup.find_all(\"li\")\n",
    "print(type(list_items))\n",
    "print(list_items[200:210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's search for the first and last university in the list and return their index number:\n",
    "for i in range(0, len(list_items)):\n",
    "    content = str(list_items[i].contents)\n",
    "    \n",
    "    if \"University of Alabama System\" in content:\n",
    "        print(\"Index of first university is: \" + str(i))    \n",
    "    \n",
    "    if \"University of Wyoming\" in content:\n",
    "        print(\"Index of last university is: \" + str(i))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we can use those indexes to subset out everything that isn't a university:\n",
    "universities = list_items[71:840]\n",
    "\n",
    "print(len(universities))\n",
    "print(universities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can grab the University Names and URLs for the wikipedia pages for the schools that have them:\n",
    "\n",
    "name_list = []\n",
    "url_list = []\n",
    "\n",
    "for uni in universities:\n",
    "    \n",
    "    name_list.append(uni.text)\n",
    "    \n",
    "    a_tag = uni.find(\"a\")\n",
    "    if a_tag:\n",
    "        ref = a_tag.get(\"href\")\n",
    "        print(ref)\n",
    "        url_list.append(ref)\n",
    "        \n",
    "    else:\n",
    "        print(\"No URL for this University\")\n",
    "        url_list.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = { \"name\" : pd.Series(name_list),\n",
    "      \"html_tag\" : pd.Series(universities),\n",
    "      \"url\" : pd.Series(url_list)}\n",
    "    \n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "df[\"url\"] = \"https://en.wikipedia.org\" + df[\"url\"]\n",
    "\n",
    "df.shape\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many names contain 'College':\n",
    "df['name'].str.contains(\"College\", na=False).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many names contain 'University':\n",
    "df['name'].str.contains(\"University\", na=False).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Scraping to Crawling\n",
    "\n",
    "So, you might have noticed that the information we collected from this scraper isn't that interesting. However, it does include a list of URLs for each University we found and we can scrape these pages as well. On the individual pages for each university, there's data on the school type, their location, endowment, and founding year, as well as other interesting information that we may be able to get to.\n",
    "\n",
    "At this point, you'd start to consider our task a basic form of web crawling - the systemic or automated browsing of multiple web pages. This is certainly a simple application of web crawling, but the idea of following hyperlinks from one URL to another is representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_pages = []\n",
    "for url in df[\"url\"]:\n",
    "    if url != \"\":\n",
    "        resp = requests.get(url)\n",
    "        uni_pages.append(resp.content)\n",
    "    else:\n",
    "        uni_pages.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add this newly scrapped data to our pandas dataframe:\n",
    "df[\"wikipedia_page\"] = uni_pages\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Our pandas dataframe now has a column containing the entire HTML wikipedia apgefor each university:\n",
    "df[\"wikipedia_page\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see what we can get from one page:\n",
    "soup = BeautifulSoup(df[\"wikipedia_page\"][0])\n",
    "table = soup.find(\"table\", {\"class\" : \"infobox\"})\n",
    "rows = table.find_all(\"tr\")\n",
    "    \n",
    "print(rows[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Now we can search across these rows for various data of interest:\n",
    "for row in rows:\n",
    "    header = row.find(\"th\")\n",
    "    data = row.find(\"td\")\n",
    "   \n",
    "    # Make sure there was actually both a th and td tag in that row, and proceed if so.\n",
    "    if header is not None and data is not None:\n",
    "        \n",
    "        if header.contents[0] == \"Type\":\n",
    "            print(\"The type of this school is \" + data.text)\n",
    "        \n",
    "        if header.contents[0] == \"Location\":\n",
    "            print(\"This location of this school is \" + data.text)\n",
    "            \n",
    "        if header.contents[0] == \"Website\":\n",
    "            print(\"The website for this school is \" + data.text)\n",
    "            \n",
    "        if \"Endowment\" in str(header.contents[0]):\n",
    "            print(\"The endowment for this school is \" + data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create empty columns of out dataframe to fill with new information:\n",
    "df[\"type\"] = \"\"\n",
    "df[\"location\"] = \"\"\n",
    "df[\"website\"] = \"\"\n",
    "df[\"established\"] = \"\"\n",
    "df[\"endowment\"] = \"\"\n",
    "\n",
    "## Loop over every wikipedia page in our dataframe and populate our new columns with the pertinent data:\n",
    "for i in range(0, len(df[\"wikipedia_page\"])):\n",
    "    tmp_soup = BeautifulSoup(df[\"wikipedia_page\"][i])\n",
    "    tmp_table = tmp_soup.find(\"table\", {\"class\" : \"infobox\"})\n",
    "    \n",
    "    if tmp_table is not None:\n",
    "        tmp_rows = tmp_table.find_all(\"tr\")\n",
    "\n",
    "        for row in tmp_rows:\n",
    "            header = row.find(\"th\")\n",
    "            data = row.find(\"td\")\n",
    "\n",
    "            if header is not None and data is not None:\n",
    "                if header.contents[0] == \"Type\":\n",
    "                    df[\"type\"][i] = data.text\n",
    "\n",
    "                if header.contents[0] == \"Location\":\n",
    "                    df[\"location\"][i] = data.text\n",
    "                \n",
    "                if header.contents[0] == \"Website\":\n",
    "                    df[\"website\"][i] = data.text  \n",
    "                    \n",
    "                ## Note that below we convert to unicode using utf-8, rather then simply str().\n",
    "                ## This is more robust in handling special characters.\n",
    "                if \"Endowment\" in header.contents[0].encode('utf-8'):\n",
    "                    df[\"endowment\"][i] = data.text\n",
    "                    \n",
    "                if \"Established\" in header.contents[0].encode('utf-8'):\n",
    "                    df[\"established\"][i] = data.text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Now we have dramatically more actionable data that could have been very difficult to collect manually.\n",
    "df[:200]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
