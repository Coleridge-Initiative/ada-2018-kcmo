{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan, and Ridhima Sodhi. \"ADA-KCMO-2018.\" Coleridge Initiative GitHub Repositories. 2018. https://github.com/Coleridge-Initiative/ada-kcmo-2018. [![DOI](https://zenodo.org/badge/119078858.svg)](https://zenodo.org/badge/latestdoi/119078858)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "    - [Learning Objectives](#Learning-Objectives)\n",
    "    - [Glossary of Terms](#Glossary-of-Terms)\n",
    "- [Python Setup](#Python-Setup)\n",
    "- [Loading the Data](#Loading-the-Data)\n",
    "- [Data Emploration & Creating Text Corpus](#Data-Emploration-&-Creating-Text-Corpus)\n",
    "- [The Principles of Topic Modeling](#The-Principles-of-Topic-Modeling)\n",
    "- [Topic Modeling in Practice](#Topic-Modeling-in-Practice)\n",
    "    - [Creating a matrix of Features from Text – Bag of N-grams](#Creating-a-matrix-of-features-from-text-–-Bag-of-N-grams)\n",
    "    - [Calculating Word Counts](#Calculating-Word-Counts)\n",
    "    - [Text Cleaning and Normalization](#Text-Cleaning-and-Normalization)\n",
    "    - [Tokenizing – Breaking Text into Pieces](#Tokenizing-–-)\n",
    "    - [Stopwords – Removing Meaningless Text](#Removing-Meaningless-Text-–-Stopwords)\n",
    "    - [Distilling Text data – Stemming and Lemmatization - ](#Stemming-and-Lemmatization---Distilling-text-data)\n",
    "    - [N-grams - Adding context by creating N-grams](#N-grams---Adding-context-by-creating-N-grams)\n",
    "    - [TF-IDF - Weighting terms based on frequency](#TF-IDF---Weighting-terms-based-on-frequency)\n",
    "- [Additional Resources](#Additional-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "**Text analysis** is used to extract useful information from or summarize a large amount of unstructured text stored in documents. This opens up the opportunity of using text data alongside more conventional data sources (e.g. surveys and administrative data). The goal of text analysis is to take a large corpus of complex and unstructured text data and extract important and meaningful messages in a comprehensible way. \n",
    "\n",
    "Text analysis can help with the following tasks:\n",
    "\n",
    "* **Information Retrieval**: Find relevant information in a large database, such as a systematic literature review, that would be very time-consuming for humans to do manually. \n",
    "\n",
    "* **Clustering and Text Categorization**: Summarize a large corpus of text by finding the most important phrases, using methods like topic modeling. \n",
    "\n",
    "* **Text Summarization**: Create category-sensitive text summaries of a large corpus of text. \n",
    "\n",
    "* **Machine Translation**: Translate documents from one language to another. \n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "In this tutorial, you will...\n",
    "* Learn how to transform a corpus of text into a structured matrix format so that we can apply natural language processing (NLP) methods\n",
    "* Learn the basics and applications of topic modeling\n",
    "* Learn how to do document tagging and evaluate the results\n",
    "\n",
    "### Glossary of Terms\n",
    "\n",
    "Glossary of Terms:\n",
    "\n",
    "* **Corpus**: A corpus is the set of all text documents used in your analysis; for example, your corpus of text may include hundreds of research articles.\n",
    "\n",
    "* **Tokenize**: Tokenization is the process by which text is separated into meaningful terms or phrases. In English this is easy to do for individual words, as they are separated by whitespace; however, it can get more complicated to  automate determining which groups of words constitute meaningful phrases. \n",
    "\n",
    "* **Stemming**: Stemming is normalizing text by reducing all forms or conjugations of a word to the word's most basic form. In English, this can mean making a rule of removing the suffixes \"ed\" or \"ing\" from the end of all words, but it gets more complex. For example, \"to go\" is irregular, so you need to tell the algorithm that \"went\" and \"goes\" stem from a common lemma, and should be considered alternate forms of the word \"go.\"\n",
    "\n",
    "* **TF-IDF**: TF-IDF (term frequency-inverse document frequency) is an example of feature engineering where the most important words are extracted by taking account their frequency in documents and the entire corpus of documents as a whole.\n",
    "\n",
    "* **Topic Modeling**: Topic modeling is an unsupervised learning method where groups of words that often appear together are clustered into topics. Typically, the words in one topic should be related and make sense (e.g. boat, ship, captain). Individual documents can fall under one topic or multiple topics. \n",
    "\n",
    "* **LDA**: LDA (Latent Dirichlet Allocation) is a type of probabilistic model commonly used for topic modeling. \n",
    "\n",
    "* **Stop Words**: Stop words are words that have little semantic meaning but occur very frequently, like prepositions, articles and common nouns. For example, every document (in English) will probably contain the words \"and\" and \"the\" many times. You will often remove them as part of preprocessing using a list of stop words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "import nltk\n",
    "import ujson\n",
    "import re\n",
    "import time\n",
    "# import progressbar\n",
    "import psycopg2\n",
    "\n",
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "from six.moves import zip, range \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "# nltk.download('stopwords') #download the latest stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Our dataset for this tutorial will be a set of articles from the Business Section of the Kansas City Star. We have parsed out for every article the title, the date, the author, and the text or articles content.\n",
    "\n",
    "Let's start by loading the table into a `pandas` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection properties\n",
    "db_name = \"appliedda\"\n",
    "db_host = \"10.10.2.10\"\n",
    "conn = psycopg2.connect(database=db_name, host=db_host) #database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM ada_kcmo.kansas_city_star_business_articles_2016_2018;', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Creating Text Corpus\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Our business articles data table has 5 fields:\n",
    "\n",
    "- `title` - article title as it appeared onlin and in the written newspaper.\n",
    "- `date` - date of article publication.\n",
    "- `author` - article author or authors.\n",
    "- `text` - article content.\n",
    "- `article_id` - unique identifier for every article.\n",
    "\n",
    "Let's take a look at examples of the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the title and text variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `title` and `text` have text content. Unsurprisingly, the article content is longer than the title. We could perform text analysis of the title, the content, or both. For the time being, we will focus of the content. \n",
    "\n",
    "First we need to form our corpus of text: we can pull out the array of descriptions from the DataFrame using the data frame's `.values` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = df['text'].values #pull all the descriptions and put them in a numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principles of Topic Modeling\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We are going to apply topic modeling, an unsupervised learning method, to our corpus to find the high-level topics in our corpus as a \"first go\" for exploring our data. Through this process, we'll discuss how to clean and preprocess our data to get the best results.\n",
    "\n",
    "Topic modeling is a broad subfield of machine learning and natural language processing. We are going to focus on a common modeling approach called Latent Dirichlet Allocation (LDA). \n",
    "\n",
    "To use topic modeling, we first have to assume that topics exist in our corpus, and that some small number of these topics can \"explain\" the corpus. Topics in this context refer to words from the corpus, in a list that is ranked by probability. A single document can be explained by multiple topics. For instance, an article on a merger in the healthcare industry could fall under the topic \"finance\" as well as the topic \"healthcare\". The set of topics used by a document is known as the document's allocation, hence, the name Latent Dirchlet Allocation, each document has an allocation of latent topics allocated by Dirchlet distribution. \n",
    "\n",
    "As a start to modeling topics, we define below a function \"create_topics\" that uses Latent Dirichlet Allocation (LDA) to find topics. We will refer to this function throughout the notebook, so don't worry if you do not understand every part of the function yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topics(tfidf, features, N_TOPICS=3, N_TOP_WORDS=5):\n",
    "    \"\"\"\n",
    "    Given a matrix of features of text data generate topics\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    tfidf: scipy sparse matrix\n",
    "        sparse matrix of text features\n",
    "    N_TOPICS: int\n",
    "        number of topics (default 10)\n",
    "    N_TOP_WORDS: int\n",
    "        number of top words to display in each topic (default 10)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ls_keywords: ls\n",
    "        list of keywords for each topics\n",
    "    doctopic: array\n",
    "        numpy array with percentages of topic that fit each category\n",
    "    N_TOPICS: int\n",
    "        number of assumed topics\n",
    "    N_TOP_WORDS: int\n",
    "        Number of top words in a given topic. \n",
    "    \"\"\"\n",
    "    \n",
    "    i=0\n",
    "    lda = LatentDirichletAllocation(n_components= N_TOPICS,\n",
    "                                    learning_method='online') #create an object that will create 5 topics\n",
    "    i+=1\n",
    "    doctopic = lda.fit_transform( tfidf )\n",
    "    i+=1\n",
    "\n",
    "    ls_keywords = []\n",
    "    for i,topic in enumerate(lda.components_):\n",
    "        word_idx = np.argsort(topic)[::-1][:N_TOP_WORDS]\n",
    "        keywords = ', '.join( features[i] for i in word_idx)\n",
    "        ls_keywords.append(keywords)\n",
    "        print(i, keywords)\n",
    "        i+=1\n",
    "            \n",
    "    return ls_keywords, doctopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling in Practice\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The first important step in working with text data is cleaning and processing the data, which includes (but is not limited to):\n",
    "\n",
    "- forming a corpus of text\n",
    "- tokenization\n",
    "- removing stop-words\n",
    "- finding words co-located together (N-grams)\n",
    "- stemming and lemmatization\n",
    "\n",
    "Each of these steps will be discussed below. \n",
    "\n",
    "The ultimate goal is to transform our text data into a form an algorithm can work with, because a document or a corpus of text cannot be fed directly into an algorithm. Algorithms expect numerical feature vectors with certain fixed sizes, and can't handle documents, which are basically sequences of symbols with variable length. We will be transforming our text corpus into a *bag of n-grams* to be further analyzed. In this form our text data is represented as a matrix where each row refers to a specific job description (document) and each column is the occurence of a word (feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Matrix of Features from Text – Bag of N-grams\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Ultimately, we want to take our collection of documents, corpus, and convert it into a matrix. Fortunately, `sklearn` has a pre-built object, `CountVectorizer`, that can tokenize, eliminate stopwords, identify n-grams, and stem our corpus, and output a matrix in one step. Before we apply the vectorizer to our corpus of data, let's apply it to a toy example so that we see what the output looks like and how a bag of words is represented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(corpus, \n",
    "                        NGRAM_RANGE = (0, 1), \n",
    "                        stop_words = None, \n",
    "                        stem = False, \n",
    "                        MIN_DF = 0.05, \n",
    "                        MAX_DF = 0.95, \n",
    "                        USE_IDF = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Turn a corpus of text into a bag-of-words.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: ls\n",
    "        test of documents in corpus    \n",
    "    NGRAM_RANGE: tuple\n",
    "        range of N-gram. Default (0,1)\n",
    "    stop_words: ls\n",
    "        list of commonly occuring words that have little semantic\n",
    "        value\n",
    "    stem: bool\n",
    "        use a stemmer to stem words\n",
    "    MIN_DF: float\n",
    "       exclude words that have a frequency less than the threshold\n",
    "    MAX_DF: float\n",
    "        exclude words that have a frequency greater than the threshold\n",
    "    USE_IDF: bool\n",
    "        Re-weigh words according to the Term Frequency-Inverse Document Frequency \n",
    "        (emphasize words unique to a document, suppress words common throughout the corpus)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bag_of_words: scipy sparse matrix\n",
    "        scipy sparse matrix of text\n",
    "    features:\n",
    "        list of words\n",
    "    \"\"\"\n",
    "    #parameters for vectorizer \n",
    "    ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "    STRIP_ACCENTS = 'unicode'\n",
    "    \n",
    "    if stem:\n",
    "        stemmer = nltk.SnowballStemmer(\"english\")\n",
    "        tokenize = lambda x: [stemmer.stem(i) for i in x.split()]\n",
    "    else:\n",
    "        tokenize = None\n",
    "    vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                                 tokenizer=tokenize, \n",
    "                                 ngram_range=NGRAM_RANGE,\n",
    "                                 stop_words = stop_words,\n",
    "                                 strip_accents=STRIP_ACCENTS,\n",
    "                                 min_df = MIN_DF,\n",
    "                                 max_df = MAX_DF)\n",
    "    \n",
    "    bag_of_words = vectorizer.fit_transform( corpus ) #transform our corpus is a bag of words \n",
    "    features = vectorizer.get_feature_names()\n",
    "\n",
    "    if USE_IDF:\n",
    "        NORM = None #turn on normalization flag\n",
    "        SMOOTH_IDF = True #prvents division by zero errors\n",
    "        SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "        transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)\n",
    "        #get the bag-of-words from the vectorizer and\n",
    "        #then use TFIDF to limit the tokens found throughout the text \n",
    "        tfidf = transformer.fit_transform(bag_of_words)\n",
    "        \n",
    "        return tfidf, features\n",
    "    else:\n",
    "        return bag_of_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create example corpus.\n",
    "toy_corpus = ['this is document one', 'this is document two', 'text analysis on documents is fun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to bag of words\n",
    "toy_bag_of_words, toy_features = create_bag_of_words( toy_corpus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review - our corpus:\n",
    "toy_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features derived from the corpus\n",
    "toy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words that results:\n",
    "np_bag_of_words = toy_bag_of_words.toarray()\n",
    "np_bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has been transformed from a document into a 3 x 9 matrix, where each row in the matrix corresponds to a document, and each column corresponds to a feature (in the order they appear in `toy_features`). A 1 indicates the existence of the feature or word in the document, and a 0 indicates the word is not present.\n",
    "\n",
    "It is very common that this representation will be a \"sparse\" matrix, or a matrix that has a lot of 0s. With sparse matrices, it is often more efficient to keep track of which values *aren't* 0 and where those non-zero entries are located, rather than to save the entire matrix. To save space, the `scipy` library has special ways of storing sparse matrices in an efficient way. \n",
    "\n",
    "Our toy corpus is now ready to be analyzed. We used this toy example to illustrate how a document is turned into a matrix to be used in text analysis. When you're applying this to real text data, the matrix will be much larger and harder to interpret, but it's important that you know the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1 - convert corpus to matrix__\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "To check your knowledge, make your own toy corpus and turn it into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n",
    "exercise_corpus = ['Batman is friends with Superman', \n",
    "                   'Superman is enemies with Lex Luthor',\n",
    "                   'Batman is enemies with Lex Luthor' ] \n",
    "exercise_bag_of_words, exercise_features = create_bag_of_words(exercise_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bag of words to array\n",
    "np_bag_of_words = exercise_bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show features:\n",
    "exercise_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output derived bag of words:\n",
    "np_bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Word Counts\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As an initial look into the data, we can examine the most frequently occuring words in our corpus. We can sum the columns of the bag_of_words and then convert to a numpy array. From here we can zip the features and word_count into a dictionary, and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(bag_of_words, feature_names):\n",
    "\n",
    "    \"\"\"\n",
    "    Get the ordered word counts from a bag_of_words\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bag_of_words: obj\n",
    "        scipy sparse matrix from CounterVectorizer\n",
    "    feature_names: ls\n",
    "        list of words\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    word_counts: dict\n",
    "        Dictionary of word counts\n",
    "    \"\"\"\n",
    "\n",
    "    # convert bag of words to array\n",
    "    np_bag_of_words = bag_of_words.toarray()\n",
    "    \n",
    "    # calculate word count.\n",
    "    word_count = np.sum(np_bag_of_words,axis=0)\n",
    "    \n",
    "    # convert to flattened array.\n",
    "    np_word_count = np.asarray(word_count).ravel()\n",
    "    \n",
    "    # create dict of words mapped to count of occurrences of each word.\n",
    "    dict_word_counts = dict( zip(feature_names, np_word_count) )\n",
    "    \n",
    "    # Create ordered dictionary\n",
    "    orddict_word_counts = OrderedDict( sorted(dict_word_counts.items(), key=lambda x: x[1], reverse=True), )\n",
    "    \n",
    "    return orddict_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ordered word counts for our example corpus.\n",
    "get_word_counts(toy_bag_of_words, toy_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the words \"document\" and \"documents\" both appear separately in the list. Should they be treated as the same words, since one is just the plural of the other, or should they be considered distinct words? These are the types of decisions you will have to make in your preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2 - getting word counts__\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Get the word counts of your exercise corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_counts(exercise_bag_of_words, exercise_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words and set of features from our social services corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bag_of_words, corpus_features = create_bag_of_words(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine our features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first aspect to notice about the feature list is that the first few entries are numbers that have no real semantic meaning. The feature lists also includes numerous other useless words, such as prepositions and articles, that will just add noise to our analysis. \n",
    "\n",
    "We can also notice the words *comprises* and *comprising*, or the words *form* and *formed*, are close enough to each other that it might not make sense to treat them as entirely separate words. Part of your cleaning and preprocessing duties will be manually inspecting your lists of features, seeing where these issues arise, and making decisions to either remove them from your analysis or address them separately. \n",
    "\n",
    "Let's get the count of the number of times that each of the words appears in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_counts(corpus_bag_of_words, corpus_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our top words are articles, prepositions and conjunctions that are not informative whatsoever, so we're probably not going to come up with anything interesting (\"garbage in, garbage out\"). \n",
    "\n",
    "Nevertheless, let's forge blindly ahead and try to create topics, and see the quality of the results that we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_corpus_keywords, corpus_doctopic = create_topics(corpus_bag_of_words, corpus_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These topics don't give us any real insight to what the data contains - one of the topics is \"to, of, in, that, for\"! According to the articles in your dataset, some might hint to subjects (\"sales\", \"consumer\", \"index\", etc.) but the signal is being swamped by the noise. \n",
    "\n",
    "We'll have to clean and process our data to get any meaningful information out of this text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "To clean and normalize text, we'll remove all special characters, numbers, and punctuation, so we're left with only the words themselves. Then we will make all the text lowercase; this uniformity will ensure that the algorithm doesn't treat \"the\" and \"The\" as different words, for example. \n",
    "\n",
    "To remove the special characters, numbers and punctuation we will use regular expressions. \n",
    "\n",
    "\n",
    "**Regular Expressions**, or \"regexes\" for short, let you find all the words or phrases in a document or text file that match a certain pattern. These rules are useful for pulling out useful information from a large amount of text. For example, if you want to find all email addresses in a document, you might look for everything that looks like *some combination of letters, _, .* followed by *@*, followed by more letters, and ending in *.com* or *.edu*. If you want to find all the credit card numbers in a document, you might look for everywhere you see the pattern \"four numbers, space, four numbers, space, four numbers, space, four numbers.\" Regexes are also helpful if you are scraping information from websites, because you can use them to separate the content from the HTML code used for formatting the website.\n",
    "\n",
    "A full tutorial on regular expressions would be outside the scope of this tutorial, but many good tutorials that can be found on-line. [regex101.com](regex101.com) is also a great interactive tool for developing and checking regular expressions.\n",
    "\n",
    ">\"Some people, when confronted with a problem, think \n",
    ">'I know, I'll use regular expressions.'   Now they have two problems.\"\n",
    "> -- Jaime Zawinski\n",
    "\n",
    "*A word of warning:* Regexes can work much more quickly than plain text sorting; however, if your regular expressions are becoming overly complicated, it's a good idea to find a simpler way to do what you want to do. Any developer should keep in mind there is a trade-off between optimization and understandability. The general philosophy of programming in Python is that your code is meant to be as understandable by *people* as much as possible, because human time is more valuable than computer time. You should therefore lean toward understandability rather than overly optimizing your code to make it run as quickly as possible. Your future-self, code-reviewers, people who inherit your code, and anyone else who has to make sense of your code in the future will appreciate it. \n",
    "\n",
    "For our purposes, we are going to use a regular expression to match all characters that are not letters -- punctuation, quotes, special characters and numbers -- and replace them with spaces. Then we'll make all of the remaining characters lowercase.  \n",
    "\n",
    "We will be using the `re` library in python for regular expression matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of the punctuations and set all characters to lowercase\n",
    "RE_PREPROCESS = re.compile( r'\\W+|\\d+' ) #the regular expressions that matches all non-characters\n",
    "\n",
    "# get rid of punctuation and make everything lowercase\n",
    "# the code below works by looping through the array of text (\"corpus\")\n",
    "# for a given piece of text ( \"description\" ) we invoke the `re.sub` command \n",
    "# the `re.sub` command takes 3 arguments: (1) the regular expression to match, \n",
    "# (2) what we want to substitute in place of that matching string (' ', a space)\n",
    "# and (3) the text we want to apply this to. \n",
    "# we then invoke the `lower()` method on the output of the `re.sub` command\n",
    "# to make all the remaining characters lowercase.\n",
    "# the result is a list, where each entry in the list is a cleaned version of the\n",
    "# corresponding entry in the original corpus.\n",
    "# we then make the list into a numpy array to use it in analysis\n",
    "\n",
    "processed_corpus = np.array( [ re.sub( RE_PREPROCESS, ' ', description ).lower() for description in corpus ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at an example of the results of this cleanup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First Description, Before Cleaning__\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, we'll look at the first description in our corpus, before it was cleaned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text includes a lot of useful information, but also includes some things we don't want or need. There are some weird special characters (...). There are also some numbers, which are informative and interesting to a human reading the text (phone numbers, addresses, ...), but when we break down the documents into individual words, the numbers will become meaningless. We'll also want to remove all punctuation, so that we can say any two things separated by a space are individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First Description, After Cleaning__\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now, let's look at this text after cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All lowercase, all numbers and special characters have been removed. Out text is now normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking Text into Pieces – Tokenizing\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we've cleaned our text, we can *tokenize* it by deciding which words or phrases are the most meaningful. Normally the `CountVectorizer` handles this for us, but in this case, we'll split our text into individual words manually to show how it is done.\n",
    "\n",
    "To go from a whole document to a list of individual words, we can use the `.split()` command. By default, this command splits based on spaces in between words, so we don't need to specify that explicitly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = processed_corpus[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Meaningless Text – Stopwords\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Stopwords are words that are found commonly throughout a text and carry little semantic meaning. Examples of common stopwords are prepositions (\"to\", \"on\", \"in\"), articles (\"the\", \"an\", \"a\"), conjunctions (\"and\", \"or\", \"but\") and common nouns. For example, the words *the* and *of* are totally ubiquitous, so they won't serve as meaningful features, whether to distinguish documents from each other or to tell what a given document is about. You may also run into words that you want to remove based on where you obtained your corpus of text or what it's about. There are many lists of common stopwords available for you to use, both for general documents and for specific contexts, so you don't have to start from scratch.   \n",
    "\n",
    "We can eliminate stopwords by checking all the words in our corpus against a list of commonly occuring stopwords that comes with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample of stopwords\n",
    "#this is an example of slicing where we implicitly start at the beginning and move to the end\n",
    "#we select every 10th entry in the array\n",
    "eng_stopwords[::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this list includes \"weren\" and \"hasn\" as well as single letters (\"t\"). Why do you think these are contained in the list of stopwords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3 - practicing slicing__\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Try slicing to retrieve every 5th word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords[::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up our data a little bit, let's see what our bag of words looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create bag of words from processed_corpus\n",
    "processed_bag_of_words, processed_features = create_bag_of_words( processed_corpus, stop_words = eng_stopwords )\n",
    "dict_processed_word_counts = get_word_counts( processed_bag_of_words, processed_features )\n",
    "dict_processed_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Now this is starting to look like a reasonable representation of our corpus of text. \n",
    "\n",
    "We mentioned that, in addition to stopwords that are common across all types of text analysis problems, there will also be specific stopwords based on the context of your domain. One quick way to remove some of these domain-specific stopwords is by dropping some of your most frequent words. \n",
    "> Notice how the top words include words like \"said\", \"year\"? It makes sense that these words are so common - so they won't be very helpful in analysis. We'll start out by dropping the top 20 words. You'll want to change this number, playing with making it bigger and smaller, to see how it affects your resulting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 20 stopwords (slice from start through 20 items in list)\n",
    "top_20_words = list(dict_processed_word_counts.keys())[:20]\n",
    "\n",
    "# create new stopword list by combining default stopwords with our top 20.\n",
    "domain_specific_stopwords = eng_stopwords + top_20_words\n",
    "\n",
    "# make a new bag of words excluding custom stopwords.\n",
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what do we have now?\n",
    "dict_processed_word_counts = get_word_counts(processed_bag_of_words, processed_features)\n",
    "dict_processed_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is a bit better - although we still see some words that are probably very common (\"may\"), words like \"health\" will probably help us come up with more specific categories within the broader realm of business articles. Let's see what topics we produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are starting to get somewhere! We can manipulate the number of topics we want to find and the number of words to use for each topic to see if we can understand more from our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for 5 topics, include 10 words in each.\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                       N_TOPICS = 5,\n",
    "                                                       N_TOP_WORDS= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some structure is starting to reveal itself. Adding more topics has revealed to larger subtopics. Let's see if increasing the number of topics gives us more information.\n",
    "\n",
    "However, we can see that some useless words are still present. This is an iterative process - after seeing the results of some analysis, you will need to go back to the preprocessing step and add more words to your list of stopwords or change how you cleaned the data.\n",
    "\n",
    "Now let's try 10 topics with 15 words each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 topics, 15 words each\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                       N_TOPICS = 10,\n",
    "                                                       N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This looks like a good amount of topics for now. Some of the top words are quite similar, like \"jobs\" and \"job\". Let's move to stemming and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization - Distilling text data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We can further process our text through *stemming and lemmatization*, or replacing words with their root or simplest form. For example \"systems,\" \"systematic,\" and \"system\" are all different words, but we can replace all these words with \"system\" without sacrificing much meaning. \n",
    "\n",
    "- A **lemma** is the original dictionary form of a word (e.g. the lemma for \"lies,\" \"lied,\" and \"lying\" is \"lie\").\n",
    "- The process of turning a word into its simplest form is **stemming**. There are several well known stemming algorithms -- Porter, Snowball, Lancaster -- that all have their respective strengths and weaknesses.\n",
    "\n",
    "For this tutorial, we'll use the Snowball Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of how a Stemmer works:\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem('lies'))\n",
    "print(stemmer.stem(\"lying\"))\n",
    "print(stemmer.stem('systematic'))\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try creating a bag of stemmed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include stemming when creating our bag of words.\n",
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords,\n",
    "                                                                 stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create topics with stemmed words.\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                       N_TOPICS = 10,\n",
    "                                                       N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What do we think of these topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams - Adding context by creating N-grams\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Obviously, reducing a document to a bag of words means losing much of its meaning - we put words in certain orders, and group words together in phrases and sentences, precisely to give them more meaning. If you follow the processing steps we've gone through so far, splitting your document into individual words and then removing stopwords, you'll completely lose all phrases like \"kick the bucket,\" \"commander in chief,\" or \"sleeps with the fishes.\" \n",
    "\n",
    "One way to address this is to break down each document similarly, but rather than treating each word as an individual unit, treat each group of 2 words, or 3 words, or *n* words, as a unit. We call this a \"bag of *n*-grams,\" where *n* is the number of words in each chunk. Then you can analyze which groups of words commonly occur together (in a fixed order). \n",
    "\n",
    "Let's transform our corpus into a bag of n-grams with *n*=2: a bag of 2-grams, AKA a bag of bi-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words with stemmed words and 2-grams (NGRAM_RANGE = (0, 2)).\n",
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords,\n",
    "                                                                 stem=True,\n",
    "                                                                 NGRAM_RANGE=(0,2))\n",
    "\n",
    "# Create topics.\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                       N_TOPICS = 10,\n",
    "                                                       N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that this lets us uncover patterns that we couldn't when we just used a bag of words: \"kansas citi\" come up as a word. Note that this still includes the individual words, as well as the bi-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF - Weighting terms based on frequency\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "A final step in cleaning and processing our text data is **Term Frequency-Inverse Document Frequency (TF-IDF)**. TF-IDF is based on the idea that the words (or terms) that are most related to a certain topic will occur frequently in documents on that topic, and infrequently in unrelated documents.  TF-IDF re-weights words so that we emphasize words that are unique to a document and suppress words that are common throughout the corpus by inversely weighting terms based on their frequency within the document and across the corpus.\n",
    "\n",
    "Let's look at how using TF-IDF affects our bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words including TF-IDF weighting.\n",
    "processed_bag_of_words, processed_features = create_bag_of_words( processed_corpus,\n",
    "                                                                  stop_words = domain_specific_stopwords,\n",
    "                                                                  stem = True,\n",
    "                                                                  NGRAM_RANGE = ( 0, 2 ),\n",
    "                                                                  USE_IDF = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's see what we have:\n",
    "dict_word_counts = get_word_counts( processed_bag_of_words, processed_features )\n",
    "dict_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words counts have been reweighted to emphasize the more meaningful words of the corpus, while de-emphasizing the words that are found commonly throughout the corpus.\n",
    "\n",
    "How does this affect our topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                       N_TOPICS = 5,\n",
    "                                                       N_TOP_WORDS= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4 - Refining a topic model**\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "You can only develop an intuition for the right number of topics and topic words suitable for a given problem by iterating until you find a good match. \n",
    "\n",
    "Change the number of topics and topic words until you get an intution of how many words and topics are enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_keywords, exercise_doctopic = create_topics( processed_bag_of_words, \n",
    "                                                      processed_features, \n",
    "                                                      N_TOPICS = 5,\n",
    "                                                      N_TOP_WORDS= 25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_keywords, exercise_doctopic = create_topics( processed_bag_of_words, \n",
    "                                                      processed_features, \n",
    "                                                      N_TOPICS = 10,\n",
    "                                                      N_TOP_WORDS= 25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the topic_id of the majority topic for each document and store it in a list\n",
    "ls_topic_id = [np.argsort(processed_doctopic[comment_id])[::-1][0] for comment_id in range(len(corpus))]\n",
    "df['topic_id'] = ls_topic_id #add to the dataframe so we can compare with the job titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each row is tagged with a topic ID. Let's see how well the topics explain the text by looking at the first topic, and seeing how similar the texts within that topic are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_num = 0\n",
    "print(processed_keywords[topic_num])\n",
    "df[df.topic_id == topic_num].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5 - Interpreting a model's \"topics\"**\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Examine the other topic IDs, and see if the \"topics\" we identified make sense as groupings of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_num = 3\n",
    "print(processed_keywords[topic_num])\n",
    "df[df.topic_id == topic_num].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- A great resource for NLP in Python is \n",
    "[Natural Language Processing with Python](https://www.amazon.com/Natural-Language-Processing-Python-Analyzing/dp/0596516495)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
