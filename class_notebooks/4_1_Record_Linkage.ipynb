{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan, and Ridhima Sodhi. \"ADA-KCMO-2018.\" Coleridge Initiative GitHub Repositories. 2018. https://github.com/Coleridge-Initiative/ada-kcmo-2018. [![DOI](https://zenodo.org/badge/119078858.svg)](https://zenodo.org/badge/latestdoi/119078858)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Linkage\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "    - [Learning Objectives](#Learning-Objectives)\n",
    "    - [Methods](#Methods)\n",
    "- [The Principles of Record Linkage](#The-Principles-of-Record-Linkage)\n",
    "- [Data Description](#Data-Description)\n",
    "- [Python Setup](#Python-Setup)\n",
    "- [Load the Data](#Load-the-Data)\n",
    "- [Data Exploration](#Data-Exploration)\n",
    "- [Record Linkage on Business Names](#Record-Linkage-on-Business-Names)\n",
    "    - [The Importance of Pre-Processing](#The-Importance-of-Pre-Processing)\n",
    "    - [Cleaning String Variables](#Cleaning-String-Variables)\n",
    "    - [Regular Expressions – `regex`](#Regular-Expressions-–-regex)\n",
    "    - [Handling Business Suffixes](#Handling-Business-Suffixes)\n",
    "    - [Record Linkage: Exact Matching on One Field](#Record-Linkage:-Exact-Matching-on-One-Field)\n",
    "- [Record Linkage on Addresses](#Record-Linkage-on-Addresses)\n",
    "    - [Address Parsing](#Address-Parsing)\n",
    "    - [Record Linkage: Exact Matching on Several Fields](#Record-Linkage:-Exact-Matching-on-Several-Fields)\n",
    "    - [Record Linkage: Rule-Based Matching](#Record-Linkage:-Rule-Based-Matching)\n",
    "    - [Record Linkage: Fellegi Sunter](#Record-Linkage:-Fellegi-Sunter)\n",
    "- [Additional Resources](#Additional-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "This notebook will provide you with an instruction into Record Linkage using Python. Upon completion of this notebook you will be able to apply record linkage techniques using the *recordlinkage* package to combine data from different sources in Python. \n",
    "It will lead you through all the steps necessary for a successful record linkage starting with data preparation  including pre-processing, cleaning and standardization of data.\n",
    "The notebook follows the underlying lecture and provides examples on how to implement record linkage techniques. \n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "The goal of this notebook is for you to understand the record linkage techniques. You will be responsible for linking the different datasets in the ADRF, and the subsequent dataset will be used in your later projects.\n",
    "\n",
    "### Methods\n",
    "\n",
    "For this notebook exercise we are interested in business data from Kansas City, MO. Business names appear in different datasets: Wage Records and Employer Data from the Missouri Department of Labor, Business Registrations from the Kansas City, MO, Department of Revenue, and Water Consumption data from the Kansas City, MO, Water Services. Here, we will combine the three.\n",
    "\n",
    "- **Analytical Exercise**: Merge Employer Wage Records, Business Registrations, and Water Services Data. \n",
    "\n",
    "- **Approach**: We will look at the data available to us, and clean & pre-process it to enable better linkage. Since the only identifiers we have for this case study are the names of the employers and addresses, we will have to use string matching techniques, part of Python's record linkage package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principles of Record Linkage\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The goal of record linkage is to determine if pairs of records describe the same identity. For instance, this is important for removing duplicates from a data source or joining two separate data sources together. Record linkage also goes by the terms data matching, merge/purge, duplication detection, de-duping, reference matching, entity resolution, disambiguation, co-reference/anaphora in various fields.\n",
    "\n",
    "There are several approaches to record linkage that include \n",
    "    - exact matching \n",
    "    - rule-based linking \n",
    "    - probabilistic linking \n",
    "- An example of **exact matching** is joining records based on social security number, exact name, or geographic code information. This is what you already have done in SQL by joining tables on an unique identifier. \n",
    "- **Rule-based matching** involves applying a cascading set of rules that reflect the domain knowledge of the records being linked. \n",
    "- In **probabilistic record linkages**, linkage weights are estimated to calculate the probability of a certain match.\n",
    "\n",
    "In practical applications you will need record linkage techiques to combine information addressing the same entity that is stored in different data sources. Record linkage will also help you to address the quality of different data sources. For example, if one of your databases has missing values you might be able to fill those by finding an identical pair in a different data source. Overall, the main applications of record linkage are\n",
    "    1. Merging two or more data files \n",
    "    2. Identifying the intersection of the two data sets \n",
    "    3. Updating data files (with the data row of the other data files) and imputing missing data\n",
    "    4. Entity disambiguation and de-duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The datasets used in this exercise will be:\n",
    "- Employer Data from the Missouri Department of Labor (`kcmo_lehd.mo_qcew_employers`)\n",
    "- Business Registrations from the Kansas City, MO, Department of Revenue (`public.mo_business_licenses`)\n",
    "- Water Consumption data from the Kansas City, MO, Water Services (`kcmo_water.ucbprem_premises` and `kcmo_water.ubbchst_consumption_history`)\n",
    "\n",
    "**Variables Used for Linking:**\n",
    "\n",
    "We will link these datasets both on Business Name and Business Address.\n",
    "\n",
    "The Employer Data and Business Registrations from KCMO both have variables for \"Business Name\":\n",
    "- In the table `kcmo_lehd.mo_qcew_employers`: `legal_name`\n",
    "- In the table `public.mo_business_licenses`: `legalname`\n",
    "\n",
    "Employer Data, Business Registrations, and Water Data all have address information:\n",
    "- Table `kcmo_lehd.mo_qcew_employers` has a UI address, a physical location and a MO location. We will use the physical location. Address parts are already parsed: `pl_addr1`, `pl_addr2`, `pl_city`, `pl_zip`.\n",
    "- In the table `public.mo_business_licenses`: `address`\n",
    "- In the table `kcmo_water.ucbprem_premises`, the address parts are already parsed: `ucbprem_street_name`, `ucbprem_street_number`, `ucbprem_ssfx_code`, `ucbprem_city`, and `ucbprem_zipc_code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python provides us with some tools we can use for record linkages so we don't have to start from scratch and code our own linkage algorithms. Before we start we need to load the package `recordlinkage`. To fully function this packages uses other packages which also need to be imported. We are adding a couple more packages to the ones you are already familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general use imports\n",
    "%pylab inline\n",
    "import datetime\n",
    "import glob\n",
    "import inspect\n",
    "import numpy as np\n",
    "import os\n",
    "import six\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import jellyfish\n",
    "import re\n",
    "\n",
    "# pandas-related imports\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "# record linkage package\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.standardise import clean\n",
    "\n",
    "# CSV file reading-related imports\n",
    "import csv\n",
    "\n",
    "# database interaction imports\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "\n",
    "print( \"Imports loaded at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As we've done in previous notebooks, let's set up our database scheme, connect to the Database using `psycopg2`, and query the three datasets we would like to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection properties\n",
    "db_name = \"appliedda\"\n",
    "db_host = \"10.10.2.10\"\n",
    "conn = psycopg2.connect(database=db_name, host=db_host) #database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SQL \n",
    "business_licenses_query = '''\n",
    "SELECT address\n",
    "        , legalname\n",
    "        , filingperiod\n",
    "FROM public.kcmo_business_licenses\n",
    "WHERE filingperiod = '2016-12-31';\n",
    "'''\n",
    "\n",
    "employer_query = '''\n",
    "SELECT legal_name\n",
    "        , pl_addr1\n",
    "        , pl_city\n",
    "        , pl_zip\n",
    "FROM kcmo_lehd.mo_qcew_employers\n",
    "WHERE year = 2016\n",
    "    AND qtr = 1\n",
    "    AND pl_city = 'KANSAS CITY'\n",
    "    AND pl_state = 'MO';\n",
    "'''\n",
    "\n",
    "water_services_query = '''\n",
    "SELECT ucbprem_street_number\n",
    "        , ucbprem_pdir_code_pre\n",
    "        , ucbprem_street_name\n",
    "        , ucbprem_ssfx_code\n",
    "        , ucbprem_zipc_code\n",
    "        , ucbprem_city\n",
    "FROM kcmo_water.ucbprem_premises\n",
    "WHERE ucbprem_city = 'KANSAS CITY'\n",
    "        and ucbprem_stat_code_addr = 'MO';\n",
    "'''\n",
    "\n",
    "# Save table in dataframe\n",
    "business = pd.read_sql(business_licenses_query, conn)\n",
    "employer = pd.read_sql(employer_query, conn)\n",
    "water = pd.read_sql(water_services_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Exploration\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Next, we want to get to know the data a bit so we need to know what kind of pre-processing we have to apply. What you want to check for example are formats, missing values, and the quality of your data in general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualizing the top rows of the different datasets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Shape and Properties__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the data frame\n",
    "print(business.shape)\n",
    "print(employer.shape)\n",
    "print(water.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Linkage on Business Names\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Our first record linkage example will be the linking of the MO Employer data with the KCMO Business Licenses data. We will link the tables on Business Name, a field that appears in both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Importance of Pre-Processing\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Data pre-processing is an important step in a data anlysis project in general, in record linkage applications in particular. The goal of pre-processing is to transform messy data into a dataset that can be used in a project workflow.\n",
    "\n",
    "Linking records from different data sources comes with different challenges that need to be addressed by the analyst. The analyst must determine whether or not two entities (individuals, businesses, geographical units) on two different files are the same. This determination is not always easy. In most of the cases there is no common uniquely identifing characteristic for a entity. For example, is Bob Miller from New York the same person as Bob Miller from Chicago in a given dataset? This determination has to be executed carefully because consequences of wrong linkages may be substantial (is person X the same person as the person X on the list of identified terrorists). Pre-processing can help to make better informed decisions.\n",
    "\n",
    "Pre-processing can be difficult because there are a lot of things to keep in mind. For example, data input errors, such as typos, misspellings, truncation, abbreviations, and missing values need to be corrected. Literature shows that pre-processing can improve matches. In some situations, 90% of the improvement in matching efficiency may be due to pre-processing. The most common reason why matching projects fail is lack of time and resources for data cleaning. \n",
    "\n",
    "In the following cells we will walk you through some pre-processing steps. These include but are not limited to removing spaces, parsing fields, and standardizing strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the the most recurring business names in the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "business['legalname'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employer['legal_name'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Right away, we notice that the record linkage between the different datasets will not be straightforward. The variable is messy and non-standardized, similar names can be written differently (in upper-case or lower-case characters, with or without suffixes, etc.) The essential next step is to process the variables in order to make the linkage the most effective and relevant possible.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cleaning String Variables\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In order to clean the Business Name variables, we will use various string transformations and cleaning. The record linkage package comes with a built in cleaning function we can also use. Finaly, RegEx commands can be used for further cleaning (`replace`) and to extract information from strings (`match`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new business name varibles on which we will do the cleaning\n",
    "business['name_clean'] = business['legalname']\n",
    "employer['name_clean'] = employer['legal_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upcasing names\n",
    "business['name_clean'] = business['name_clean'].str.upper()\n",
    "employer['name_clean'] = employer['name_clean'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning names (using the record linkage package tool, see imports)\n",
    "# Clean removes any characters such as '-', '.', '/', '\\', ':'. \n",
    "business['name_clean'] = clean(business['name_clean'], lowercase=False, strip_accents='ascii')\n",
    "employer['name_clean'] = clean(employer['name_clean'], lowercase=False, strip_accents='ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regular Expressions – `regex`\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "Regular expressions (regex) are a way of searching for a character pattern. They can be used for matching or replacing operations in strings.\n",
    "\n",
    "When defining a regular expression search pattern, it is a good idea to start out by writing down, explicitly, in plain English, what you are trying to search for and exactly how you identify when you've found a match.\n",
    "For example, if we look at an author field formatted as \"&lt;last_name&gt; , &lt;first_name&gt; &lt;middle_name&gt;\", in plain English, this is how I would explain where to find the last name: \"starting from the beginning of the line, take all the characters until you see a comma.\"\n",
    "\n",
    "\n",
    "In a regular expression, there are special reserved characters and character classes. For example:\n",
    "- \"`^`\" matches the beginning of the line or cell\n",
    "- \"`.`\" matches any character\n",
    "- \"`+`\" means one or more repetitions of the preceding expressions\n",
    "\n",
    "Anything that is not a special charater or class is just looked for explicitly. A comma, for example, is not a special character in regular expressions, so inserting \"`,`\" in a regular expression will simply match that character in the string.\n",
    "\n",
    "In our example, in order to extract the last name, the resulting regular expression would be:\n",
    "\"`^.+,`\". We start at the beginning of the line ( \"`^`\" ), matching any characters ( \"`.+`\" ) until we come to the literal character of a comma ( \"`,`\" ).\n",
    "\n",
    "\n",
    "_Note: if you want to actually look for one of these reserved characters, it must be escaped, so that, for example, the expression looks for a literal period, rather than the special regular expression meaning of a period. To escape a reserved character in a regular expression, precede it with a back slash ( \"`\\`\" ). For example, \"`\\.`\" will match a \"`.`\" character in a string._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__REGEX CHEATSHEET__\n",
    "\n",
    "\n",
    "    - abc...     Letters\n",
    "    - 123...     Digits\n",
    "    - \\d         Any Digit\n",
    "    - \\D         Any non-Digit Character\n",
    "    - .          Any Character\n",
    "    - \\.         Period\n",
    "    - [a,b,c]    Only a, b or c\n",
    "    - [^a,b,c]   Not a,b, or c\n",
    "    - [a-z]      Characters a to z\n",
    "    - [0-9]      Numbers 0 to 9\n",
    "    - \\w any     Alphanumeric chracter\n",
    "    - \\W         any non-Alphanumeric character\n",
    "    - {m}        m Repetitions\n",
    "    - {m,n}      m to n repetitions\n",
    "    - *          Zero or more repetitions\n",
    "    - +          One or more repetitions\n",
    "    - ?          Optional Character\n",
    "    - \\s         any Whitespace\n",
    "    - \\S         any non-Whitespace character\n",
    "    - ^...$      Starts & Ends\n",
    "    - (...)      Capture Group\n",
    "    - (a(bc))    Capture sub-Group\n",
    "    - (.*)       Capture All\n",
    "    - (abc|def)  Capture abc or def\n",
    "\n",
    "__Examples:__\n",
    "    - `(\\d\\d|\\D)`      will match 22X, 23G, 56H, etc...\n",
    "    - `(\\w)`           will match any characters between 0-9 or a-z\n",
    "    - `(\\w{1-3})`      will match any alphanumeric character of a length of 1 to 3. \n",
    "    - `(spell|spells)` will match spell or spells\n",
    "    - `(corpo?)        will match corp or corpo\n",
    "    - `(feb 2.)`       will match feb 20, feb 21, feb 2a, etc.\n",
    "\n",
    "\n",
    "__Using REGEX to match characters:__\n",
    "\n",
    "In python, to use a regular expression like this to search for matches in a given string, we use the built-in \"`re`\" package ( https://docs.python.org/2/library/re.html ), specifically the \"`re.search()`\" method. To use \"`re.search()`\", pass it first the regular expression you want to use to search, enclosed in quotation marks, and then the string you want to search within. \n",
    "\n",
    "\n",
    "\n",
    "__Using REGEX for replacing characters:__\n",
    "\n",
    "The `re` package also has an \"`re.sub()`\" method used to replace regular expressions by other strings. The method can be applied to an entire pandas column (replacing expression1 with expression2) with the following syntax: `df['variable'].str.replace(r'expression1', 'expression2')`. Note the `r` before the first string to signal we are using regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applications:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove multiple successive spaces:\n",
    "business['name_clean'] = business['name_clean'].str.replace(r'\\s\\s+', ' ')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace(r'\\s\\s+', ' ')\n",
    "\n",
    "# replace U S A by USA\n",
    "business['name_clean'] = business['name_clean'].str.replace(r'\\bU\\sS\\sA\\b', 'USA')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace(r'\\bU\\sS\\sA\\b', 'USA')\n",
    "\n",
    "# replace U S by US\n",
    "business['name_clean'] = business['name_clean'].str.replace(r'\\bU\\sS\\b', 'US')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace(r'\\bU\\sS\\b', 'US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling the word \"THE\":**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the KCMO Business Licenses data, the word \"THE\" at the beginning of a company name has been moved to the end. \n",
    "\n",
    "Instead of moving it back to the front of the name, let's just remove \"THE\" when it is the first or last word of the Business Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"THE\" when it is the First Word:\n",
    "business['name_clean'] = business['name_clean'].str.replace(r'^THE\\b', '')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace(r'^THE\\b', '')\n",
    "\n",
    "# Remove \"THE\" when it is the Last Word:\n",
    "business['name_clean'] = business['name_clean'].str.replace(r'\\bTHE$', '')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace(r'\\bTHE$', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Business Suffixes\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "You may have noticed that several business names finish with a legal suffix (\"CO\", \"INC\", \"LIMITED LIABILITY\", etc.). Unfortunately, examples below will show that these suffixes are not consistent between tables, and they make record linkage impossible. Below we detail one possible way of dealing with legal suffixes. \n",
    "\n",
    "We will start by using regular expressions to standardize legal suffixes. We will then isolate them into a separate variable. When matching the dataframes, we can now choose to match on both business name and legal suffixes, or on business name stripped of the legal suffix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardizing legal suffixes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Company by CO\n",
    "business['name_clean'] = business['name_clean'].str.replace('COMPANY', 'CO')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace('COMPANY', 'CO')\n",
    "\n",
    "# replace CORPORATION by CORP\n",
    "business['name_clean'] = business['name_clean'].str.replace('CORPORATION', 'CORP')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace('CORPORATION', 'CORP')\n",
    "\n",
    "# replace National Association by NA\n",
    "business['name_clean'] = business['name_clean'].str.replace('NATIONAL ASSOCIATION', 'NA')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace('NATIONAL ASSOCIATION', 'NA')\n",
    "\n",
    "# replace N A by NA\n",
    "business['name_clean'] = business['name_clean'].str.replace(r'\\bN\\sA\\b', 'NA')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace(r'\\bN\\sA\\b', 'NA')\n",
    "\n",
    "# replace Limited Liability Company by LLC\n",
    "business['name_clean'] = business['name_clean'].str.replace('LIMITED LIABILITY COMPANY', 'LLC')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace('LIMITED LIABILITY COMPANY', 'LLC')\n",
    "\n",
    "# replace L L C by LLC\n",
    "business['name_clean'] = business['name_clean'].str.replace(r'\\bL\\sL\\sC\\b', 'LLC')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace(r'\\bL\\sL\\sC\\b', 'LLC')\n",
    "\n",
    "# replace Limited Partnership by LP\n",
    "business['name_clean'] = business['name_clean'].str.replace('LIMITED PARTNERSHIP', 'LP')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace('LIMITED PARTNERSHIP', 'LP')\n",
    "\n",
    "# replace L P by LP\n",
    "business['name_clean'] = business['name_clean'].str.replace(r'\\bL\\sP\\b', 'LP')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace(r'\\bL\\sP\\b', 'LP')\n",
    "\n",
    "# replace Partnership by PTNSHP\n",
    "business['name_clean'] = business['name_clean'].str.replace('PARTNERSHIP', 'PTNSHP')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace('PARTNERSHIP', 'PTNSHP')\n",
    "\n",
    "# replace ASSOCIATION by ASSOC\n",
    "business['name_clean'] = business['name_clean'].str.replace('ASSOCIATION', 'ASSOC')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace('ASSOCIATION', 'ASSOC')\n",
    "\n",
    "# replace ASSOCIATES or ASSOCIATE by ASSOC\n",
    "business['name_clean'] = business['name_clean'].str.replace(r'ASSOCIATES?', 'ASSOC')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace(r'ASSOCIATES?', 'ASSOC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any other possible standardizations? Insert them below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a legal suffix variable and removing legal suffixes from business names:**\n",
    "\n",
    "Next, let's isolate the last word of the business name and keep it as a separate variable. We can then remove all business suffixes from the legal names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last word of the clean name\n",
    "business['legal_suffix'] = business['name_clean'].str.split(' ').str.get(-1)\n",
    "employer['legal_suffix'] = employer['name_clean'].str.split(' ').str.get(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of legal suffixes\n",
    "legal = pd.Series(['INC', 'LTD', 'CORP', 'CO', 'LLC', 'LP', 'ASSOC', 'PTNSHP'])\n",
    "\n",
    "# Only keep the legal suffix if it is in the list\n",
    "business['legal_suffix'] = business['legal_suffix'] * business['legal_suffix'].isin(legal)\n",
    "employer['legal_suffix'] = employer['legal_suffix'] * employer['legal_suffix'].isin(legal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the legal suffixes from the clean name\n",
    "business['name_clean'] = business['name_clean'].str.replace(r'\\b(INC|LTD|CORP|CO|LLC|LP|ASSOC|PRNSHP)\\b', '')\n",
    "employer['name_clean'] = employer['name_clean'].str.replace(r'\\b(INC|LTD|CORP|CO|LLC|LP|ASSOC|PRNSHP)\\b', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleaning, striping of strings\n",
    "business['name_clean'] = clean(business['name_clean'], lowercase=False, strip_accents='ascii')\n",
    "employer['name_clean'] = clean(employer['name_clean'], lowercase=False, strip_accents='ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Linkage: Exact Matching on One Field\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that the name standardizing and cleaning is done, we will procede to the record linkage. In this case we will present the simplest form of linkage: exact matching on a single variable (`name_clean`). We use the pandas `merge` for the join. Since we want to keep all data entries from both tables, we will do an outer merge on `name_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_names = pd.merge(business, employer, how = 'outer', on = 'name_clean', indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How did our merge perform?\n",
    "linked_names['_merge'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Only 20% of the business names were merged correctly. You can try improving the merge count by changing the name cleaning and standardizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Linkage on Addresses\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we have the business name cleaned and standardized across datasets, we can use the different name pairs for string matching. However, before doing that we also would like to look at other variables of interest which might help us in better linkage in terms of increasing probability of a correct match (and reducing errors). The variable which seem immediately important is the address of a given establishment. We can pre-process this attribute by using regex functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = business[business['address'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employer = employer[(employer['pl_addr1'].notnull())&(employer['pl_zip'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water = water[(water['ucbprem_street_number'].notnull())\n",
    "              &(water['ucbprem_pdir_code_pre'].notnull())\n",
    "              &(water['ucbprem_street_name'].notnull())\n",
    "              &(water['ucbprem_ssfx_code'].notnull())\n",
    "              &(water['ucbprem_zipc_code'].notnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The water data's address fields are entirely parsed into `street_name`, `street_number`, `ssfx_code` (street suffix code), `city`, and `zipcode`. In the employer data, the address is partially parsed but the `pl_addr1` field still contains the street name, number and ssfx code. In the business data finally all fields are concatenated into the `address` field.\n",
    "\n",
    "The first step will therefore be to standardize the parsing across the 3 tables. All parsed address features will be prefixed with a `p_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address Parsing\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business['p_address'] = clean(business['address'], strip_accents = 'ascii'\n",
    "                              , replace_by_none='[^ \\\\-\\\\_A-Za-z0-9#]+'\n",
    "                              , replace_by_whitespace='[_]').str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "employer['p_address'] = clean(employer['pl_addr1'], strip_accents = 'ascii'\n",
    "                              , replace_by_none='[^ \\\\-\\\\_A-Za-z0-9#]+').str.upper()\n",
    "employer['p_zipcode'] = clean(employer['pl_zip']).str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water['p_street_number'] = clean(water['ucbprem_street_number'], strip_accents = 'ascii').str.upper()\n",
    "water['p_prefix'] = clean(water['ucbprem_pdir_code_pre'], strip_accents = 'ascii').str.upper().fillna('')\n",
    "water['p_street_name'] = clean(water['ucbprem_street_name'], strip_accents = 'ascii').str.upper()\n",
    "water['p_suffix'] = clean(water['ucbprem_ssfx_code'], strip_accents = 'ascii').str.upper().fillna('')\n",
    "water['p_zipcode'] = clean(water['ucbprem_zipc_code'], strip_accents = 'ascii').str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting Zipcode: **\n",
    "\n",
    "US Zipcode typically follows a set pattern of 5 digits or 5 digits followed by a hypen and then 4 digits. We can use this information to extract zipcodes. In the end, we will use only the first 5 digits of the zip code (the first few lines of data inform us that not all observations have the 9-digit zipcode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1\n",
    "# 5 digits of principal zipcode\n",
    "business['p_zipcode_short'] = business['p_address'].str.extract(r'(\\d{5})$')\n",
    "# Breaking the code: \n",
    "# \\d tells that we need a digit\n",
    "# \\d{5} tells that we need 5 digits consecutively\n",
    "# () enclosing brackets tell that we need to extract this information in the new variable\n",
    "# $ tells us that this pattern has to come at the end of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 2\n",
    "# Let's extract the full zipcode from the business data\n",
    "# '5 digits <hyphen> 4 digits'\n",
    "business['p_zipcode_full'] = business['p_address'].str.extract(r'(\\d{5}-\\d{4})$')\n",
    "# Breaking the code: \n",
    "# \\d{5} ---- tells that we need 5 digits consecutively\n",
    "# '-' this is just passing the string exactly as we need it\n",
    "# \\d{4} tells us 4 more digits\n",
    "# $ tells us that this pattern has to come at the end of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we only want to keep the first 5 digits of the zipcode, let's match eithe of \n",
    "# the 2 first patterns and keep the first 5 digits.\n",
    "# Pattern 3\n",
    "# We can also pass both the expressions in our query as an OR.\n",
    "business['p_zipcode'] = business['p_address'].str.extract(r'(\\d{5}-\\d{4}|\\d{5})$').str[:5]\n",
    "# Breaking the code: \n",
    "# | tells us we need one or the other of the two possible zipcodes<span style=\"background-color: #FFFF00\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del business['p_zipcode_full']\n",
    "del business['p_zipcode_short']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the same for the water data and employer data\n",
    "water['p_zipcode'] = water['p_zipcode'].str[:5]\n",
    "employer['p_zipcode'] = employer['p_zipcode'].str[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Extracting State: **\n",
    "\n",
    "The business data seems to have businesses from both Missouri and Kansas. Let's parse out the state name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business['p_state'] = business['p_address'].str.extract(r'\\b(MO|KS)\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then restrict to businesses in Missouri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = business[business['p_state']==\"MO\"].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting city:**\n",
    "\n",
    "Let's extract the address city when the business is in Kansas City. We will then restrict our business data to these observations in Kansas City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business['p_city'] = business['p_address'].str.extract(r'\\b(KANSAS CITY)\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = business[business['p_city']==\"KANSAS CITY\"].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have extrtacted them, let's strip the address field \n",
    "# in the business of zipcode, state, and city\n",
    "business['p_address'] = business['p_address'].str.replace(r'(\\d{5}-\\d{4}|\\d{5})$', '')\n",
    "business['p_address'] = business['p_address'].str.replace(r'\\b(MO|KANSAS CITY)\\b', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Extracting Street Name, Number, Suffix Code:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to match the 3 datasets, we still have to parse out street name, number, and suffix code from the business and employer tables. We will proceed in similar fashion using regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning: triming spaces\n",
    "business['p_address'] = business['p_address'].str.replace(r'\\s\\s+', ' ').str.strip()\n",
    "employer['p_address'] = employer['p_address'].str.replace(r'\\s\\s+', ' ').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business[business['p_address'].str.contains(r'\\bSTE\\b')].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few businesses have addresses with suite information (\"STE\" followed by a number or a letter) or apartment information (\"APT\" followed by a number or letter). Since this does not appear in the other datasets, let's get rid of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern: matching STE followed by any number of digits\n",
    "business['p_address'] = business['p_address'].str.replace(r'\\b((STE|SUITE|BLDG|APT|FL|UNIT|HNGR)\\s\\w+)\\b', '')\n",
    "employer['p_address'] = employer['p_address'].str.replace(r'\\b((STE|SUITE|BLDG|APT|FL|UNIT|HNGR)\\s\\w+)\\b', '')\n",
    "# Breaking up the code: \n",
    "# \\b tells us we start at the begining/end of a word\n",
    "# STE|SUITE|BLDG|APT tells us we need to match one of those strings\n",
    "# \\s tells us we need to match a space\n",
    "# \\w tells that we need a alpha-numeric character\n",
    "# \\w+ tells us that we need any number of consecutive alpha-numeric character (a word)\n",
    "\n",
    "# Pattern: Remove # followed by a number\n",
    "business['p_address'] = business['p_address'].str.replace(r'\\s\\#\\s\\w+\\b', '')\n",
    "employer['p_address'] = employer['p_address'].str.replace(r'\\s\\#\\s\\w+\\b', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "business['p_address'] = business['p_address'].str.replace(r'\\s\\s+', ' ').str.strip()\n",
    "employer['p_address'] = employer['p_address'].str.replace(r'\\s\\s+', ' ').str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the addresses on both the business and the employer tables are in similar format, let's parse out the different elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract street number:\n",
    "business['p_street_number'] = business['p_address'].str.extract(r'^(\\d+|\\d+\\w+|\\d+\\-\\w+)\\b')\n",
    "employer['p_street_number'] = employer['p_address'].str.extract(r'^(\\d+|\\d+\\w+|\\d+\\-\\w+)\\b')\n",
    "# \\d+ matches a succession of digits\n",
    "# \\d+\\w+ matches a succession of digits followed by letters (14C)\n",
    "# \\d+\\-\\w+ matches digits followed by a dash and more characters (14-C or 123-125)\n",
    "\n",
    "# Let's remove the street number from the address\n",
    "business['p_address'] = business['p_address'].str.replace(r'^(\\d+|\\d+\\w)\\b', '')\n",
    "employer['p_address'] = employer['p_address'].str.replace(r'^(\\d+|\\d+\\w)\\b', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "business['p_address'] = business['p_address'].str.replace(r'\\s\\s+', ' ').str.strip()\n",
    "employer['p_address'] = employer['p_address'].str.replace(r'\\s\\s+', ' ').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing left in the address is street name, with prefix and suffix when applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a list of street prefixes based on the ones in the water data\n",
    "prefixes = water[water['p_prefix'].notnull()]['p_prefix'].unique()\n",
    "print(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a list of street suffix codes based on the ones in the water data\n",
    "suffixes = water[water['p_suffix'].notnull()]['p_suffix'].unique()\n",
    "print(suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if first word is a prefix. Replace Null values by empty strings\n",
    "business['p_prefix'] = (business['p_address'].str.extract(r'^(\\w+)') \n",
    "                        * business['p_address'].str.extract(r'^(\\w+)').isin(prefixes)).fillna('')\n",
    "employer['p_prefix'] = (employer['p_address'].str.extract(r'^(\\w+)') \n",
    "                        * employer['p_address'].str.extract(r'^(\\w+)').isin(prefixes)).fillna('')\n",
    "\n",
    "# Test if last word is a suffix. Replace Null values by empty strings\n",
    "business['p_suffix'] = (business['p_address'].str.extract(r'(\\w+)$') \n",
    "                        * business['p_address'].str.extract(r'(\\w+)$').isin(suffixes)).fillna('')\n",
    "employer['p_suffix'] = (employer['p_address'].str.extract(r'(\\w+)$') \n",
    "                        * employer['p_address'].str.extract(r'(\\w+)$').isin(suffixes)).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The street name is whatever is left in the address field once we remove prefix and suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business['p_street_name'] = business['p_address']\n",
    "employer['p_street_name'] = employer['p_address']\n",
    "\n",
    "# Remove prefix when applicable\n",
    "business['p_street_name'] = np.where(business['p_prefix']=='', business['p_street_name']\n",
    "                                     , business['p_street_name'].str.replace(r'^(\\w+)', ''))\n",
    "employer['p_street_name'] = np.where(employer['p_prefix']=='', employer['p_street_name']\n",
    "                                     , employer['p_street_name'].str.replace(r'^(\\w+)', ''))\n",
    "\n",
    "# Remove suffix when applicable\n",
    "business['p_street_name'] = np.where(business['p_suffix']=='', business['p_street_name']\n",
    "                                     , business['p_street_name'].str.replace(r'(\\w+)$', ''))\n",
    "employer['p_street_name'] = np.where(employer['p_suffix']=='', employer['p_street_name']\n",
    "                                     , employer['p_street_name'].str.replace(r'(\\w+)$', ''))\n",
    "\n",
    "del business['p_address']\n",
    "del employer['p_address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "business['p_street_name'] = business['p_street_name'].str.replace(r'\\s\\s+', ' ').str.strip()\n",
    "employer['p_street_name'] = employer['p_street_name'].str.replace(r'\\s\\s+', ' ').str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Methods of Record Linkaged\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we have standardized all three datasets, we can procede to the record linkage. This time, we will use all the fields we created for more accurate linkage. The `recordlinkage` package is a quite powerful tool for you to use when you want to link records using several variables, and you want to avoid linking only records that match perfectly on every variable. It comes with different built in distance metrics and comparison functions, and it also allows you to create your own.\n",
    "\n",
    "Below we will provide examples of record linkage according to 3 methods:\n",
    "- Exact Record Linkage on all fields\n",
    "- Rule-Based Record Linkage\n",
    "- Probabilistic Record Linkage\n",
    "\n",
    "In the examples, we will link employer data with water data, but all methods can be used to link all 3 dataframes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Linkage: Exact Matching on Several Fields\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Exact record linkage is similar to what we did in the first section with business names. Here we use all cleaned variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_data_exact = pd.merge(employer, water, how = 'outer'\n",
    "                             , on = ['p_street_number', 'p_prefix', 'p_street_name', 'p_suffix', 'p_zipcode']\n",
    "                             , indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_data_exact['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Linkage: Rule-Based Matching\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Instead of matching only on exact features, we can try to improve our results by looking at *similar* entries. Indexing allows you to create candidate links, which basically means identifying pairs of data rows which might refer to the same real world entity. This is also called the comparison space (matrix). There are different ways to index data. The easiest is to create a full index and consider every pair a match. This is also the least efficient method, because we will be comparing every row of one dataset with every row of the other dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This line is very slow -- we will not run it here.\n",
    "\n",
    "# # Let's generate a full index first (comparison table of all possible linkage combinations)\n",
    "# indexer = rl.FullIndex()\n",
    "# pairs = indexer.index(employer.head(100), business.head(100))\n",
    "# # Returns a pandas MultiIndex object\n",
    "# print(len(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do better if we actually include our knowledge about the data to eliminate bad links from the start. This can be done through blocking. The recordlinkage packages gives you multiple options for this. For example, you can block by using variables, which means only links exactly equal on specified values will be kept. You can also use a neighborhood index in which the rows in your dataframe are ranked by some value and python will only link between the rows that are closeby.\n",
    "> We suppose here that Zipcode and Street Number are correct. Let's only look for *similar* addresses on street name, prefix, and suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexerBL = rl.BlockIndex(on=['p_zipcode', 'p_street_number', 'p_prefix', 'p_suffix'])\n",
    "pairs2 = indexerBL.index(employer, water)\n",
    "# Returns a pandas MultiIndex object\n",
    "print(len(pairs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate compare object (we are using the blocked ones here)\n",
    "# You want to give python the name of the MultiIndex and the names of the datasets\n",
    "compare = rl.Compare(pairs2, employer, water)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have set up our comparison space. We can start to compare our files and see if we find matches. We will demonstrate an exact match and rule based approaches using distance measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact comparison\n",
    "# This compares all the pairs of strings for exact matches \n",
    "# It is similar to a JOIN-- \n",
    "exact = compare.exact('p_street_name','p_street_name',name='exact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command gives us the probability of match between two strings basis the levenshtein distance\n",
    "# The measure is 0 if there are no similarities in thee string, 1 if it's identical  \n",
    "levenshtein = compare.string('p_street_name','p_street_name', name='levenshtein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command gives us the probability of match between two strings basis the jarowinkler distance\n",
    "# The measure is 0 if there are no similarities in thee string, 1 if it's identical \n",
    "jarowinkler_name = compare.string('p_street_name','p_street_name', method='jarowinkler', name='jarowinkler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally- we can compare the different metrics for an aggregate comparison of their performance \n",
    "print(compare.vectors.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our comparison measures we need to classify the measure in matches and non matches. A rule based approach would be to say if the similarity of our indicators is 0.70 or higher we consider this a match, everything else we won't match. This decision needs to be made by the analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify matches \n",
    "matches = compare.vectors[compare.vectors.max(axis=1) > 0.80]\n",
    "matches = matches.sort_values(\"jarowinkler\")\n",
    "matches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have the list of matches we can fuse our dataset, because at the end we want to have a combined dataset. We are using a function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse(dfA, dfB, dfmatches):\n",
    "    newDF = dfA.copy()\n",
    "    columns = dfB.columns.values\n",
    "    \n",
    "    for col in columns:\n",
    "        newDF[col] = newDF.apply(lambda _: '', axis=1)\n",
    "        \n",
    "    for row in dfmatches.iterrows():\n",
    "        indexA = row[0][0]\n",
    "        indexB = row[0][1]\n",
    "        \n",
    "        for col in columns:\n",
    "            newDF.loc[indexA][col] = dfB.loc[indexB][col]\n",
    "    return newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = fuse(employer, water, matches)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Linkage: Fellegi Sunter\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Another way of classifying records is the Fellegi Sunter Method. If Fellegi Sunter is used to classify record pairs you would follow all the steps we have done so far. However, now we would estimate probabilities to construct weights. These weights will then be applied during the classification to give certain characteristics more importance. For example we are more certain that very unique names are a match than Bob Millers.\n",
    "\n",
    "This method is detailed in the class presentation but will not be implemented here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Parsing\n",
    "\n",
    "* Python online documentation: https://docs.python.org/2/library/string.html#deprecated-string-functions\n",
    "* Python 2.7 Tutorial(Splitting and Joining Strings): http://www.pitt.edu/~naraehan/python2/split_join.html\n",
    "\n",
    "### Regular Expression\n",
    "\n",
    "* Python documentation: https://docs.python.org/2/library/re.html#regular-expression-syntax\n",
    "* Online regular expression tester (good for learning): http://regex101.com/\n",
    "\n",
    "### String Comparators\n",
    "\n",
    "* GitHub page of jellyfish: https://github.com/jamesturk/jellyfish\n",
    "* Different distances that measure the differences between strings:\n",
    "    - Levenshtein distance: https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "    - Damerau–Levenshtein distance: https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance\n",
    "    - Jaro–Winkler distance: https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance\n",
    "    - Hamming distance: https://en.wikipedia.org/wiki/Hamming_distance\n",
    "    - Match rating approach: https://en.wikipedia.org/wiki/Match_rating_approach\n",
    "\n",
    "### Fellegi-Sunter Record Linkage \n",
    "\n",
    "* Introduction to Probabilistic Record Linkage: http://www.bristol.ac.uk/media-library/sites/cmm/migrated/documents/problinkage.pdf\n",
    "* Paper Review: https://www.cs.umd.edu/class/spring2012/cmsc828L/Papers/HerzogEtWires10.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
