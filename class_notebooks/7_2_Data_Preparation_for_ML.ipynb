{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan, and Ridhima Sodhi. \"ADA-KCMO-2018.\" Coleridge Initiative GitHub Repositories. 2018. https://github.com/Coleridge-Initiative/ada-kcmo-2018. [![DOI](https://zenodo.org/badge/119078858.svg)](https://zenodo.org/badge/latestdoi/119078858)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Machine Learning\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Before we begin, run the code cell below to initialize the libraries we'll be using in this assignment. We're already familiar with `numpy`, `pandas`, and `psycopg2` from previous tutorials. Here we'll also be using [`scikit-learn`](http://scikit-learn.org) to fit modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"appliedda\"\n",
    "hostname = \"10.10.2.10\"\n",
    "conn = psycopg2.connect(database=db_name, host = hostname) #database connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Labels\n",
    "\n",
    "Labels are the dependent variables, or *Y* variables, that we are trying to predict. In the machine learning framework, your labels are usually *binary*: true or false, encoded as 1 or 0. In this case, our label is whether an employer at least one year old is likely to disappear in the coming year. We need to pick our year of prediction. We will be looking back one year to see if this employer existed 1 year ago, and forward one year to see if the employer still exists one year from now. \n",
    "> For this example, let's use 2013 (Q1) as our reference year (year of prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(year, db_name = db_name, hostname = hostname, overwrite = False):\n",
    "    \n",
    "    conn = psycopg2.connect(database=db_name, host = hostname) #database connection\n",
    "    cursor = conn.cursor()\n",
    "   \n",
    "    sql_script=\"\"\"\n",
    "    -- First, let's make a list of the employers present at time t: Q1 of 2013\n",
    "\n",
    "    DROP TABLE IF EXISTS ada_kcmo.labels_{year};\n",
    "    CREATE TABLE ada_kcmo.labels_{year} AS\n",
    "    SELECT CONCAT(a.ein, a.run, a.ui_acct) AS id\n",
    "            , a.ein, a.run, a.ui_acct\n",
    "            , case when b.flag = 1 then 0 else 1 end as label \n",
    "    FROM (\n",
    "        SELECT x.ein, x.run, x.ui_acct\n",
    "        FROM (\n",
    "            SELECT ein, run, ui_acct\n",
    "            FROM kcmo_lehd.mo_qcew_employers\n",
    "            WHERE year = {year}\n",
    "            AND qtr = 1\n",
    "        ) AS x\n",
    "        INNER JOIN (\n",
    "            SELECT ein, run, ui_acct\n",
    "            FROM kcmo_lehd.mo_qcew_employers\n",
    "            WHERE year = {year}-1\n",
    "            AND qtr = 1\n",
    "        ) AS y\n",
    "        ON x.ein = y.ein AND x.run = y.run AND x.ui_acct = y.ui_acct\n",
    "    ) AS a\n",
    "    LEFT JOIN (\n",
    "        SELECT ein, run, ui_acct, 1 as flag \n",
    "        FROM kcmo_lehd.mo_qcew_employers\n",
    "        WHERE year = {year}+1\n",
    "        AND qtr = 1   \n",
    "    ) AS b\n",
    "    ON a.ein = b.ein AND a.run = b.run AND a.ui_acct = b.ui_acct;\n",
    "    \n",
    "    ALTER TABLE ada_kcmo.labels_{year} OWNER TO ada_kcmo_admin;\n",
    "\n",
    "    COMMIT;\n",
    "\n",
    "    \"\"\".format(year = year)\n",
    "    \n",
    "    # Let's check if the table already exists:\n",
    "    cursor.execute('''\n",
    "    SELECT * FROM information_schema.tables \n",
    "    WHERE table_name = 'labels_{year}' \n",
    "    AND table_schema = 'ada_kcmo';\n",
    "    '''.format(year = year))\n",
    "    # Let's write table if it does not exist (or if overwrite = True)\n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "        cursor.execute(sql_script)\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "    df = pd.read_sql('SELECT * FROM ada_kcmo.labels_{}'.format(year), conn)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = generate_labels(2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index = df_labels['label'], columns =  'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features\n",
    "\n",
    "Our features are our independent variables or predictors. Good features make machine learning systems effective. \n",
    "The better the features the easier it is the capture the structure of the data. You generate features using domain knowledge. In general, it is better to have more complex features and a simpler model rather than vice versa. Keeping the model simple makes it faster to train and easier to understand rather then extensively searching for the \"right\" model and \"right\" set of parameters. \n",
    "\n",
    "Machine Learning Algorithms learn a solution to a problem from sample data. The set of features is the best representation of the sample data to learn a solution to a problem. \n",
    "\n",
    "- **Feature engineering** is the process of transforming raw data into features that better represent the underlying problem/data/structure  to the predictive models, resulting in improved model accuracy on unseen data.\" ( from [Discover Feature Engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) ).  In text, for example, this might involve deriving traits of the text like word counts, verb counts, or topics to feed into a model rather than simply giving it the raw text.\n",
    "\n",
    "Example of feature engineering are: \n",
    "\n",
    "- **Transformations**, such a log, square, and square root.\n",
    "- **Dummy (binary) variables**, also known as *indicator variables*, often done by taking categorical variables\n",
    "(such as city) which do not have a numerical value, and adding them to models as a binary value.\n",
    "- **Discretization**. Several methods require features to be discrete instead of continuous. This is often done \n",
    "by binning, which you can do by equal width. \n",
    "- **Aggregation.** Aggregate features often constitute the majority of features for a given problem. These use \n",
    "different aggregation functions (*count, min, max, average, standard deviation, etc.*) which summarize several\n",
    "values into one feature, aggregating over varying windows of time and space. For example, given urban data, \n",
    "we would want to calculate the *number* (and *min, max, mean, variance*, etc.) of crimes within an *m*-mile radius\n",
    "of an address in the past *t* months for varying values of *m* and *t*, and then use all of them as features.\n",
    "\n",
    ">Our preliminary features are the following\n",
    ">\n",
    ">- `n_spells` (Aggregation): Total number of spells someonse has had up until the date of prediction.\n",
    ">- `age` (Transformation): The age feature is created by substracting the bdate_year with the current year of prediction. \n",
    ">- `edlevel` (Binary): 0 if the person has less than a high school education and 1 if they are more than a high school education. \n",
    ">- `workexp` (Binary): 0 if no work experience 1 if there is some sort of work experience\n",
    ">- `married` (Binary): 1 if the person is married 0 if they are not. \n",
    ">- `gender`: (Binary) 1(male) 2(female)\n",
    ">- `n_days_last_spell`: (Aggregation) The number of days since a person's last spell.\n",
    ">- `(foodstamp, tanf, granf)`: (Binary) 0 if the last benefit was not foodstamp, tanf or grantf, 1 if it was"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### New vs Old Employers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a first binary feature to defining \"old\" and \"new\" firms. Old firms are determined according to age cutoff, with a default value is 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def employer_age_features(year, age_cutoff = 5, db_name = db_name, hostname = hostname, overwrite = False):\n",
    "    \n",
    "    conn = psycopg2.connect(database=db_name, host = hostname) #database connection\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    sql_script = '''\n",
    "    DROP TABLE IF EXISTS ada_kcmo.features_age_{year};\n",
    "    CREATE TABLE ada_kcmo.features_age_{year} AS\n",
    "    SELECT a.*, CASE WHEN b.flag = 1 THEN 0 ELSE 1 END AS new_employer\n",
    "    FROM (\n",
    "        SELECT ein, run, ui_acct \n",
    "        FROM ada_kcmo.labels_{year}\n",
    "    ) AS a\n",
    "    LEFT JOIN (\n",
    "        SELECT ein, run, ui_acct, 1 as flag \n",
    "        FROM kcmo_lehd.mo_qcew_employers\n",
    "        WHERE year = {year}-{age_cutoff}\n",
    "        AND qtr = 1   \n",
    "    ) AS b\n",
    "    ON a.ein = b.ein AND a.run = b.run AND a.ui_acct = b.ui_acct;\n",
    "    \n",
    "    ALTER TABLE ada_kcmo.features_age_{year} OWNER TO ada_kcmo_admin;    \n",
    "    \n",
    "    COMMIT;\n",
    "    '''.format(year = year, age_cutoff = age_cutoff)\n",
    "    \n",
    "    # Let's check if the table already exists:\n",
    "    cursor.execute('''\n",
    "    SELECT * FROM information_schema.tables \n",
    "    WHERE table_name = 'features_age_{year}' \n",
    "    AND table_schema = 'ada_kcmo';\n",
    "    '''.format(year = year))\n",
    "    # Let's write table if it does not exist (or if overwrite = True)\n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "        cursor.execute(sql_script)\n",
    "    \n",
    "    cursor.close()\n",
    "        \n",
    "    df = pd.read_sql('SELECT * FROM ada_kcmo.features_age_{}'.format(year), conn)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = employer_age_features(2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWI Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of features we would like to include are the QWI statistics. Since we are looking at firms what are at least one year old, it might be interesting to consider both the current QWI numbers, and the numbers from the year before. \n",
    "\n",
    "Note that these statistics are taken at company level (EIN), instead of individual entity level (combination of EIN, RUN, and UI Account Number). This is because QWI is calculated at firm level. We therefore merge on EIN, instead of using all three variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(database = db_name, host = hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwi = pd.read_sql('SELECT * FROM ada_kcmo.qwi_ein_{year}_1'.format(year = 2013), conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's also consider the QWI statistics one year before our prediction quarter. We can create additional features accounting for the variation in level of the QWI statustics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwi_m1 = pd.read_sql('SELECT* FROM ada_kcmo.qwi_ein_{year}_1'.format(year = 2012), conn)\n",
    "df_qwi_m1 = df_qwi_m1.add_prefix('m1_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwi_m1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwi = pd.merge(df_qwi, df_qwi_m1, how = 'left', left_on = 'ein', right_on = 'm1_ein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['nb_jobs_current_qtr', 'emp_current_qtr'\n",
    "            , 'emp_4qtrs_ago', 'emp_3qtrs_ago', 'emp_2qtrs_ago', 'emp_prev_qtr', 'emp_next_qtr'\n",
    "            , 'emp_begin_qtr', 'emp_end_qtr', 'emp_full_qtr'\n",
    "            , 'accessions_current', 'accessions_consecutive_qtr', 'accessions_full_qtr'\n",
    "            , 'separations', 'new_hires', 'recalls']:\n",
    "    m1_var = 'm1_{}'.format(var)\n",
    "    change_var = 'change_{}'.format(var)\n",
    "    df_qwi[change_var] = df_qwi[var] - df_qwi[m1_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Missing Values\n",
    "`NULL` values will make it impossible to run our Machine Leaning Algorithm. Let's see if there are any in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnan_rows = df_qwi.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwi[isnan_rows].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows_df_qwi = df_qwi.shape[0]\n",
    "nrows_df_qwi_isnan = df_qwi[isnan_rows].shape[0]\n",
    "print('%of rows with NaNs: {} '.format(float(nrows_df_qwi_isnan)/nrows_df_qwi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwi = df_qwi[~isnan_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the two previous queries into a unique SQL query that will retrive all the relevant QWI statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwi_features(year, db_name = db_name, hostname = hostname, overwrite = False):\n",
    "    conn = psycopg2.connect(database=db_name, host = hostname) #database connection\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    sql_script = '''\n",
    "    DROP TABLE IF EXISTS ada_kcmo.features_qwi_{year};\n",
    "    CREATE TABLE ada_kcmo.features_qwi_{year} AS\n",
    "    SELECT a.*\n",
    "            , b.nb_jobs_current_qtr AS m1_nb_jobs_current_qtr\n",
    "            , b.emp_current_qtr AS m1_emp_current_qtr\n",
    "            , b.emp_4qtrs_ago AS m1_emp_4qtrs_ago\n",
    "            , b.emp_3qtrs_ago AS m1_emp_3qtrs_ago\n",
    "            , b.emp_2qtrs_ago AS m1_emp_2qtrs_ago\n",
    "            , b.emp_prev_qtr AS m1_emp_prev_qtr\n",
    "            , b.emp_next_qtr AS m1_emp_next_qtr\n",
    "            , b.emp_begin_qtr AS m1_emp_begin_qtr\n",
    "            , b.emp_end_qtr AS m1_emp_end_qtr\n",
    "            , b.emp_full_qtr AS m1_emp_full_qtr\n",
    "            , b.accessions_current AS m1_accessions_current\n",
    "            , b.accessions_consecutive_qtr AS m1_accessions_consecutive_qtr\n",
    "            , b.accessions_full_qtr AS m1_accessions_full_qtr\n",
    "            , b.separations AS m1_separations\n",
    "            , b.new_hires AS m1_new_hires\n",
    "            , b.recalls AS m1_recalls\n",
    "    FROM(\n",
    "        SELECT * \n",
    "        FROM ada_kcmo.qwi_ein_{year}_1\n",
    "    ) AS a\n",
    "    LEFT JOIN (\n",
    "        SELECT *\n",
    "        FROM ada_kcmo.qwi_ein_{year_m1}_1\n",
    "    ) AS b\n",
    "    ON a.ein = b.ein;\n",
    "    \n",
    "    ALTER TABLE ada_kcmo.features_qwi_{year} OWNER TO ada_kcmo_admin; \n",
    "    \n",
    "    COMMIT;\n",
    "    '''.format(year = year, year_m1 = year-1)\n",
    "    \n",
    "    # Let's check if the table already exists:\n",
    "    cursor.execute('''\n",
    "    SELECT * FROM information_schema.tables \n",
    "    WHERE table_name = 'features_qwi_{year}'\n",
    "    AND table_schema = 'ada_kcmo';\n",
    "    '''.format(year = year))\n",
    "    # Let's write table if it does not exist (or if overwrite = True)\n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "        cursor.execute(sql_script)\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "    df = pd.read_sql('SELECT * FROM ada_kcmo.features_qwi_{};'.format(year), conn)\n",
    "    \n",
    "    for var in ['nb_jobs_current_qtr', 'emp_current_qtr'\n",
    "                , 'emp_4qtrs_ago', 'emp_3qtrs_ago', 'emp_2qtrs_ago', 'emp_prev_qtr', 'emp_next_qtr'\n",
    "                , 'emp_begin_qtr', 'emp_end_qtr', 'emp_full_qtr'\n",
    "                , 'accessions_current', 'accessions_consecutive_qtr', 'accessions_full_qtr'\n",
    "                , 'separations', 'new_hires', 'recalls']:\n",
    "        m1_var = 'm1_{}'.format(var)\n",
    "        change_var = 'change_{}'.format(var)\n",
    "        df[change_var] = df[var] - df[m1_var]\n",
    "   \n",
    "    # Remove NULL rows\n",
    "    isnan_rows = df.isnull().any(axis=1)\n",
    "    df = df[~isnan_rows]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwi = qwi_features(2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Wages and Employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use wage and employee statistics from the MO wage records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(database = db_name, host = hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT ein, run, ui_acct\n",
    "        , mon1_empl+mon2_empl+mon3_empl AS total_empl\n",
    "        , total_wage \n",
    "FROM kcmo_lehd.mo_qcew_employers \n",
    "WHERE year = 2013 AND qtr = 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wages = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an additional feature for average monthly wage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wages['avg_wage'] = df_wages['total_wage']/df_wages['total_empl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation \n",
    "\n",
    "It is important to to do a quick check of our matrix to see if we have any outlier values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wages.describe(include = 'all', percentiles=[0.01,0.05,0.25,0.50,0.75,0.95,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of some data inconsistencies in total employees and total wages, some average wages could not be calculated (when `total_empl == 0` and `total_wages == 0`) and some have `inf` values (when `total_empl == 0`). These `NULL` and `inf` values will be problematic for the machine learning algorithm. \n",
    "\n",
    "Let's impute these missing values to the medial value of all average wages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((df_wages['avg_wage'].isnull()) | (df_wages['avg_wage'] == inf))\n",
    "vals_to_replace = df_wages[mask]['avg_wage'].values\n",
    "df_wages['avg_wage'].replace(vals_to_replace,np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_avg_wage = df_wages['avg_wage'].median()\n",
    "print(median_avg_wage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wages['avg_wage'].fillna(median_avg_wage, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wages.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers \n",
    "\n",
    "Some values of average wage still seem impossible for very unlikely. Certain employers can have an average wage of 0, and some outliers have average wages far exceeding the 99th percentile. These are things you'd want to do a \"sanity check\" on with someone who knows the data will.\n",
    "\n",
    "Here, we believe these are data errors and chose to drop these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all rows where the wage is 0 or above 50,000 per month\n",
    "outlier_rows = ((df_wages['avg_wage'] == 0) | (df_wages['avg_wage'] > 50000))\n",
    "df_wages[outlier_rows].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows_wages = df_wages.shape[0]\n",
    "nrows_wages_outliers = df_wages[outlier_rows].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%of outlier rows: {} '.format(float(nrows_wages_outliers)/nrows_wages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wages = df_wages[~outlier_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling of Values\n",
    "\n",
    "Certain models will have issue with the distance between features such as number of employees and average wages. Number of employees is typically a number between 1 and 100 while average wages are usually between 1000 and 4000. In order to circumvent this problem we can scale our features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: let's scale average wages:\n",
    "min_avg_wage = df_wages['avg_wage'].min()\n",
    "max_avg_wage = df_wages['avg_wage'].max()\n",
    "\n",
    "df_wages['avg_wage_scaled'] = (df_wages['avg_wage']-min_avg_wage)/(max_avg_wage-min_avg_wage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_wages[['avg_wage', 'avg_wage_scaled']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the original var by the scaled var\n",
    "df_wages['avg_wage'] = df_wages['avg_wage_scaled']\n",
    "del df_wages['avg_wage_scaled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generic function can be used to scale other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_var(df, var):\n",
    "    min_var = df[var].min()\n",
    "    max_var = df[var].max()\n",
    "    scaled_var = '{}_scaled'.format(var)\n",
    "\n",
    "    df[scaled_var] = (df[var] - min_var)/(max_var - min_var)\n",
    "    \n",
    "    return df[scaled_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wages['total_empl_scaled'] = scaling_var(df_wages, 'total_empl')\n",
    "df_wages['total_wage_scaled'] = scaling_var(df_wages, 'total_wage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the steps above can be summarized in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wages_features(year, db_name = db_name, hostname = hostname, overwrite = False):\n",
    "    \n",
    "    conn = psycopg2.connect(database=db_name, host = hostname) #database connection\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    sql_script = '''\n",
    "    DROP TABLE IF EXISTS ada_kcmo.features_wages_{year};\n",
    "    CREATE TABLE ada_kcmo.features_wages_{year} AS    \n",
    "    SELECT ein, run, ui_acct\n",
    "            , mon1_empl+mon2_empl+mon3_empl AS total_empl\n",
    "            , total_wage \n",
    "    FROM kcmo_lehd.mo_qcew_employers \n",
    "    WHERE year = {year} AND qtr = 1;\n",
    "    \n",
    "    ALTER TABLE ada_kcmo.features_wages_{year} OWNER TO ada_kcmo_admin; \n",
    "    \n",
    "    COMMIT;\n",
    "    '''.format(year = year)\n",
    "    \n",
    "    # Let's check if the table already exists:\n",
    "    cursor.execute('''\n",
    "    SELECT * FROM information_schema.tables \n",
    "    WHERE table_name = 'features_wages_{year}'\n",
    "    AND table_schema = 'ada_kcmo';\n",
    "    '''.format(year = year))\n",
    "    \n",
    "    # Let's write table if it does not exist (or if overwrite = True)\n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "        cursor.execute(sql_script)\n",
    "    \n",
    "    cursor.close()    \n",
    "    \n",
    "    df = pd.read_sql('SELECT * FROM ada_kcmo.features_wages_{}'.format(year), conn)\n",
    "    df['avg_wage'] = df['total_wage']/df['total_empl']\n",
    "    \n",
    "    # Flag null, infinite average wage values\n",
    "    mask = ((df['avg_wage'].isnull()) | (df['avg_wage'] == inf))\n",
    "    vals_to_replace = df[mask]['avg_wage'].values\n",
    "    df['avg_wage'].replace(vals_to_replace,np.NaN, inplace=True)\n",
    "    \n",
    "    # Impute the median wage value\n",
    "    df['avg_wage'].fillna(df['avg_wage'].median(), inplace=True)\n",
    "    \n",
    "    # Remove Outliers\n",
    "    outlier_rows = ((df['avg_wage'] == 0) | (df['avg_wage'] > 50000))\n",
    "    df_wages = df[~outlier_rows]\n",
    "    \n",
    "    # Scaling values\n",
    "    df['total_wage_scaled'] = scaling_var(df, 'total_wage')\n",
    "    df['total_empl_scaled'] = scaling_var(df, 'total_empl')\n",
    "    df['avg_wage_scaled'] = scaling_var(df, 'avg_wage')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wages = wages_features(2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine all our subset of features into one features table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.merge(df_age, df_qwi, how = 'left', on = 'ein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.merge(df_features, df_wages, how = 'left', on = ['ein', 'run', 'ui_acct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge our features with our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = pd.merge(df_labels, df_features, how = 'left', on = ['ein', 'run', 'ui_acct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's now write the table into our class schema so we can use it for the Machine Learning notebook. In order to write a data table, we have to create an engine with SQLAlchemy (see notebook on Databases for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if the table already exists:  \n",
    "conn = psycopg2.connect(database=db_name, host = hostname) #database connection\n",
    "cursor = conn.cursor()    \n",
    "cursor.execute('''\n",
    "SELECT * FROM information_schema.tables \n",
    "WHERE table_name = 'table_employers_2013'\n",
    "AND table_schema = 'ada_kcmo';\n",
    "''')\n",
    "\n",
    "# Let's write table if it does not exist (or if overwrite = True)\n",
    "overwrite = False\n",
    "if not(cursor.rowcount) or overwrite:\n",
    "    engine = create_engine('postgresql://{}/{}'.format(hostname, db_name))\n",
    "    df_table.to_sql('table_employers_2013', engine, schema = 'ada_kcmo', index = False, if_exists='replace')\n",
    "    \n",
    "    # Change Admin rights of table to admin\n",
    "    conn = psycopg2.connect(database = db_name, host = hostname)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('ALTER TABLE ada_kcmo.table_employers_2013 OWNER TO ada_kcmo_admin; COMMIT;')\n",
    "\n",
    "cursor.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2013 = pd.read_sql('SELECT * FROM ada_kcmo.table_employers_2013 LIMIT 100', conn)\n",
    "table_2013.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Function for Label and Features Generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have recapitulated all the above steps into a general function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table(year, db_name = db_name, hostname = hostname, schema = 'ada_kcmo', overwrite = False):\n",
    "    \n",
    "    # Generate Labels\n",
    "    print(\"Generating labels\")\n",
    "    df_label = generate_labels(year, db_name = db_name, hostname = hostname, overwrite = overwrite)\n",
    "    \n",
    "    # Generate Features\n",
    "    print(\"Generating features\")\n",
    "    df_age = employer_age_features(year, db_name = db_name, hostname = hostname, overwrite = overwrite)\n",
    "    df_qwi = qwi_features(year, db_name = db_name, hostname = hostname, overwrite = overwrite)\n",
    "    df_wages = wages_features(year, db_name = db_name, hostname = hostname, overwrite = overwrite)\n",
    "    \n",
    "    # Merge Labels and Features together\n",
    "    print(\"Merging labels and features\")\n",
    "    df_table = pd.merge(df_label, df_age, how = 'inner', on = ['ein', 'run', 'ui_acct'])\n",
    "    df_table = pd.merge(df_table, df_qwi, how = 'inner', on = 'ein')\n",
    "    df_table = pd.merge(df_table, df_wages, how = 'inner', on = ['ein', 'run', 'ui_acct'])\n",
    "    \n",
    "    # Removing NULL values\n",
    "    isnan_rows = df_table.isnull().any(axis=1)\n",
    "    df_table = df_table[~isnan_rows]\n",
    "    \n",
    "    # Write Table\n",
    "    print(\"Writing table\")\n",
    "    \n",
    "    # Let's check if the table already exists:  \n",
    "    conn = psycopg2.connect(database=db_name, host = hostname) #database connection\n",
    "    cursor = conn.cursor()    \n",
    "    cursor.execute('''\n",
    "    SELECT * FROM information_schema.tables \n",
    "    WHERE table_name = 'table_employers_{year}'\n",
    "    AND table_schema = '{schema}';\n",
    "    '''.format(year = year, schema = schema))\n",
    "    \n",
    "    # Let's write table if it does not exist (or if overwrite = True)\n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "        table_name = 'table_employers_{}'.format(year)\n",
    "        engine = create_engine('postgresql://{}/{}'.format(hostname, db_name))\n",
    "        df_table.to_sql(table_name, engine, schema = 'ada_kcmo', index = False, if_exists='replace')\n",
    "        \n",
    "        # Change Admin rights of table to admin\n",
    "        conn = psycopg2.connect(database = db_name, host = hostname)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('ALTER TABLE ada_kcmo.table_employers_{} OWNER TO ada_kcmo_admin; COMMIT;'.format(year))\n",
    "\n",
    "    cursor.close()        \n",
    "    \n",
    "    return df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_2013 = generate_table(2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_2014 = generate_table(2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_2014.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
