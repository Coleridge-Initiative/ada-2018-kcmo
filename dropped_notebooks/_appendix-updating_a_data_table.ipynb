{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and updating a data table\n",
    "\n",
    "As one derives more complicated data about entities of interest in research, it can be helpful to keep that information together in one place.  To do this, one can create a table that is intended to hold finalized data and update it with new derived information as that information is generated.\n",
    "\n",
    "Below we outline:\n",
    "\n",
    "- creating a table, including updating its ownership and permissions such that others in your project can access and work with it.\n",
    "- Updating that table by:\n",
    "\n",
    "    - Creating a new copy of the table with additional rows, then deleting the old version.\n",
    "    - Adding a column to a table, then updating columns in a table either by:\n",
    "\n",
    "        - looping in Python and updating row-by-row\n",
    "        - updating the column (or columns) all at once with a single UPDATE command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Setup - Imports](#Setup---Imports)\n",
    "- [Setup - Database](#Setup---Database)\n",
    "\n",
    "    - [Setup - Database - SQLAlchemy](#Setup---Database---SQLAlchemy)\n",
    "    - [Setup - Database - psycopg2](#Setup---Database---psycopg2)\n",
    "    - [Setup - Database - rollback if needed](#Setup---Database---rollback-if-needed)\n",
    "    \n",
    "- [Overview](#Overview)\n",
    "- [Create a new table](#Create-a-new-table)\n",
    "\n",
    "    - [CREATE table](#CREATE-table)\n",
    "    - [CREATE table from results of SELECT](#CREATE-table-from-results-of-SELECT)\n",
    "    - [Provide access to table](#Provide-access-to-table)\n",
    "\n",
    "- [Adding columns to data table](#Adding-columns-to-data-table)\n",
    "\n",
    "    - [Option 1 - Add columns using CREATE TABLE AS](#Option-1---Add-columns-using-CREATE-TABLE-AS)\n",
    "    - [Add columns to existing table](#Add-columns-to-existing-table)\n",
    "\n",
    "        - [Option 2 - UPDATE all at once using SQL](#Option-2---UPDATE-all-at-once-using-SQL)\n",
    "        - [Option 3 - For each row, calculate and store value using Python](#Option-3---For-each-row,-calculate-and-store-value-using-Python)\n",
    "        \n",
    "- [Doing work step-by-step using temporary tables](#Doing-work-step-by-step-using-temporary-tables)\n",
    "- [Choosing between the three options](#Choosing-between-the-three-options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup - Imports\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general use imports\n",
    "import datetime\n",
    "import glob\n",
    "import inspect\n",
    "import numpy\n",
    "import os\n",
    "import six\n",
    "import warnings\n",
    "\n",
    "# pandas-related imports\n",
    "import pandas\n",
    "import sqlalchemy\n",
    "\n",
    "# CSV file reading-related imports\n",
    "import csv\n",
    "\n",
    "# database interaction imports\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "\n",
    "print( \"Imports loaded at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup - Database\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema name\n",
    "schema_name = \"ildoc\"\n",
    "\n",
    "# admin role\n",
    "admin_role = schema_name + \"_admin\"\n",
    "select_role = schema_name + \"_select\"\n",
    "\n",
    "# ==> database table names - just like file names above, store reused database information in variables here.\n",
    "\n",
    "# work table name\n",
    "work_db_table = \"person\"\n",
    "\n",
    "print( \"Database variables initialized at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection properties\n",
    "db_host = \"10.10.2.10\"\n",
    "db_port = -1\n",
    "db_username = None\n",
    "db_password = None\n",
    "db_name = \"appliedda\"\n",
    "\n",
    "print( \"Database connection properties initialized at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Database - `SQLAlchemy`\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Initialize database connections.  First, SQLAlchemy engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize database connections\n",
    "# Create connection to database using SQLAlchemy\n",
    "#     (3 '/' indicates use enviroment settings for username, host, and port)\n",
    "sqlalchemy_connection_string = \"postgresql://\"\n",
    "\n",
    "if ( ( db_host is not None ) and ( db_host != \"\" ) ):\n",
    "    sqlalchemy_connection_string += str( db_host )\n",
    "#-- END check to see if host --#\n",
    "\n",
    "sqlalchemy_connection_string += \"/\"\n",
    "\n",
    "if ( ( db_name is not None ) and ( db_name != \"\" ) ):\n",
    "    sqlalchemy_connection_string += str( db_name )\n",
    "#-- END check to see if host --#\n",
    "\n",
    "# create engine.\n",
    "pgsql_engine = sqlalchemy.create_engine( sqlalchemy_connection_string )\n",
    "\n",
    "print( \"SQLAlchemy engine created at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Database - `psycopg2`\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "And then a direct psycopg2 connection and cursor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create psycopg2 connection to Postgresql\n",
    "\n",
    "# example connect() call that uses all the possible parameters\n",
    "#pgsql_connection = psycopg2.connect( host = db_host, port = db_port, database = db_name, user = db_username, password = db_password )\n",
    "\n",
    "# for SQLAlchemy, just needed database name. Same for DBAPI?\n",
    "pgsql_connection = psycopg2.connect( host = db_host, database = db_name )\n",
    "\n",
    "print( \"Postgresql connection to database \\\"\" + db_name + \"\\\" created at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cursor that maps column names to values\n",
    "pgsql_cursor = pgsql_connection.cursor( cursor_factory = psycopg2.extras.DictCursor )\n",
    "\n",
    "print( \"Postgresql cursor for database \\\"\" + db_name + \"\\\" created at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Database - rollback if needed\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rollback, in case you need it.\n",
    "pgsql_connection.rollback()\n",
    "\n",
    "print( \"Postgresql connection for database \\\"\" + db_name + \"\\\" rolled back at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As one derives more complicated data about entities of interest in research, it can be helpful to keep that information together in one place.  To do this, one can create a table that is intended to hold finalized data and update it with new derived information as that information is generated.\n",
    "\n",
    "When building up a data table like this, the basic strategy is to start with a table where each unit of interest for your analysis gets its own row (so, one person per row, or one company per row, or one set of quarterly earnings per row), then add columns to the table as you figure out logic to derive data points for use in your analysis and modeling.\n",
    "\n",
    "There are a number of ways you can do this.  Below, we will use an example of creating a data table for heads of household from the IDHS benefits data.  We will show how to create a table with a row per person, then show three options for adding columns to hold new variables/features:\n",
    "\n",
    "- Create a copy of the table that also includes new columns, using the `CREATE TABLE ... AS` SQL statement.\n",
    "- Add the columns to your table, then update by:\n",
    "\n",
    "    - using the SQL `UPDATE` statement to derive and set the values for the column using SQL.\n",
    "    - using Python and SQL to derive column values in Python, then update the table row-by-row.\n",
    "    \n",
    "\n",
    "Each method has different strengths and weaknesses, and we'll discuss those in a summary section at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new table\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, you'll want create a table to hold your analysis data and configure it so it is usable by the others in your project.\n",
    "\n",
    "For this example, we'll create a table to hold the following information for each head of household:\n",
    "\n",
    "- recptno\n",
    "- sex\n",
    "- rac\n",
    "- rootrace\n",
    "- foreignbn\n",
    "- ssn_hash\n",
    "- fname_hash\n",
    "- lname_hash\n",
    "- birth_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE table\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The basic syntax for creating a table is:\n",
    "\n",
    "    CREATE TABLE <schema_name>.<table_name>\n",
    "    (\n",
    "        <column_list>\n",
    "    );\n",
    "    \n",
    "So, to create a table to hold our head of household information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create table - declare variables\n",
    "table_name = \"\"\n",
    "table_name = work_db_table\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \"CREATE TABLE \" + schema_name + \".\" + table_name\n",
    "\n",
    "# add columns\n",
    "sql_string += \" (\"\n",
    "sql_string += \" id BIGSERIAL PRIMARY KEY\"\n",
    "sql_string += \", recptno bigint\"\n",
    "sql_string += \", sex bigint\"\n",
    "sql_string += \", rac bigint\"\n",
    "sql_string += \", rootrace bigint\"\n",
    "sql_string += \", foreignbn bigint\"\n",
    "sql_string += \", ssn_hash text\"\n",
    "sql_string += \", fname_hash text\"\n",
    "sql_string += \", lname_hash text\"\n",
    "sql_string += \", birth_date date\"\n",
    "sql_string += \" )\"\n",
    "sql_string += \";\"\n",
    "\n",
    "print( \"====> \" + str( sql_string ) )\n",
    "\n",
    "# run SQL\n",
    "pgsql_cursor.execute( sql_string )\n",
    "pgsql_connection.commit()\n",
    "\n",
    "#temp_df = pandas.read_sql( sql_string, con = pgsql_engine )\n",
    "#temp_length = len( temp_df )\n",
    "#temp_df.head( n = temp_length )\n",
    "\n",
    "print( \"====> \" + str( sql_string ) + \" completed at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE table from results of SELECT\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "When you start with a data table you want to build on, you can use the CREATE TABLE AS syntax to make a copy of an existing table, populated with the data from that table (or a subset).\n",
    "\n",
    "The most basic form of the CREATE TABLE AS syntax:\n",
    "\n",
    "    CREATE TABLE <schema_name>.<table_name>\n",
    "    AS\n",
    "    SELECT <column_list> FROM <schema_name>.<table_name>;\n",
    "    \n",
    "So, to create a data work table that includes our columns of interest for all the people in the `idhs.hh_member` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create table - declare variables\n",
    "table_name = \"\"\n",
    "table_name = work_db_table\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \"CREATE TABLE \" + schema_name + \".\" + table_name\n",
    "sql_string += \" AS SELECT\"\n",
    "sql_string += \" recptno\"\n",
    "sql_string += \", sex\"\n",
    "sql_string += \", rac\"\n",
    "sql_string += \", rootrace\"\n",
    "sql_string += \", foreignbn\"\n",
    "sql_string += \", ssn_hash\"\n",
    "sql_string += \", fname_hash\"\n",
    "sql_string += \", lname_hash\"\n",
    "sql_string += \", birth_date\"\n",
    "sql_string += \" FROM idhs.hh_member\"\n",
    "#sql_string += \" WHERE EXTRACT( year FROM birth_date ) = 1976\"\n",
    "#sql_string += \" LIMIT 1000\"\n",
    "sql_string += \";\"\n",
    "\n",
    "print( \"====> \" + str( sql_string ) )\n",
    "\n",
    "# run SQL\n",
    "pgsql_cursor.execute( sql_string )\n",
    "pgsql_connection.commit()\n",
    "\n",
    "#temp_df = pandas.read_sql( sql_string, con = pgsql_engine )\n",
    "#temp_length = len( temp_df )\n",
    "#temp_df.head( n = temp_length )\n",
    "\n",
    "print( \"====> \" + str( sql_string ) + \" completed at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create a table this way, you can expand the `AS` `SELECT` to join multiple tables and create derived columns (more on this later), and you can also use its `WHERE` clause to filter and subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide access to table\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Once you've created a new table, you need to provide access to it for the rest of your project members.  This includes:\n",
    "\n",
    "- Changing the ownership of the table so it is owned by your project's database admin role (\"`<project>_admin`\").\n",
    "- Giving your project's admin role all privileges on the table.\n",
    "- Giving your project's read-only role (\"`<project>_select`\") \"SELECT\" privileges on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE ownership - declare variables\n",
    "table_name = \"\"\n",
    "table_name = work_db_table\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \"ALTER TABLE \" + schema_name + \".\" + table_name\n",
    "sql_string += \" OWNER TO \" + admin_role\n",
    "sql_string += \";\"\n",
    "\n",
    "# run SQL\n",
    "pgsql_cursor.execute( sql_string )\n",
    "pgsql_connection.commit()\n",
    "\n",
    "#temp_df = pandas.read_sql( sql_string, con = pgsql_engine )\n",
    "#temp_length = len( temp_df )\n",
    "#temp_df.head( n = temp_length )\n",
    "\n",
    "print( \"====> \" + str( sql_string ) + \" completed at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# admin_role privileges - declare variables\n",
    "table_name = \"\"\n",
    "table_name = work_db_table\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \"GRANT ALL PRIVILEGES ON TABLE \" + schema_name + \".\" + table_name\n",
    "sql_string += \" TO \" + admin_role\n",
    "sql_string += \";\"\n",
    "\n",
    "# run SQL\n",
    "pgsql_cursor.execute( sql_string )\n",
    "pgsql_connection.commit()\n",
    "\n",
    "#temp_df = pandas.read_sql( sql_string, con = pgsql_engine )\n",
    "#temp_length = len( temp_df )\n",
    "#temp_df.head( n = temp_length )\n",
    "\n",
    "print( \"====> \" + str( sql_string ) + \" completed at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_role privileges - declare variables\n",
    "table_name = \"\"\n",
    "table_name = work_db_table\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \"GRANT SELECT ON TABLE \" + schema_name + \".\" + table_name\n",
    "sql_string += \" TO \" + select_role\n",
    "sql_string += \";\"\n",
    "\n",
    "# run SQL\n",
    "pgsql_cursor.execute( sql_string )\n",
    "pgsql_connection.commit()\n",
    "\n",
    "#temp_df = pandas.read_sql( sql_string, con = pgsql_engine )\n",
    "#temp_length = len( temp_df )\n",
    "#temp_df.head( n = temp_length )\n",
    "\n",
    "print( \"====> \" + str( sql_string ) + \" completed at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding columns to data table\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Once you have created your initial table, you can start adding on additional columns with more detailed information on the person in each row.\n",
    "\n",
    "There are a number of ways one can do this:\n",
    "\n",
    "- Use the \"CREATE TABLE AS\" syntax to make a new copy of the table, adding the code to derive additional values into the `AS SELECT` section of the SQL.\n",
    "- Add columns to your existing table using `ALTER TABLE`, then populate those columns by:\n",
    "\n",
    "    - building the logic for deriving the values for new columns entirely in SQL and using \"`UPDATE ... FROM`\".\n",
    "    - using a combination of Python and SQL to build logic for deriving values in Python for a given row, then updating the table row-by-row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1 - Add columns using CREATE TABLE AS\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The first option is to add columns to your data set by creating a new table from the existing set of columns using \"`CREATE TABLE ... AS `\", including the new columns and the logic to derive them in the SELECT along with all of the existing columns from the old table.\n",
    "\n",
    "When you use this method, each time you create a new table, you should also:\n",
    "\n",
    "- provide access to the new table, as documented above in []().\n",
    "- \"`DROP`\" the previous version of the table, so that you free up space you no longer need.\n",
    "\n",
    "For example, the SQL below creates a new table from our existing head of household table, adding a given person's \"`docnbr`\" from Illinois department of corrections data for any heads of household in our data table whose \"`ssn_hash`\" matches one in the \"`ildoc.person table`\".\n",
    "\n",
    "First, create the new table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table - declare variables\n",
    "existing_table_name = \"\"\n",
    "new_table_name = existing_table_name + \"_001\"\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \"CREATE TABLE \" + schema_name + \".\" + new_table_name\n",
    "sql_string += \" AS SELECT\"\n",
    "sql_string += \" existing.recptno\"\n",
    "sql_string += \", existing.sex\"\n",
    "sql_string += \", existing.rac\"\n",
    "sql_string += \", existing.rootrace\"\n",
    "sql_string += \", existing.foreignbn\"\n",
    "sql_string += \", existing.ssn_hash\"\n",
    "sql_string += \", existing.fname_hash\"\n",
    "sql_string += \", existing.lname_hash\"\n",
    "sql_string += \", existing.birth_date\"\n",
    "sql_string += \", ildoc.ildoc_docnbr\"\n",
    "sql_string += \" FROM \" + schema_name + \".\" + existing_table_name + \" existing\"\n",
    "sql_string += \" LEFT OUTER JOIN ildoc.person ildoc ON ( existing.ssn_hash = ildoc.ssn_hash )\"\n",
    "#sql_string += \" WHERE EXTRACT( year FROM birth_date ) = 1976\"\n",
    "#sql_string += \" LIMIT 1000\"\n",
    "sql_string += \";\"\n",
    "\n",
    "print( \"====> \" + str( sql_string ) )\n",
    "\n",
    "# run SQL\n",
    "pgsql_cursor.execute( sql_string )\n",
    "pgsql_connection.commit()\n",
    "\n",
    "#temp_df = pandas.read_sql( sql_string, con = pgsql_engine )\n",
    "#temp_length = len( temp_df )\n",
    "#temp_df.head( n = temp_length )\n",
    "\n",
    "print( \"====> \" + str( sql_string ) + \" completed at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide access to the new table: [Provide access to table](#Provide-access-to-table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DROP the old table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create table - declare variables\n",
    "existing_table_name = \"\"\n",
    "new_table_name = existing_table_name + \"_001\"\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \"DROP TABLE \" + schema_name + \".\" + existing_table_name\n",
    "sql_string += \";\"\n",
    "\n",
    "print( \"====> \" + str( sql_string ) )\n",
    "\n",
    "# run SQL\n",
    "pgsql_cursor.execute( sql_string )\n",
    "pgsql_connection.commit()\n",
    "\n",
    "#temp_df = pandas.read_sql( sql_string, con = pgsql_engine )\n",
    "#temp_length = len( temp_df )\n",
    "#temp_df.head( n = temp_length )\n",
    "\n",
    "print( \"====> \" + str( sql_string ) + \" completed at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add columns to existing table\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The next two options are ways to update a table in place by adding columns to hold new data, then populating the new columns.\n",
    "\n",
    "First, add a column to the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add column\n",
    "table_name = \"\"\n",
    "column_name = \"<column_name>\"\n",
    "column_type = \"<column_type>\"\n",
    "\n",
    "# generate SQL\n",
    "sql_string = \"ALTER TABLE \" + schema_name + \".\" + table_name\n",
    "\n",
    "# start date values\n",
    "sql_string += \" ADD COLUMN \" + column_name + \" \" + column_type\n",
    "\n",
    "sql_string += \";\"\n",
    "\n",
    "# run SQL\n",
    "pgsql_cursor.execute( sql_string )\n",
    "pgsql_connection.commit()\n",
    "\n",
    "#temp_df = pandas.read_sql( sql_string, con = pgsql_engine )\n",
    "#temp_length = len( temp_df )\n",
    "#temp_df.head( n = temp_length )\n",
    "\n",
    "print( \"====> \" + str( sql_string ) + \" completed at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - UPDATE all at once using SQL\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Very basic example (nothing near the complexity of the stuff above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==> Set `start_date` from `start_date_orig`.\n",
    "\n",
    "# declare variables\n",
    "current_column = \"start_date\"\n",
    "\n",
    "# UPDATE\n",
    "sql_string = \"UPDATE \" + schema_name + \".\" + work_db_table\n",
    "sql_string += \" SET \" + current_column + \" = TO_DATE( \" + current_column + \"_orig, 'YYYY-MM-DD' )\"\n",
    "\n",
    "# WHERE clause\n",
    "# WHERE clause\n",
    "where_clause = \"WHERE \" + current_column + \" IS NULL\"\n",
    "where_clause += \" AND ( ( \" + current_column + \"_orig IS NOT NULL ) AND ( \" + current_column + \"_orig != '' ) )\"\n",
    "sql_string += \" \" + where_clause\n",
    "\n",
    "sql_string += \";\"\n",
    "\n",
    "print( \"SQL: \" + sql_string )\n",
    "\n",
    "# run SQL\n",
    "pgsql_cursor.execute( sql_string )\n",
    "pgsql_connection.commit()\n",
    "\n",
    "print( \"UPDATEd \" + where_clause + \" at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3 - For each row, calculate and store value using Python\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "To update each row individually:\n",
    "\n",
    "- Loop over your table of interest.\n",
    "- For each row, get info needed from that row, then calculate whatever value you care about for that row.\n",
    "- Store the value either back to the row in your table of interest, or store derived information elsewhere as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "sql_string = \"\"\n",
    "id_column_name = \"\"\n",
    "recptno_value = None\n",
    "ssn_hash_value = None\n",
    "bdate_year_value = None\n",
    "work_cursor = None\n",
    "work_year_18 = None\n",
    "work_year_19 = None\n",
    "work_year_20 = None\n",
    "work_year_21 = None\n",
    "work_year_in_list = []\n",
    "in_q3_year_list = []\n",
    "non_q3_year_list = []\n",
    "years_worked_in_q3 = -1\n",
    "years_worked_non_q3 = -1\n",
    "years_worked_only_q3 = -1\n",
    "row_counter = -1\n",
    "\n",
    "# declare variables working with work cursor.\n",
    "work_sql_string = \"\"\n",
    "work_row = None\n",
    "wage_year = None\n",
    "wage_quarter = None\n",
    "\n",
    "# make a work cursor, so you can query and update independent of your loop over your people.\n",
    "work_cursor = pgsql_connection.cursor( cursor_factory = psycopg2.extras.DictCursor )\n",
    "\n",
    "# get IDs from work table.\n",
    "sql_string = \"SELECT * FROM \" + schema_name + \".\" + work_db_table\n",
    "\n",
    "# only get rows that have not yet been updated - this lets you pick up if the program is interrupted.\n",
    "#sql_string += \" WHERE years_worked_in_q3 IS NULL\"\n",
    "\n",
    "sql_string += \";\"\n",
    "\n",
    "print( sql_string )\n",
    "\n",
    "# get list of records in person file, so we can process one-by-one\n",
    "pgsql_cursor.execute( sql_string )\n",
    "row_counter = 0\n",
    "for current_row in pgsql_cursor:\n",
    "    \n",
    "    # increment row Counter\n",
    "    row_counter += 1\n",
    "    \n",
    "    # initialize variables to make sure we empty out from last row.\n",
    "    recptno_value = None\n",
    "    ssn_hash_value = None\n",
    "    bdate_year_value = None\n",
    "    work_year_18 = -1\n",
    "    work_year_19 = -1\n",
    "    work_year_20 = -1\n",
    "    work_year_21 = -1\n",
    "    work_year_in_list = []\n",
    "    in_q3_year_list = []\n",
    "    non_q3_year_list = []\n",
    "    work_sql_string = \"\"\n",
    "    years_worked_in_q3 = -1\n",
    "    years_worked_non_q3 = -1\n",
    "    years_worked_only_q3 = -1\n",
    "    \n",
    "    # get values from record\n",
    "    recptno_value = current_row.get( \"recptno\", None )\n",
    "    ssn_hash_value = current_row.get( \"ssn_hash\", None )\n",
    "    bdate_year_value = current_row.get( \"bdate_year\", None )\n",
    "    \n",
    "    # for that recipient, perform logic to derive value for recipient.\n",
    "    \n",
    "    # Example: number of years worked Q3 for work_years 18-21,\n",
    "    #     number of years worked in quarters outside of Q3,\n",
    "    #     and number of years worked only in Q3.\n",
    "    work_year_18 = bdate_year_value + 18\n",
    "    work_year_19 = work_year_18 + 1\n",
    "    work_year_20 = work_year_19 + 1\n",
    "    work_year_21 = work_year_20 + 1\n",
    "    \n",
    "    # make list of work years, converted to strings for use in query.\n",
    "    work_year_in_list = [ str( work_year_18 ), str( work_year_19 ), str( work_year_20 ), str( work_year_21 ) ]\n",
    "    \n",
    "    # get all wage records for this person in these years.\n",
    "    in_q3_year_list = []\n",
    "    non_q3_year_list = []\n",
    "    \n",
    "    # create SQL to retrieve wage records...\n",
    "    work_sql_string = \"SELECT * FROM ides.il_wage\"\n",
    "    \n",
    "    # ...for the current person...\n",
    "    work_sql_string += \" WHERE ssn = '\" + ssn_hash_value + \"'\"\n",
    "    \n",
    "    # ...in the specified years...\n",
    "    work_sql_string += \" AND year IN ( \" + \", \".join( work_year_in_list ) + \" )\"\n",
    "    \n",
    "    # ...ordered by year and quarter, ascending.\n",
    "    work_sql_string += \" ORDER BY year ASC, quarter ASC\"\n",
    "\n",
    "    work_sql_string += \";\"\n",
    "    \n",
    "    # call the query and loop over results\n",
    "    work_cursor.execute()\n",
    "    for work_row in work_cursor:\n",
    "        \n",
    "        # ==> Here you do whatever work you need to for a given wage record.\n",
    "        \n",
    "        # get data\n",
    "        wage_year = work_row.get( \"year\", None )\n",
    "        wage_quarter = work_row.get( \"quarter\", None )\n",
    "        \n",
    "        # quarter 3?\n",
    "        if wage_quarter == 3:\n",
    "            \n",
    "            # 3 - if year not already in list, add it (can have multiple rows per quarter, so don't want to count)\n",
    "            if wage_year not in in_q3_year_list:\n",
    "                \n",
    "                in_q3_year_list.append( wage_year )\n",
    "                \n",
    "            #-- END check to see if we need to add year. --#\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # 1, 2, or 4 - if year not already in list, add it (can have multiple rows per quarter, so don't want to count)\n",
    "            if wage_year not in non_q3_year_list:\n",
    "                \n",
    "                non_q3_year_list.append( wage_year )\n",
    "                \n",
    "            #-- END check to see if we need to add year. --#\n",
    "            \n",
    "        #-- END check what quarter. --#\n",
    "        \n",
    "    #-- END loop over wage records. --#\n",
    "    \n",
    "    # ==> calculate values you care about:\n",
    "    \n",
    "    years_worked_in_q3 = len( in_q3_year_list )\n",
    "    years_worked_non_q3 = len( non_q3_year_list )\n",
    "    years_worked_only_q3 = 0\n",
    "    \n",
    "    # loop over q3 year list\n",
    "    for current_year in in_q3_year_list:\n",
    "        \n",
    "        # is that year also in non_q3 list?\n",
    "        if current_year not in non_q3_year_list:\n",
    "            \n",
    "            # no - just q3\n",
    "            years_worked_only_q3 += 1\n",
    "            \n",
    "        #-- END check to see if q3 year in non-q3 year list. --#\n",
    "        \n",
    "    #-- END loop over years in Q3 list. --#\n",
    "    \n",
    "    # ==> UPDATE\n",
    "    work_sql_string = \"UPDATE \" + schema_name + \".\" + work_db_table\n",
    "    work_sql_string += \" SET years_worked_in_q3 = \" + str( years_worked_in_q3 )\n",
    "    work_sql_string += \", years_worked_non_q3 = \" + str( years_worked_non_q3 )\n",
    "    work_sql_string += \", years_worked_only_q3 = \" + str( years_worked_only_q3 )\n",
    "    work_sql_string += \" WHERE recptno = \" + str( recptno_value )\n",
    "    work_sql_string += \";\"\n",
    "    \n",
    "    # execute and commit.\n",
    "    work_cursor.execute()\n",
    "    pgsql_connection.commit()\n",
    "\n",
    "    # every <cluster_size> people, output a message\n",
    "    cluster_size = 1000\n",
    "    if ( ( row_counter % cluster_size ) == 0 ):\n",
    "        \n",
    "        print( \"Processsed \" + str( row_counter ) + \" people.\" )\n",
    "        \n",
    "        # if you want, you could also only commit every <cluster_size> UPDATES.\n",
    "        # - in some cases, this will improve performance.\n",
    "        # To do this, commend out the commit above, then uncomment this one.\n",
    "        #pgsql_connection.commit()\n",
    "        \n",
    "    #-- END check to see if this is a multiple of <cluster_size> --#\n",
    "    \n",
    "#-- END loop over people. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing work step-by-step using temporary tables\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For new variables/features whose derivation involves particularly complicated logic, you will want to break up the work of creating these values to make it easier to validate the steps in your process.  One way to do this is to use the Python update pattern above to build out the feature derivation in Python code, step-by-step.  Another is to use the CREATE TABLE AS pattern to build up your variable/feature over multiple SQL statements, building up the results in temporary tables as you go and eventually joining the results from your temporary table into your main data table.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing between the three options\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Some considerations when choosing between these three options:\n",
    "\n",
    "- _**\"`CREATE TABLE AS`\" is the quickest way to add columns to a table.**_\n",
    "\n",
    "    - It is more efficient in a database to create new rows than it is to update existing rows.  This is especially true when you are working with a large data set.  If you are working with a large data set, however, particularly in a shared environment, cleaning up after yourself becomes very important - you'll need to be vigilant about dropping old versions of your table as you go so you don't take all the space in the database.\n",
    "\n",
    "- _**if your tables are embedded in a system where many processes interact with them, you'll probably want to add on rather than create new.**_\n",
    "\n",
    "    - If you move from implementing a data project to implementing a data system, you will likely want to move away from making new tables every time you want to add columns and start updating existing tables with new information, such that you minimize the chance for disrupting existing processes that refer to your tables.  Even in a project, adding on to existing tables could make sense if you have a lot of analysis code you want to re-run within a project over time.  This can be mitigated with disciplined renaming as you go to a point.  Eventually, though, as you build up additional tasks to run each time you recreate (indexes, primary key creation, etc.), you should consider adding on to existing tables rather than risking breaking code because you subtly alter the table fro mone version to the next.\n",
    "\n",
    "- _**As complexity of logic increases, you'll want to break up your work**_\n",
    "\n",
    "    - As complexity of logic for deriving a variable/feature increases, you'll want to find some way to break up the work of creating that feature, rather than trying to cram it all into one monolotihic SQL statement.  You can do this entirely in SQL by incrementally building up needed values in a temporary table then joining the results to your data table.  You can do this by writing the logic to derive the value in Python then updating each row with the result.  Or, you can do a combination.  But you'll want to break up the work of deriving complex features somehow so that you can verify/validate the steps in the process as you go.\n",
    "\n",
    "- _**Sometimes reliability or comprehensibility trumps performance**_\n",
    "\n",
    "    - Sometimes a slower Python loop can be preferable to a monolithic SQL statement if you expect problems that could cause your program to die in the middle.  A Python program can make incremental updates such that it can pick up where it left off if something goes wrong (like a surprise server shutdown).  Sometimes data or an algorithm will be susceptible to dying prematurely, and building robust code will be more important to you than pure performance.\n",
    "    - The same sort of consideration can be true based on your comfort with different languages.  Sometimes, a slow Python, R, or Stata program you understand that completes in a week and that can pick up if it dies will still be faster than working for weeks on a program in a language with which you are less comfortable.\n",
    "\n",
    "- _**EXCEPT when data is big enough that you have to tune for performance**_\n",
    "\n",
    "    - On the other hand, sometimes, with really large data sets, something written a certain way is just too slow.  Then you have to bite the bullet and figure out a way to make your code complete, even if it means using a language you aren't as comfortable with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
